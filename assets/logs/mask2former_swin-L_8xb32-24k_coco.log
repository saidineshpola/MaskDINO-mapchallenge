nohup: ignoring input
=====================================================
Starting distributed training with config: mask2former_swin-L_8xb32-24k_coco.py
Using 4 GPUs: 3,4,5,6
Work directory: work_dirs/mask2former_swin-L_8xb32-24k_coco
Log file: work_dirs/mask2former_swin-L_8xb32-24k_coco/training_20250511_060256.log
Master Addr: 127.0.0.1, Master Port: 29500, NNODES: 1, NODE_RANK: 0
=====================================================
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
W0511 06:02:57.297590 139657904020352 torch/distributed/run.py:779] 
W0511 06:02:57.297590 139657904020352 torch/distributed/run.py:779] *****************************************
W0511 06:02:57.297590 139657904020352 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0511 06:02:57.297590 139657904020352 torch/distributed/run.py:779] *****************************************
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
05/11 06:03:02 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.2 (main, Jul 16 2024, 09:34:26) [GCC 11.4.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 1123419503
    GPU 0,1,2,3: NVIDIA L40S
    CUDA_HOME: None
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.4.1+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.19.1+cu121
    OpenCV: 4.11.0
    MMEngine: 0.10.6

Runtime environment:
    cudnn_benchmark: False
    dist_cfg: {'backend': 'nccl'}
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    seed: 1123419503
    Distributed launcher: pytorch
    Distributed training: True
    GPU number: 4
------------------------------------------------------------

05/11 06:03:03 - mmengine - INFO - Config:
_delete_ = True
auto_scale_lr = dict(base_batch_size=16, enable=False)
backbone_embed_multi = dict(decay_mult=0.0, lr_mult=0.1)
backbone_norm_multi = dict(decay_mult=0.0, lr_mult=0.1)
backend_args = None
base_batch_size = 16
batch_augments = [
    dict(
        img_pad_value=0,
        mask_pad_value=0,
        pad_mask=True,
        pad_seg=False,
        size=(
            320,
            320,
        ),
        type='BatchFixedSizePad'),
]
batch_size = 16
custom_keys = dict({
    'absolute_pos_embed':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone':
    dict(decay_mult=1.0, lr_mult=0.1),
    'backbone.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.patch_embed.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.0.blocks.0.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.0.blocks.1.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.0.downsample.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.1.blocks.0.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.1.blocks.1.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.1.downsample.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.0.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.1.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.10.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.11.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.12.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.13.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.14.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.15.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.16.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.17.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.2.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.3.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.4.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.5.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.6.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.7.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.8.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.9.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.downsample.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.3.blocks.0.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.3.blocks.1.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'level_embed':
    dict(decay_mult=0.0, lr_mult=1.0),
    'query_embed':
    dict(decay_mult=0.0, lr_mult=1.0),
    'query_feat':
    dict(decay_mult=0.0, lr_mult=1.0),
    'relative_position_bias_table':
    dict(decay_mult=0.0, lr_mult=0.1)
})
data_root = '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/'
dataset_type = 'SatelliteDataset'
default_hooks = dict(
    checkpoint=dict(
        by_epoch=False,
        interval=2000,
        max_keep_ckpts=3,
        rule='greater',
        save_best='coco/segm_mAP_50',
        save_last=True,
        type='CheckpointHook'),
    early_stopping=dict(
        min_delta=0.005,
        monitor='coco/segm_mAP_50',
        patience=6,
        type='EarlyStoppingHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='DetVisualizationHook'))
default_scope = 'mmdet'
depths = [
    2,
    2,
    18,
    2,
]
dynamic_intervals = [
    (
        23040.0,
        24000,
    ),
]
embed_multi = dict(decay_mult=0.0, lr_mult=1.0)
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
image_size = (
    320,
    320,
)
interval = 2000
launcher = 'pytorch'
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=False, type='LogProcessor', window_size=50)
max_iters = 24000
mean = [
    88.03,
    104.33,
    115.77,
]
model = dict(
    backbone=dict(
        attn_drop_rate=0.0,
        convert_weights=True,
        depths=[
            2,
            2,
            18,
            2,
        ],
        drop_path_rate=0.3,
        drop_rate=0.0,
        embed_dims=192,
        frozen_stages=-1,
        init_cfg=dict(
            checkpoint=
            'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth',
            type='Pretrained'),
        mlp_ratio=4,
        num_heads=[
            6,
            12,
            24,
            48,
        ],
        out_indices=(
            0,
            1,
            2,
            3,
        ),
        patch_norm=True,
        qk_scale=None,
        qkv_bias=True,
        type='SwinTransformer',
        window_size=12,
        with_cp=False),
    data_preprocessor=dict(
        batch_augments=[
            dict(
                img_pad_value=0,
                mask_pad_value=0,
                pad_mask=True,
                pad_seg=False,
                size=(
                    320,
                    320,
                ),
                type='BatchFixedSizePad'),
        ],
        bgr_to_rgb=True,
        mask_pad_value=0,
        mean=[
            88.03,
            104.33,
            115.77,
        ],
        pad_mask=True,
        pad_seg=False,
        pad_size_divisor=32,
        seg_pad_value=255,
        std=[
            44.37,
            43.48,
            41.56,
        ],
        type='DetDataPreprocessor'),
    init_cfg=None,
    panoptic_fusion_head=dict(
        init_cfg=None,
        loss_panoptic=None,
        num_stuff_classes=0,
        num_things_classes=1,
        type='MaskFormerFusionHead'),
    panoptic_head=dict(
        enforce_decoder_input_project=False,
        feat_channels=256,
        in_channels=[
            192,
            384,
            768,
            1536,
        ],
        loss_cls=dict(
            class_weight=[
                1.0,
                1.0,
            ],
            loss_weight=2.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=False),
        loss_dice=dict(
            activate=True,
            eps=1.0,
            loss_weight=5.0,
            naive_dice=True,
            reduction='mean',
            type='DiceLoss',
            use_sigmoid=True),
        loss_mask=dict(
            loss_weight=5.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=True),
        num_queries=100,
        num_stuff_classes=0,
        num_things_classes=1,
        num_transformer_feat_level=4,
        out_channels=256,
        pixel_decoder=dict(
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                layer_cfg=dict(
                    ffn_cfg=dict(
                        act_cfg=dict(inplace=True, type='ReLU'),
                        embed_dims=256,
                        feedforward_channels=2048,
                        ffn_drop=0.0,
                        num_fcs=2),
                    self_attn_cfg=dict(
                        batch_first=True,
                        dropout=0.0,
                        embed_dims=256,
                        num_heads=8,
                        num_levels=4,
                        num_points=4)),
                num_layers=6),
            norm_cfg=dict(num_groups=32, type='GN'),
            num_outs=4,
            positional_encoding=dict(normalize=True, num_feats=128),
            type='MSDeformAttnPixelDecoder'),
        positional_encoding=dict(normalize=True, num_feats=128),
        strides=[
            4,
            8,
            16,
            32,
        ],
        transformer_decoder=dict(
            init_cfg=None,
            layer_cfg=dict(
                cross_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8),
                ffn_cfg=dict(
                    act_cfg=dict(inplace=True, type='ReLU'),
                    embed_dims=256,
                    feedforward_channels=2048,
                    ffn_drop=0.0,
                    num_fcs=2),
                self_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8)),
            num_layers=9,
            return_intermediate=True),
        type='Mask2FormerHead'),
    test_cfg=dict(
        filter_low_score=True,
        instance_on=True,
        iou_thr=0.5,
        max_per_image=100,
        panoptic_on=False,
        semantic_on=False),
    train_cfg=dict(
        assigner=dict(
            match_costs=[
                dict(type='ClassificationCost', weight=2.0),
                dict(
                    type='CrossEntropyLossCost', use_sigmoid=True, weight=5.0),
                dict(eps=1.0, pred_act=True, type='DiceCost', weight=5.0),
            ],
            type='HungarianAssigner'),
        importance_sample_ratio=0.75,
        num_points=12544,
        oversample_ratio=3.0,
        sampler=dict(type='MaskPseudoSampler')),
    type='Mask2Former')
num_classes = 1
num_stuff_classes = 1
num_things_classes = 1
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.01, norm_type=2),
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        eps=1e-08,
        lr=0.0001,
        type='AdamW',
        weight_decay=0.05),
    paramwise_cfg=dict(
        custom_keys=dict({
            'absolute_pos_embed':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone':
            dict(decay_mult=1.0, lr_mult=0.1),
            'backbone.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.patch_embed.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.0.blocks.0.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.0.blocks.1.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.0.downsample.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.1.blocks.0.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.1.blocks.1.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.1.downsample.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.0.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.1.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.10.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.11.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.12.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.13.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.14.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.15.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.16.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.17.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.2.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.3.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.4.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.5.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.6.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.7.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.8.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.9.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.downsample.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.3.blocks.0.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.3.blocks.1.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'level_embed':
            dict(decay_mult=0.0, lr_mult=1.0),
            'query_embed':
            dict(decay_mult=0.0, lr_mult=1.0),
            'query_feat':
            dict(decay_mult=0.0, lr_mult=1.0),
            'relative_position_bias_table':
            dict(decay_mult=0.0, lr_mult=0.1)
        }),
        norm_decay_mult=0.0),
    type='OptimWrapper')
param_scheduler = dict(
    begin=0,
    by_epoch=False,
    end=24000,
    gamma=0.1,
    milestones=[
        19200.0,
        21600.0,
    ],
    type='MultiStepLR')
pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth'
std = [
    44.37,
    43.48,
    41.56,
]
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=16,
    dataset=dict(
        ann_file=
        '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/test/annotations/annotation_non_augmented.json',
        backend_args=None,
        data_prefix=dict(
            img=
            '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/test/images/'
        ),
        data_root='/usr/instance_segmentation/',
        filter_cfg=dict(filter_empty_gt=True, min_size=15),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                320,
                320,
            ), type='Resize'),
            dict(
                pad_val=dict(img=(
                    114,
                    114,
                    114,
                )),
                size=(
                    320,
                    320,
                ),
                type='Pad'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='SatelliteDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    ann_file=
    '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/test/annotations/annotation_non_augmented.json',
    backend_args=None,
    format_only=False,
    metric=[
        'segm',
    ],
    type='CocoMetric')
test_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        320,
        320,
    ), type='Resize'),
    dict(pad_val=dict(img=(
        114,
        114,
        114,
    )), size=(
        320,
        320,
    ), type='Pad'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(
    dynamic_intervals=[
        (
            23040.0,
            24000,
        ),
    ],
    max_iters=24000,
    type='IterBasedTrainLoop',
    val_interval=2000)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=16,
    dataset=dict(
        ann_file=
        '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/train/annotations/annotation_non_augmented.json',
        backend_args=None,
        data_prefix=dict(
            img=
            '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/train/images/'
        ),
        data_root='/usr/instance_segmentation/',
        filter_cfg=dict(filter_empty_gt=True, min_size=15),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=True,
                with_mask=True),
            dict(img_scale=(
                320,
                320,
            ), pad_val=114.0, type='CachedMosaic'),
            dict(
                keep_ratio=True,
                ratio_range=(
                    0.1,
                    2.0,
                ),
                scale=(
                    320,
                    320,
                ),
                type='RandomResize'),
            dict(
                allow_negative_crop=True,
                crop_size=(
                    320,
                    320,
                ),
                recompute_bbox=True,
                type='RandomCrop'),
            dict(type='YOLOXHSVRandomAug'),
            dict(prob=0.5, type='RandomFlip'),
            dict(
                pad_val=dict(img=(
                    114,
                    114,
                    114,
                )),
                size=(
                    320,
                    320,
                ),
                type='Pad'),
            dict(
                img_scale=(
                    320,
                    320,
                ),
                max_cached_images=20,
                pad_val=(
                    114,
                    114,
                    114,
                ),
                ratio_range=(
                    1.0,
                    1.0,
                ),
                type='CachedMixUp'),
            dict(min_gt_bbox_wh=(
                1,
                1,
            ), type='FilterAnnotations'),
            dict(type='PackDetInputs'),
        ],
        type='SatelliteDataset'),
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='InfiniteSampler'))
train_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(
        poly2mask=False,
        type='LoadAnnotations',
        with_bbox=True,
        with_mask=True),
    dict(img_scale=(
        320,
        320,
    ), pad_val=114.0, type='CachedMosaic'),
    dict(
        keep_ratio=True,
        ratio_range=(
            0.1,
            2.0,
        ),
        scale=(
            320,
            320,
        ),
        type='RandomResize'),
    dict(
        allow_negative_crop=True,
        crop_size=(
            320,
            320,
        ),
        recompute_bbox=True,
        type='RandomCrop'),
    dict(type='YOLOXHSVRandomAug'),
    dict(prob=0.5, type='RandomFlip'),
    dict(pad_val=dict(img=(
        114,
        114,
        114,
    )), size=(
        320,
        320,
    ), type='Pad'),
    dict(
        img_scale=(
            320,
            320,
        ),
        max_cached_images=20,
        pad_val=(
            114,
            114,
            114,
        ),
        ratio_range=(
            1.0,
            1.0,
        ),
        type='CachedMixUp'),
    dict(min_gt_bbox_wh=(
        1,
        1,
    ), type='FilterAnnotations'),
    dict(type='PackDetInputs'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=16,
    dataset=dict(
        ann_file=
        '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/val/annotations/annotation_non_augmented.json',
        backend_args=None,
        data_prefix=dict(
            img=
            '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/val/images/'
        ),
        data_root='/usr/instance_segmentation/',
        filter_cfg=dict(filter_empty_gt=True, min_size=15),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                320,
                320,
            ), type='Resize'),
            dict(
                pad_val=dict(img=(
                    114,
                    114,
                    114,
                )),
                size=(
                    320,
                    320,
                ),
                type='Pad'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='SatelliteDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    ann_file=
    '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/val/annotations/annotation_non_augmented.json',
    backend_args=None,
    format_only=False,
    metric=[
        'segm',
    ],
    type='CocoMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = 'work_dirs/mask2former_swin-L_8xb32-24k_coco'

05/11 06:03:06 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
(LOWEST      ) EarlyStoppingHook                  
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
(LOWEST      ) EarlyStoppingHook                  
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=2.74s)
creating index...
Done (t=2.80s)
creating index...
Done (t=2.84s)
creating index...
index created!
index created!
index created!
Done (t=3.13s)
creating index...
index created!
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.50s)
creating index...
index created!
Done (t=0.50s)
creating index...
index created!
Done (t=0.47s)
creating index...
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr_mult=0.1
index created!
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:weight_decay=0.05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:decay_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm0.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm0.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm0.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm0.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm0.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm0.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm0.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm0.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr=1e-05
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr_mult=0.1
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.0.gn.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.0.gn.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.1.gn.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.1.gn.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.2.gn.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.2.gn.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.3.gn.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.3.gn.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.0.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.0.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.0.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.0.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.0.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.0.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.0.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.0.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.0.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.0.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.0.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.0.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.0.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.0.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.0.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.0.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.0.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.0.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.0.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.0.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.0.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.0.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.0.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.0.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.0.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.0.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.0.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.0.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.0.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.0.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.1.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.1.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.2.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.2.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.post_norm.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.post_norm.bias:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:lr=0.0001
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:lr_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:lr=0.0001
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:lr_mult=1.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:lr=0.0001
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:weight_decay=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:decay_mult=0.0
05/11 06:03:14 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:lr_mult=1.0
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.39s)
creating index...
Done (t=0.48s)
creating index...
index created!
index created!
Done (t=0.40s)
creating index...
index created!
Done (t=0.39s)
creating index...
index created!
loading annotations into memory...
Done (t=0.40s)
creating index...
index created!
05/11 06:03:17 - mmengine - INFO - Loads checkpoint by http backend from path: https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth
05/11 06:03:17 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
05/11 06:03:17 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
05/11 06:03:17 - mmengine - INFO - Checkpoints will be saved to /data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-L_8xb32-24k_coco.
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/torch/autograd/graph.py:769: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/torch/autograd/graph.py:769: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/torch/autograd/graph.py:769: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/watchtower/.pyenv/versions/dmcon/lib/python3.10/site-packages/torch/autograd/graph.py:769: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
05/11 06:04:35 - mmengine - INFO - Iter(train) [   50/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 10:19:05  time: 1.5510  data_time: 0.0310  memory: 38889  grad_norm: 94.6134  loss: 55.8301  loss_cls: 1.1044  loss_mask: 0.6242  loss_dice: 4.2766  d0.loss_cls: 0.9625  d0.loss_mask: 0.3420  d0.loss_dice: 4.2547  d1.loss_cls: 0.7032  d1.loss_mask: 0.3731  d1.loss_dice: 4.2390  d2.loss_cls: 0.7075  d2.loss_mask: 0.3974  d2.loss_dice: 4.2584  d3.loss_cls: 0.7765  d3.loss_mask: 0.4184  d3.loss_dice: 4.2553  d4.loss_cls: 0.8321  d4.loss_mask: 0.4038  d4.loss_dice: 4.2731  d5.loss_cls: 0.8746  d5.loss_mask: 0.4219  d5.loss_dice: 4.2677  d6.loss_cls: 0.9035  d6.loss_mask: 0.4414  d6.loss_dice: 4.2531  d7.loss_cls: 0.9464  d7.loss_mask: 0.4932  d7.loss_dice: 4.2473  d8.loss_cls: 0.9967  d8.loss_mask: 0.5488  d8.loss_dice: 4.2332
05/11 06:05:52 - mmengine - INFO - Iter(train) [  100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 10:16:26  time: 1.5441  data_time: 0.0287  memory: 38821  grad_norm: 50.4397  loss: 52.2752  loss_cls: 0.4710  loss_mask: 0.3219  loss_dice: 4.3944  d0.loss_cls: 0.9239  d0.loss_mask: 0.3105  d0.loss_dice: 4.3988  d1.loss_cls: 0.4699  d1.loss_mask: 0.3235  d1.loss_dice: 4.3868  d2.loss_cls: 0.4436  d2.loss_mask: 0.3225  d2.loss_dice: 4.4164  d3.loss_cls: 0.4412  d3.loss_mask: 0.3198  d3.loss_dice: 4.4119  d4.loss_cls: 0.4456  d4.loss_mask: 0.3217  d4.loss_dice: 4.4140  d5.loss_cls: 0.4447  d5.loss_mask: 0.3205  d5.loss_dice: 4.4280  d6.loss_cls: 0.4503  d6.loss_mask: 0.3163  d6.loss_dice: 4.4067  d7.loss_cls: 0.4517  d7.loss_mask: 0.3146  d7.loss_dice: 4.4174  d8.loss_cls: 0.4611  d8.loss_mask: 0.3179  d8.loss_dice: 4.4087
05/11 06:07:09 - mmengine - INFO - Iter(train) [  150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 10:14:30  time: 1.5427  data_time: 0.0282  memory: 39558  grad_norm: 59.7059  loss: 53.1970  loss_cls: 0.4411  loss_mask: 0.4096  loss_dice: 4.4574  d0.loss_cls: 0.9021  d0.loss_mask: 0.3927  d0.loss_dice: 4.4471  d1.loss_cls: 0.4264  d1.loss_mask: 0.4069  d1.loss_dice: 4.4321  d2.loss_cls: 0.4162  d2.loss_mask: 0.4134  d2.loss_dice: 4.4226  d3.loss_cls: 0.4156  d3.loss_mask: 0.4108  d3.loss_dice: 4.4227  d4.loss_cls: 0.4246  d4.loss_mask: 0.4152  d4.loss_dice: 4.4227  d5.loss_cls: 0.4259  d5.loss_mask: 0.4220  d5.loss_dice: 4.4082  d6.loss_cls: 0.4300  d6.loss_mask: 0.4156  d6.loss_dice: 4.4227  d7.loss_cls: 0.4410  d7.loss_mask: 0.4159  d7.loss_dice: 4.4412  d8.loss_cls: 0.4404  d8.loss_mask: 0.4133  d8.loss_dice: 4.4417
05/11 06:08:26 - mmengine - INFO - Iter(train) [  200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 10:12:34  time: 1.5394  data_time: 0.0322  memory: 38234  grad_norm: 67.9502  loss: 47.2898  loss_cls: 0.4112  loss_mask: 0.4319  loss_dice: 3.8258  d0.loss_cls: 0.8946  d0.loss_mask: 0.3801  d0.loss_dice: 4.0000  d1.loss_cls: 0.3955  d1.loss_mask: 0.4018  d1.loss_dice: 3.9129  d2.loss_cls: 0.3908  d2.loss_mask: 0.4204  d2.loss_dice: 3.8640  d3.loss_cls: 0.3974  d3.loss_mask: 0.4305  d3.loss_dice: 3.8331  d4.loss_cls: 0.3960  d4.loss_mask: 0.4448  d4.loss_dice: 3.8208  d5.loss_cls: 0.4067  d5.loss_mask: 0.4447  d5.loss_dice: 3.8050  d6.loss_cls: 0.4083  d6.loss_mask: 0.4342  d6.loss_dice: 3.8091  d7.loss_cls: 0.4124  d7.loss_mask: 0.4309  d7.loss_dice: 3.8157  d8.loss_cls: 0.4127  d8.loss_mask: 0.4391  d8.loss_dice: 3.8197
05/11 06:09:42 - mmengine - INFO - Iter(train) [  250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 10:09:49  time: 1.5258  data_time: 0.0266  memory: 38562  grad_norm: 75.9617  loss: 45.1709  loss_cls: 0.4476  loss_mask: 0.4481  loss_dice: 3.5530  d0.loss_cls: 0.8798  d0.loss_mask: 0.3738  d0.loss_dice: 3.8083  d1.loss_cls: 0.4171  d1.loss_mask: 0.4044  d1.loss_dice: 3.6855  d2.loss_cls: 0.4294  d2.loss_mask: 0.4278  d2.loss_dice: 3.6141  d3.loss_cls: 0.4367  d3.loss_mask: 0.4326  d3.loss_dice: 3.5815  d4.loss_cls: 0.4420  d4.loss_mask: 0.4422  d4.loss_dice: 3.5589  d5.loss_cls: 0.4492  d5.loss_mask: 0.4469  d5.loss_dice: 3.5441  d6.loss_cls: 0.4488  d6.loss_mask: 0.4427  d6.loss_dice: 3.5543  d7.loss_cls: 0.4502  d7.loss_mask: 0.4458  d7.loss_dice: 3.5548  d8.loss_cls: 0.4507  d8.loss_mask: 0.4498  d8.loss_dice: 3.5509
05/11 06:10:59 - mmengine - INFO - Iter(train) [  300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 10:07:51  time: 1.5303  data_time: 0.0311  memory: 38977  grad_norm: 75.7183  loss: 45.5712  loss_cls: 0.4903  loss_mask: 0.4303  loss_dice: 3.5609  d0.loss_cls: 0.8562  d0.loss_mask: 0.3890  d0.loss_dice: 3.9103  d1.loss_cls: 0.4232  d1.loss_mask: 0.4141  d1.loss_dice: 3.7389  d2.loss_cls: 0.4646  d2.loss_mask: 0.4072  d2.loss_dice: 3.6306  d3.loss_cls: 0.4721  d3.loss_mask: 0.4169  d3.loss_dice: 3.6013  d4.loss_cls: 0.4831  d4.loss_mask: 0.4201  d4.loss_dice: 3.5666  d5.loss_cls: 0.4839  d5.loss_mask: 0.4268  d5.loss_dice: 3.5570  d6.loss_cls: 0.4831  d6.loss_mask: 0.4240  d6.loss_dice: 3.5548  d7.loss_cls: 0.4932  d7.loss_mask: 0.4260  d7.loss_dice: 3.5597  d8.loss_cls: 0.4868  d8.loss_mask: 0.4341  d8.loss_dice: 3.5661
05/11 06:12:15 - mmengine - INFO - Iter(train) [  350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 10:05:46  time: 1.5246  data_time: 0.0292  memory: 38520  grad_norm: 66.5242  loss: 42.1909  loss_cls: 0.5621  loss_mask: 0.3648  loss_dice: 3.2336  d0.loss_cls: 0.8597  d0.loss_mask: 0.3507  d0.loss_dice: 3.5939  d1.loss_cls: 0.4550  d1.loss_mask: 0.3613  d1.loss_dice: 3.4213  d2.loss_cls: 0.5084  d2.loss_mask: 0.3497  d2.loss_dice: 3.3149  d3.loss_cls: 0.5251  d3.loss_mask: 0.3526  d3.loss_dice: 3.2821  d4.loss_cls: 0.5397  d4.loss_mask: 0.3575  d4.loss_dice: 3.2238  d5.loss_cls: 0.5444  d5.loss_mask: 0.3576  d5.loss_dice: 3.2159  d6.loss_cls: 0.5454  d6.loss_mask: 0.3560  d6.loss_dice: 3.2168  d7.loss_cls: 0.5581  d7.loss_mask: 0.3591  d7.loss_dice: 3.2233  d8.loss_cls: 0.5572  d8.loss_mask: 0.3642  d8.loss_dice: 3.2365
05/11 06:13:32 - mmengine - INFO - Iter(train) [  400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 10:04:35  time: 1.5388  data_time: 0.0285  memory: 38487  grad_norm: 76.4843  loss: 42.2579  loss_cls: 0.6040  loss_mask: 0.3631  loss_dice: 3.1827  d0.loss_cls: 0.8246  d0.loss_mask: 0.3668  d0.loss_dice: 3.6539  d1.loss_cls: 0.4656  d1.loss_mask: 0.3707  d1.loss_dice: 3.4248  d2.loss_cls: 0.5312  d2.loss_mask: 0.3610  d2.loss_dice: 3.3114  d3.loss_cls: 0.5512  d3.loss_mask: 0.3623  d3.loss_dice: 3.2492  d4.loss_cls: 0.5722  d4.loss_mask: 0.3653  d4.loss_dice: 3.1917  d5.loss_cls: 0.5779  d5.loss_mask: 0.3640  d5.loss_dice: 3.1810  d6.loss_cls: 0.5809  d6.loss_mask: 0.3603  d6.loss_dice: 3.1679  d7.loss_cls: 0.5943  d7.loss_mask: 0.3607  d7.loss_dice: 3.1800  d8.loss_cls: 0.5989  d8.loss_mask: 0.3630  d8.loss_dice: 3.1772
05/11 06:14:49 - mmengine - INFO - Iter(train) [  450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 10:03:34  time: 1.5433  data_time: 0.0287  memory: 39019  grad_norm: 75.2431  loss: 43.7883  loss_cls: 0.6499  loss_mask: 0.3386  loss_dice: 3.2917  d0.loss_cls: 0.8246  d0.loss_mask: 0.3732  d0.loss_dice: 3.8389  d1.loss_cls: 0.5089  d1.loss_mask: 0.3543  d1.loss_dice: 3.5684  d2.loss_cls: 0.5663  d2.loss_mask: 0.3493  d2.loss_dice: 3.4383  d3.loss_cls: 0.5959  d3.loss_mask: 0.3447  d3.loss_dice: 3.3831  d4.loss_cls: 0.6177  d4.loss_mask: 0.3457  d4.loss_dice: 3.3157  d5.loss_cls: 0.6224  d5.loss_mask: 0.3416  d5.loss_dice: 3.3053  d6.loss_cls: 0.6321  d6.loss_mask: 0.3367  d6.loss_dice: 3.2890  d7.loss_cls: 0.6418  d7.loss_mask: 0.3374  d7.loss_dice: 3.2972  d8.loss_cls: 0.6476  d8.loss_mask: 0.3375  d8.loss_dice: 3.2944
05/11 06:16:06 - mmengine - INFO - Iter(train) [  500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 10:02:04  time: 1.5321  data_time: 0.0285  memory: 39007  grad_norm: 84.1602  loss: 42.2589  loss_cls: 0.6839  loss_mask: 0.3482  loss_dice: 3.0836  d0.loss_cls: 0.8043  d0.loss_mask: 0.4012  d0.loss_dice: 3.6925  d1.loss_cls: 0.5530  d1.loss_mask: 0.3609  d1.loss_dice: 3.3810  d2.loss_cls: 0.6228  d2.loss_mask: 0.3531  d2.loss_dice: 3.2360  d3.loss_cls: 0.6504  d3.loss_mask: 0.3479  d3.loss_dice: 3.1628  d4.loss_cls: 0.6708  d4.loss_mask: 0.3471  d4.loss_dice: 3.1022  d5.loss_cls: 0.6754  d5.loss_mask: 0.3412  d5.loss_dice: 3.1007  d6.loss_cls: 0.6744  d6.loss_mask: 0.3426  d6.loss_dice: 3.0885  d7.loss_cls: 0.6828  d7.loss_mask: 0.3453  d7.loss_dice: 3.0855  d8.loss_cls: 0.6814  d8.loss_mask: 0.3481  d8.loss_dice: 3.0914
05/11 06:17:23 - mmengine - INFO - Iter(train) [  550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 10:01:03  time: 1.5445  data_time: 0.0355  memory: 38622  grad_norm: 76.9092  loss: 41.0524  loss_cls: 0.6813  loss_mask: 0.3249  loss_dice: 2.9964  d0.loss_cls: 0.8023  d0.loss_mask: 0.3657  d0.loss_dice: 3.5533  d1.loss_cls: 0.5597  d1.loss_mask: 0.3412  d1.loss_dice: 3.2639  d2.loss_cls: 0.6299  d2.loss_mask: 0.3329  d2.loss_dice: 3.1334  d3.loss_cls: 0.6522  d3.loss_mask: 0.3289  d3.loss_dice: 3.0648  d4.loss_cls: 0.6706  d4.loss_mask: 0.3282  d4.loss_dice: 3.0076  d5.loss_cls: 0.6721  d5.loss_mask: 0.3244  d5.loss_dice: 3.0106  d6.loss_cls: 0.6729  d6.loss_mask: 0.3265  d6.loss_dice: 3.0053  d7.loss_cls: 0.6778  d7.loss_mask: 0.3249  d7.loss_dice: 2.9966  d8.loss_cls: 0.6790  d8.loss_mask: 0.3251  d8.loss_dice: 3.0002
05/11 06:18:40 - mmengine - INFO - Iter(train) [  600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:59:44  time: 1.5371  data_time: 0.0300  memory: 39136  grad_norm: 74.0526  loss: 41.2672  loss_cls: 0.6862  loss_mask: 0.3100  loss_dice: 3.0321  d0.loss_cls: 0.8007  d0.loss_mask: 0.3475  d0.loss_dice: 3.5983  d1.loss_cls: 0.5720  d1.loss_mask: 0.3269  d1.loss_dice: 3.3088  d2.loss_cls: 0.6420  d2.loss_mask: 0.3170  d2.loss_dice: 3.1543  d3.loss_cls: 0.6643  d3.loss_mask: 0.3128  d3.loss_dice: 3.0904  d4.loss_cls: 0.6803  d4.loss_mask: 0.3116  d4.loss_dice: 3.0352  d5.loss_cls: 0.6786  d5.loss_mask: 0.3084  d5.loss_dice: 3.0303  d6.loss_cls: 0.6809  d6.loss_mask: 0.3068  d6.loss_dice: 3.0264  d7.loss_cls: 0.6887  d7.loss_mask: 0.3091  d7.loss_dice: 3.0283  d8.loss_cls: 0.6843  d8.loss_mask: 0.3093  d8.loss_dice: 3.0261
05/11 06:19:57 - mmengine - INFO - Iter(train) [  650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:58:33  time: 1.5407  data_time: 0.0300  memory: 38532  grad_norm: 76.5999  loss: 39.3638  loss_cls: 0.6960  loss_mask: 0.3067  loss_dice: 2.8353  d0.loss_cls: 0.7911  d0.loss_mask: 0.3517  d0.loss_dice: 3.3877  d1.loss_cls: 0.5886  d1.loss_mask: 0.3244  d1.loss_dice: 3.0943  d2.loss_cls: 0.6500  d2.loss_mask: 0.3177  d2.loss_dice: 2.9552  d3.loss_cls: 0.6711  d3.loss_mask: 0.3144  d3.loss_dice: 2.8918  d4.loss_cls: 0.6898  d4.loss_mask: 0.3100  d4.loss_dice: 2.8433  d5.loss_cls: 0.6873  d5.loss_mask: 0.3061  d5.loss_dice: 2.8365  d6.loss_cls: 0.6894  d6.loss_mask: 0.3056  d6.loss_dice: 2.8392  d7.loss_cls: 0.6948  d7.loss_mask: 0.3070  d7.loss_dice: 2.8374  d8.loss_cls: 0.6985  d8.loss_mask: 0.3060  d8.loss_dice: 2.8368
05/11 06:21:14 - mmengine - INFO - Iter(train) [  700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:57:16  time: 1.5383  data_time: 0.0306  memory: 38998  grad_norm: 68.9520  loss: 38.5736  loss_cls: 0.7269  loss_mask: 0.3118  loss_dice: 2.7055  d0.loss_cls: 0.7855  d0.loss_mask: 0.3735  d0.loss_dice: 3.3007  d1.loss_cls: 0.6381  d1.loss_mask: 0.3318  d1.loss_dice: 2.9798  d2.loss_cls: 0.6970  d2.loss_mask: 0.3246  d2.loss_dice: 2.8325  d3.loss_cls: 0.7130  d3.loss_mask: 0.3171  d3.loss_dice: 2.7710  d4.loss_cls: 0.7253  d4.loss_mask: 0.3152  d4.loss_dice: 2.7204  d5.loss_cls: 0.7255  d5.loss_mask: 0.3121  d5.loss_dice: 2.7091  d6.loss_cls: 0.7257  d6.loss_mask: 0.3111  d6.loss_dice: 2.7140  d7.loss_cls: 0.7270  d7.loss_mask: 0.3137  d7.loss_dice: 2.7136  d8.loss_cls: 0.7270  d8.loss_mask: 0.3117  d8.loss_dice: 2.7133
05/11 06:22:30 - mmengine - INFO - Iter(train) [  750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:55:32  time: 1.5204  data_time: 0.0309  memory: 38786  grad_norm: 74.0034  loss: 39.1298  loss_cls: 0.7296  loss_mask: 0.2991  loss_dice: 2.7795  d0.loss_cls: 0.7753  d0.loss_mask: 0.3491  d0.loss_dice: 3.3844  d1.loss_cls: 0.6531  d1.loss_mask: 0.3079  d1.loss_dice: 3.0336  d2.loss_cls: 0.7092  d2.loss_mask: 0.3064  d2.loss_dice: 2.8927  d3.loss_cls: 0.7182  d3.loss_mask: 0.3025  d3.loss_dice: 2.8267  d4.loss_cls: 0.7239  d4.loss_mask: 0.3007  d4.loss_dice: 2.7938  d5.loss_cls: 0.7259  d5.loss_mask: 0.2996  d5.loss_dice: 2.7834  d6.loss_cls: 0.7249  d6.loss_mask: 0.2986  d6.loss_dice: 2.7919  d7.loss_cls: 0.7260  d7.loss_mask: 0.2977  d7.loss_dice: 2.7875  d8.loss_cls: 0.7290  d8.loss_mask: 0.2980  d8.loss_dice: 2.7816
05/11 06:23:46 - mmengine - INFO - Iter(train) [  800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:54:03  time: 1.5283  data_time: 0.0381  memory: 38588  grad_norm: 67.7607  loss: 38.9056  loss_cls: 0.7103  loss_mask: 0.2975  loss_dice: 2.7832  d0.loss_cls: 0.7726  d0.loss_mask: 0.3359  d0.loss_dice: 3.3601  d1.loss_cls: 0.6443  d1.loss_mask: 0.3067  d1.loss_dice: 3.0228  d2.loss_cls: 0.7067  d2.loss_mask: 0.3026  d2.loss_dice: 2.8881  d3.loss_cls: 0.7107  d3.loss_mask: 0.2993  d3.loss_dice: 2.8184  d4.loss_cls: 0.7141  d4.loss_mask: 0.2992  d4.loss_dice: 2.7772  d5.loss_cls: 0.7116  d5.loss_mask: 0.2980  d5.loss_dice: 2.7764  d6.loss_cls: 0.7096  d6.loss_mask: 0.2966  d6.loss_dice: 2.7820  d7.loss_cls: 0.7099  d7.loss_mask: 0.2969  d7.loss_dice: 2.7846  d8.loss_cls: 0.7111  d8.loss_mask: 0.2963  d8.loss_dice: 2.7831
05/11 06:25:04 - mmengine - INFO - Iter(train) [  850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:53:15  time: 1.5576  data_time: 0.0314  memory: 39579  grad_norm: 63.0222  loss: 38.8560  loss_cls: 0.7202  loss_mask: 0.2742  loss_dice: 2.7906  d0.loss_cls: 0.7707  d0.loss_mask: 0.3077  d0.loss_dice: 3.3923  d1.loss_cls: 0.6457  d1.loss_mask: 0.2824  d1.loss_dice: 3.0315  d2.loss_cls: 0.7098  d2.loss_mask: 0.2838  d2.loss_dice: 2.8896  d3.loss_cls: 0.7117  d3.loss_mask: 0.2800  d3.loss_dice: 2.8311  d4.loss_cls: 0.7204  d4.loss_mask: 0.2768  d4.loss_dice: 2.7933  d5.loss_cls: 0.7185  d5.loss_mask: 0.2748  d5.loss_dice: 2.7873  d6.loss_cls: 0.7148  d6.loss_mask: 0.2736  d6.loss_dice: 2.7959  d7.loss_cls: 0.7174  d7.loss_mask: 0.2754  d7.loss_dice: 2.8018  d8.loss_cls: 0.7198  d8.loss_mask: 0.2759  d8.loss_dice: 2.7889
05/11 06:26:21 - mmengine - INFO - Iter(train) [  900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:51:56  time: 1.5359  data_time: 0.0294  memory: 39381  grad_norm: 65.6522  loss: 38.6580  loss_cls: 0.7373  loss_mask: 0.2834  loss_dice: 2.7435  d0.loss_cls: 0.7758  d0.loss_mask: 0.3281  d0.loss_dice: 3.3269  d1.loss_cls: 0.6868  d1.loss_mask: 0.2908  d1.loss_dice: 2.9778  d2.loss_cls: 0.7299  d2.loss_mask: 0.2874  d2.loss_dice: 2.8474  d3.loss_cls: 0.7334  d3.loss_mask: 0.2847  d3.loss_dice: 2.7808  d4.loss_cls: 0.7377  d4.loss_mask: 0.2819  d4.loss_dice: 2.7439  d5.loss_cls: 0.7381  d5.loss_mask: 0.2816  d5.loss_dice: 2.7455  d6.loss_cls: 0.7347  d6.loss_mask: 0.2824  d6.loss_dice: 2.7488  d7.loss_cls: 0.7381  d7.loss_mask: 0.2836  d7.loss_dice: 2.7531  d8.loss_cls: 0.7400  d8.loss_mask: 0.2843  d8.loss_dice: 2.7503
05/11 06:27:39 - mmengine - INFO - Iter(train) [  950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:50:53  time: 1.5496  data_time: 0.0306  memory: 39491  grad_norm: 55.5828  loss: 39.2806  loss_cls: 0.7552  loss_mask: 0.2803  loss_dice: 2.7912  d0.loss_cls: 0.7825  d0.loss_mask: 0.3245  d0.loss_dice: 3.4183  d1.loss_cls: 0.6960  d1.loss_mask: 0.2930  d1.loss_dice: 3.0374  d2.loss_cls: 0.7349  d2.loss_mask: 0.2877  d2.loss_dice: 2.8988  d3.loss_cls: 0.7349  d3.loss_mask: 0.2854  d3.loss_dice: 2.8353  d4.loss_cls: 0.7390  d4.loss_mask: 0.2817  d4.loss_dice: 2.7949  d5.loss_cls: 0.7444  d5.loss_mask: 0.2808  d5.loss_dice: 2.7955  d6.loss_cls: 0.7447  d6.loss_mask: 0.2790  d6.loss_dice: 2.7969  d7.loss_cls: 0.7456  d7.loss_mask: 0.2830  d7.loss_dice: 2.8107  d8.loss_cls: 0.7498  d8.loss_mask: 0.2801  d8.loss_dice: 2.7992
05/11 06:28:55 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 06:28:55 - mmengine - INFO - Iter(train) [ 1000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:49:32  time: 1.5344  data_time: 0.0303  memory: 38902  grad_norm: 78.5600  loss: 37.5271  loss_cls: 0.7271  loss_mask: 0.2836  loss_dice: 2.6368  d0.loss_cls: 0.7621  d0.loss_mask: 0.3359  d0.loss_dice: 3.2331  d1.loss_cls: 0.6915  d1.loss_mask: 0.2998  d1.loss_dice: 2.8615  d2.loss_cls: 0.7323  d2.loss_mask: 0.2967  d2.loss_dice: 2.7342  d3.loss_cls: 0.7307  d3.loss_mask: 0.2912  d3.loss_dice: 2.6682  d4.loss_cls: 0.7293  d4.loss_mask: 0.2843  d4.loss_dice: 2.6299  d5.loss_cls: 0.7305  d5.loss_mask: 0.2818  d5.loss_dice: 2.6372  d6.loss_cls: 0.7285  d6.loss_mask: 0.2809  d6.loss_dice: 2.6397  d7.loss_cls: 0.7276  d7.loss_mask: 0.2847  d7.loss_dice: 2.6421  d8.loss_cls: 0.7248  d8.loss_mask: 0.2820  d8.loss_dice: 2.6388
05/11 06:30:12 - mmengine - INFO - Iter(train) [ 1050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:48:23  time: 1.5452  data_time: 0.0300  memory: 38917  grad_norm: 58.3341  loss: 37.2365  loss_cls: 0.7410  loss_mask: 0.2665  loss_dice: 2.6213  d0.loss_cls: 0.7488  d0.loss_mask: 0.3111  d0.loss_dice: 3.2089  d1.loss_cls: 0.7013  d1.loss_mask: 0.2817  d1.loss_dice: 2.8349  d2.loss_cls: 0.7335  d2.loss_mask: 0.2788  d2.loss_dice: 2.7062  d3.loss_cls: 0.7284  d3.loss_mask: 0.2742  d3.loss_dice: 2.6520  d4.loss_cls: 0.7349  d4.loss_mask: 0.2689  d4.loss_dice: 2.6266  d5.loss_cls: 0.7385  d5.loss_mask: 0.2665  d5.loss_dice: 2.6245  d6.loss_cls: 0.7354  d6.loss_mask: 0.2662  d6.loss_dice: 2.6257  d7.loss_cls: 0.7379  d7.loss_mask: 0.2669  d7.loss_dice: 2.6319  d8.loss_cls: 0.7414  d8.loss_mask: 0.2661  d8.loss_dice: 2.6167
05/11 06:31:29 - mmengine - INFO - Iter(train) [ 1100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:47:06  time: 1.5383  data_time: 0.0291  memory: 39622  grad_norm: 62.6964  loss: 37.1029  loss_cls: 0.7495  loss_mask: 0.2765  loss_dice: 2.5885  d0.loss_cls: 0.7566  d0.loss_mask: 0.3275  d0.loss_dice: 3.1650  d1.loss_cls: 0.7316  d1.loss_mask: 0.2894  d1.loss_dice: 2.7876  d2.loss_cls: 0.7567  d2.loss_mask: 0.2864  d2.loss_dice: 2.6688  d3.loss_cls: 0.7488  d3.loss_mask: 0.2812  d3.loss_dice: 2.6248  d4.loss_cls: 0.7490  d4.loss_mask: 0.2797  d4.loss_dice: 2.5831  d5.loss_cls: 0.7476  d5.loss_mask: 0.2796  d5.loss_dice: 2.5863  d6.loss_cls: 0.7431  d6.loss_mask: 0.2788  d6.loss_dice: 2.5899  d7.loss_cls: 0.7434  d7.loss_mask: 0.2782  d7.loss_dice: 2.5981  d8.loss_cls: 0.7451  d8.loss_mask: 0.2768  d8.loss_dice: 2.5854
05/11 06:32:46 - mmengine - INFO - Iter(train) [ 1150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:45:45  time: 1.5334  data_time: 0.0295  memory: 39327  grad_norm: 60.4006  loss: 37.5086  loss_cls: 0.7541  loss_mask: 0.2795  loss_dice: 2.6270  d0.loss_cls: 0.7380  d0.loss_mask: 0.3259  d0.loss_dice: 3.1961  d1.loss_cls: 0.7509  d1.loss_mask: 0.2956  d1.loss_dice: 2.8171  d2.loss_cls: 0.7660  d2.loss_mask: 0.2914  d2.loss_dice: 2.7012  d3.loss_cls: 0.7572  d3.loss_mask: 0.2865  d3.loss_dice: 2.6463  d4.loss_cls: 0.7545  d4.loss_mask: 0.2828  d4.loss_dice: 2.6189  d5.loss_cls: 0.7594  d5.loss_mask: 0.2801  d5.loss_dice: 2.6217  d6.loss_cls: 0.7508  d6.loss_mask: 0.2789  d6.loss_dice: 2.6201  d7.loss_cls: 0.7491  d7.loss_mask: 0.2805  d7.loss_dice: 2.6291  d8.loss_cls: 0.7500  d8.loss_mask: 0.2796  d8.loss_dice: 2.6204
05/11 06:34:03 - mmengine - INFO - Iter(train) [ 1200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:44:26  time: 1.5360  data_time: 0.0296  memory: 39119  grad_norm: 56.5360  loss: 38.0015  loss_cls: 0.7710  loss_mask: 0.3012  loss_dice: 2.6365  d0.loss_cls: 0.7292  d0.loss_mask: 0.3546  d0.loss_dice: 3.2517  d1.loss_cls: 0.7599  d1.loss_mask: 0.3214  d1.loss_dice: 2.8400  d2.loss_cls: 0.7720  d2.loss_mask: 0.3150  d2.loss_dice: 2.7120  d3.loss_cls: 0.7637  d3.loss_mask: 0.3087  d3.loss_dice: 2.6653  d4.loss_cls: 0.7615  d4.loss_mask: 0.3031  d4.loss_dice: 2.6368  d5.loss_cls: 0.7595  d5.loss_mask: 0.3024  d5.loss_dice: 2.6349  d6.loss_cls: 0.7562  d6.loss_mask: 0.3027  d6.loss_dice: 2.6411  d7.loss_cls: 0.7587  d7.loss_mask: 0.3026  d7.loss_dice: 2.6442  d8.loss_cls: 0.7649  d8.loss_mask: 0.3019  d8.loss_dice: 2.6291
05/11 06:35:19 - mmengine - INFO - Iter(train) [ 1250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:42:59  time: 1.5272  data_time: 0.0300  memory: 38801  grad_norm: 53.3676  loss: 35.5408  loss_cls: 0.7406  loss_mask: 0.2675  loss_dice: 2.4503  d0.loss_cls: 0.7312  d0.loss_mask: 0.3242  d0.loss_dice: 3.0455  d1.loss_cls: 0.7560  d1.loss_mask: 0.2837  d1.loss_dice: 2.6307  d2.loss_cls: 0.7568  d2.loss_mask: 0.2774  d2.loss_dice: 2.5294  d3.loss_cls: 0.7452  d3.loss_mask: 0.2743  d3.loss_dice: 2.4729  d4.loss_cls: 0.7394  d4.loss_mask: 0.2699  d4.loss_dice: 2.4409  d5.loss_cls: 0.7380  d5.loss_mask: 0.2682  d5.loss_dice: 2.4467  d6.loss_cls: 0.7349  d6.loss_mask: 0.2676  d6.loss_dice: 2.4506  d7.loss_cls: 0.7370  d7.loss_mask: 0.2673  d7.loss_dice: 2.4506  d8.loss_cls: 0.7331  d8.loss_mask: 0.2679  d8.loss_dice: 2.4431
05/11 06:36:37 - mmengine - INFO - Iter(train) [ 1300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:41:59  time: 1.5562  data_time: 0.0299  memory: 38706  grad_norm: 53.7131  loss: 35.7279  loss_cls: 0.7652  loss_mask: 0.2631  loss_dice: 2.4433  d0.loss_cls: 0.7210  d0.loss_mask: 0.3091  d0.loss_dice: 3.0276  d1.loss_cls: 0.7880  d1.loss_mask: 0.2766  d1.loss_dice: 2.6223  d2.loss_cls: 0.7889  d2.loss_mask: 0.2738  d2.loss_dice: 2.5234  d3.loss_cls: 0.7766  d3.loss_mask: 0.2669  d3.loss_dice: 2.4795  d4.loss_cls: 0.7706  d4.loss_mask: 0.2643  d4.loss_dice: 2.4454  d5.loss_cls: 0.7684  d5.loss_mask: 0.2637  d5.loss_dice: 2.4538  d6.loss_cls: 0.7681  d6.loss_mask: 0.2620  d6.loss_dice: 2.4494  d7.loss_cls: 0.7662  d7.loss_mask: 0.2633  d7.loss_dice: 2.4565  d8.loss_cls: 0.7627  d8.loss_mask: 0.2641  d8.loss_dice: 2.4440
05/11 06:37:53 - mmengine - INFO - Iter(train) [ 1350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:40:33  time: 1.5281  data_time: 0.0289  memory: 38688  grad_norm: 52.2790  loss: 34.6899  loss_cls: 0.7418  loss_mask: 0.2701  loss_dice: 2.3445  d0.loss_cls: 0.7199  d0.loss_mask: 0.3230  d0.loss_dice: 2.9420  d1.loss_cls: 0.7928  d1.loss_mask: 0.2896  d1.loss_dice: 2.5358  d2.loss_cls: 0.7779  d2.loss_mask: 0.2807  d2.loss_dice: 2.4263  d3.loss_cls: 0.7680  d3.loss_mask: 0.2725  d3.loss_dice: 2.3763  d4.loss_cls: 0.7547  d4.loss_mask: 0.2701  d4.loss_dice: 2.3459  d5.loss_cls: 0.7548  d5.loss_mask: 0.2698  d5.loss_dice: 2.3438  d6.loss_cls: 0.7502  d6.loss_mask: 0.2697  d6.loss_dice: 2.3457  d7.loss_cls: 0.7465  d7.loss_mask: 0.2698  d7.loss_dice: 2.3510  d8.loss_cls: 0.7446  d8.loss_mask: 0.2693  d8.loss_dice: 2.3427
05/11 06:39:11 - mmengine - INFO - Iter(train) [ 1400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:39:27  time: 1.5514  data_time: 0.0360  memory: 38603  grad_norm: 52.6878  loss: 35.3184  loss_cls: 0.7568  loss_mask: 0.2638  loss_dice: 2.4046  d0.loss_cls: 0.7472  d0.loss_mask: 0.3209  d0.loss_dice: 2.9921  d1.loss_cls: 0.8057  d1.loss_mask: 0.2838  d1.loss_dice: 2.5758  d2.loss_cls: 0.7919  d2.loss_mask: 0.2768  d2.loss_dice: 2.4681  d3.loss_cls: 0.7755  d3.loss_mask: 0.2718  d3.loss_dice: 2.4309  d4.loss_cls: 0.7704  d4.loss_mask: 0.2649  d4.loss_dice: 2.3986  d5.loss_cls: 0.7675  d5.loss_mask: 0.2635  d5.loss_dice: 2.4002  d6.loss_cls: 0.7636  d6.loss_mask: 0.2640  d6.loss_dice: 2.4036  d7.loss_cls: 0.7622  d7.loss_mask: 0.2639  d7.loss_dice: 2.4122  d8.loss_cls: 0.7586  d8.loss_mask: 0.2628  d8.loss_dice: 2.3970
05/11 06:40:29 - mmengine - INFO - Iter(train) [ 1450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:38:29  time: 1.5630  data_time: 0.0298  memory: 39257  grad_norm: 55.4679  loss: 36.3514  loss_cls: 0.7876  loss_mask: 0.2659  loss_dice: 2.4796  d0.loss_cls: 0.7358  d0.loss_mask: 0.3266  d0.loss_dice: 3.0867  d1.loss_cls: 0.8360  d1.loss_mask: 0.2812  d1.loss_dice: 2.6542  d2.loss_cls: 0.8199  d2.loss_mask: 0.2743  d2.loss_dice: 2.5520  d3.loss_cls: 0.7971  d3.loss_mask: 0.2690  d3.loss_dice: 2.5094  d4.loss_cls: 0.7932  d4.loss_mask: 0.2656  d4.loss_dice: 2.4754  d5.loss_cls: 0.7919  d5.loss_mask: 0.2649  d5.loss_dice: 2.4812  d6.loss_cls: 0.7880  d6.loss_mask: 0.2641  d6.loss_dice: 2.4767  d7.loss_cls: 0.7832  d7.loss_mask: 0.2652  d7.loss_dice: 2.4891  d8.loss_cls: 0.7879  d8.loss_mask: 0.2660  d8.loss_dice: 2.4838
05/11 06:41:46 - mmengine - INFO - Iter(train) [ 1500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:37:13  time: 1.5396  data_time: 0.0297  memory: 38521  grad_norm: 55.7640  loss: 36.1275  loss_cls: 0.7518  loss_mask: 0.2658  loss_dice: 2.4622  d0.loss_cls: 0.7590  d0.loss_mask: 0.3233  d0.loss_dice: 3.1326  d1.loss_cls: 0.8317  d1.loss_mask: 0.2867  d1.loss_dice: 2.6613  d2.loss_cls: 0.7986  d2.loss_mask: 0.2790  d2.loss_dice: 2.5577  d3.loss_cls: 0.7677  d3.loss_mask: 0.2711  d3.loss_dice: 2.5041  d4.loss_cls: 0.7599  d4.loss_mask: 0.2670  d4.loss_dice: 2.4687  d5.loss_cls: 0.7545  d5.loss_mask: 0.2671  d5.loss_dice: 2.4748  d6.loss_cls: 0.7541  d6.loss_mask: 0.2662  d6.loss_dice: 2.4784  d7.loss_cls: 0.7549  d7.loss_mask: 0.2673  d7.loss_dice: 2.4764  d8.loss_cls: 0.7531  d8.loss_mask: 0.2669  d8.loss_dice: 2.4655
05/11 06:43:03 - mmengine - INFO - Iter(train) [ 1550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:35:54  time: 1.5370  data_time: 0.0295  memory: 38653  grad_norm: 47.1503  loss: 34.2724  loss_cls: 0.7500  loss_mask: 0.2697  loss_dice: 2.2974  d0.loss_cls: 0.7176  d0.loss_mask: 0.3276  d0.loss_dice: 2.8718  d1.loss_cls: 0.8376  d1.loss_mask: 0.2884  d1.loss_dice: 2.4423  d2.loss_cls: 0.7943  d2.loss_mask: 0.2800  d2.loss_dice: 2.3670  d3.loss_cls: 0.7684  d3.loss_mask: 0.2738  d3.loss_dice: 2.3257  d4.loss_cls: 0.7571  d4.loss_mask: 0.2708  d4.loss_dice: 2.3045  d5.loss_cls: 0.7575  d5.loss_mask: 0.2693  d5.loss_dice: 2.3071  d6.loss_cls: 0.7517  d6.loss_mask: 0.2702  d6.loss_dice: 2.3112  d7.loss_cls: 0.7535  d7.loss_mask: 0.2693  d7.loss_dice: 2.3128  d8.loss_cls: 0.7482  d8.loss_mask: 0.2702  d8.loss_dice: 2.3071
05/11 06:44:21 - mmengine - INFO - Iter(train) [ 1600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:34:51  time: 1.5589  data_time: 0.0298  memory: 39843  grad_norm: 54.5746  loss: 38.3997  loss_cls: 0.7670  loss_mask: 0.2900  loss_dice: 2.6582  d0.loss_cls: 0.7633  d0.loss_mask: 0.3553  d0.loss_dice: 3.3195  d1.loss_cls: 0.8384  d1.loss_mask: 0.3145  d1.loss_dice: 2.8353  d2.loss_cls: 0.8029  d2.loss_mask: 0.3024  d2.loss_dice: 2.7421  d3.loss_cls: 0.7793  d3.loss_mask: 0.2957  d3.loss_dice: 2.7009  d4.loss_cls: 0.7779  d4.loss_mask: 0.2906  d4.loss_dice: 2.6589  d5.loss_cls: 0.7747  d5.loss_mask: 0.2912  d5.loss_dice: 2.6673  d6.loss_cls: 0.7731  d6.loss_mask: 0.2921  d6.loss_dice: 2.6603  d7.loss_cls: 0.7672  d7.loss_mask: 0.2911  d7.loss_dice: 2.6699  d8.loss_cls: 0.7663  d8.loss_mask: 0.2915  d8.loss_dice: 2.6627
05/11 06:45:38 - mmengine - INFO - Iter(train) [ 1650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:33:30  time: 1.5336  data_time: 0.0299  memory: 38254  grad_norm: 45.3267  loss: 33.5228  loss_cls: 0.7210  loss_mask: 0.2467  loss_dice: 2.2631  d0.loss_cls: 0.7115  d0.loss_mask: 0.3035  d0.loss_dice: 2.8696  d1.loss_cls: 0.8276  d1.loss_mask: 0.2625  d1.loss_dice: 2.4228  d2.loss_cls: 0.7781  d2.loss_mask: 0.2561  d2.loss_dice: 2.3379  d3.loss_cls: 0.7524  d3.loss_mask: 0.2465  d3.loss_dice: 2.2933  d4.loss_cls: 0.7391  d4.loss_mask: 0.2453  d4.loss_dice: 2.2693  d5.loss_cls: 0.7338  d5.loss_mask: 0.2448  d5.loss_dice: 2.2682  d6.loss_cls: 0.7306  d6.loss_mask: 0.2452  d6.loss_dice: 2.2726  d7.loss_cls: 0.7231  d7.loss_mask: 0.2460  d7.loss_dice: 2.2737  d8.loss_cls: 0.7215  d8.loss_mask: 0.2471  d8.loss_dice: 2.2700
05/11 06:46:54 - mmengine - INFO - Iter(train) [ 1700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:32:09  time: 1.5343  data_time: 0.0312  memory: 39663  grad_norm: 44.5349  loss: 34.5119  loss_cls: 0.7416  loss_mask: 0.2481  loss_dice: 2.3535  d0.loss_cls: 0.6829  d0.loss_mask: 0.2969  d0.loss_dice: 2.9287  d1.loss_cls: 0.8442  d1.loss_mask: 0.2660  d1.loss_dice: 2.5014  d2.loss_cls: 0.7986  d2.loss_mask: 0.2602  d2.loss_dice: 2.4259  d3.loss_cls: 0.7726  d3.loss_mask: 0.2498  d3.loss_dice: 2.3757  d4.loss_cls: 0.7621  d4.loss_mask: 0.2489  d4.loss_dice: 2.3523  d5.loss_cls: 0.7560  d5.loss_mask: 0.2480  d5.loss_dice: 2.3480  d6.loss_cls: 0.7484  d6.loss_mask: 0.2495  d6.loss_dice: 2.3581  d7.loss_cls: 0.7437  d7.loss_mask: 0.2499  d7.loss_dice: 2.3580  d8.loss_cls: 0.7406  d8.loss_mask: 0.2483  d8.loss_dice: 2.3538
05/11 06:48:11 - mmengine - INFO - Iter(train) [ 1750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:30:48  time: 1.5326  data_time: 0.0302  memory: 38585  grad_norm: 48.1671  loss: 34.2380  loss_cls: 0.7194  loss_mask: 0.2427  loss_dice: 2.3458  d0.loss_cls: 0.7103  d0.loss_mask: 0.2924  d0.loss_dice: 2.9124  d1.loss_cls: 0.8322  d1.loss_mask: 0.2638  d1.loss_dice: 2.4782  d2.loss_cls: 0.7838  d2.loss_mask: 0.2533  d2.loss_dice: 2.4152  d3.loss_cls: 0.7577  d3.loss_mask: 0.2459  d3.loss_dice: 2.3797  d4.loss_cls: 0.7430  d4.loss_mask: 0.2427  d4.loss_dice: 2.3429  d5.loss_cls: 0.7307  d5.loss_mask: 0.2437  d5.loss_dice: 2.3507  d6.loss_cls: 0.7313  d6.loss_mask: 0.2430  d6.loss_dice: 2.3487  d7.loss_cls: 0.7244  d7.loss_mask: 0.2441  d7.loss_dice: 2.3529  d8.loss_cls: 0.7188  d8.loss_mask: 0.2423  d8.loss_dice: 2.3457
05/11 06:49:29 - mmengine - INFO - Iter(train) [ 1800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:29:40  time: 1.5533  data_time: 0.0302  memory: 38569  grad_norm: 51.4825  loss: 35.7928  loss_cls: 0.7366  loss_mask: 0.2588  loss_dice: 2.4765  d0.loss_cls: 0.6995  d0.loss_mask: 0.3184  d0.loss_dice: 3.0254  d1.loss_cls: 0.8494  d1.loss_mask: 0.2796  d1.loss_dice: 2.6010  d2.loss_cls: 0.8018  d2.loss_mask: 0.2695  d2.loss_dice: 2.5325  d3.loss_cls: 0.7721  d3.loss_mask: 0.2629  d3.loss_dice: 2.4954  d4.loss_cls: 0.7574  d4.loss_mask: 0.2604  d4.loss_dice: 2.4741  d5.loss_cls: 0.7446  d5.loss_mask: 0.2613  d5.loss_dice: 2.4764  d6.loss_cls: 0.7434  d6.loss_mask: 0.2622  d6.loss_dice: 2.4826  d7.loss_cls: 0.7365  d7.loss_mask: 0.2622  d7.loss_dice: 2.4857  d8.loss_cls: 0.7369  d8.loss_mask: 0.2585  d8.loss_dice: 2.4709
05/11 06:50:45 - mmengine - INFO - Iter(train) [ 1850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:28:21  time: 1.5369  data_time: 0.0302  memory: 38872  grad_norm: 48.4209  loss: 34.1848  loss_cls: 0.7354  loss_mask: 0.2353  loss_dice: 2.3247  d0.loss_cls: 0.7060  d0.loss_mask: 0.2902  d0.loss_dice: 2.9266  d1.loss_cls: 0.8559  d1.loss_mask: 0.2535  d1.loss_dice: 2.4760  d2.loss_cls: 0.8109  d2.loss_mask: 0.2442  d2.loss_dice: 2.3973  d3.loss_cls: 0.7779  d3.loss_mask: 0.2400  d3.loss_dice: 2.3639  d4.loss_cls: 0.7609  d4.loss_mask: 0.2363  d4.loss_dice: 2.3274  d5.loss_cls: 0.7493  d5.loss_mask: 0.2365  d5.loss_dice: 2.3256  d6.loss_cls: 0.7448  d6.loss_mask: 0.2359  d6.loss_dice: 2.3335  d7.loss_cls: 0.7394  d7.loss_mask: 0.2352  d7.loss_dice: 2.3314  d8.loss_cls: 0.7365  d8.loss_mask: 0.2354  d8.loss_dice: 2.3187
05/11 06:52:03 - mmengine - INFO - Iter(train) [ 1900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:27:07  time: 1.5448  data_time: 0.0294  memory: 39109  grad_norm: 45.3646  loss: 34.7676  loss_cls: 0.7240  loss_mask: 0.2646  loss_dice: 2.3550  d0.loss_cls: 0.7110  d0.loss_mask: 0.3362  d0.loss_dice: 2.9525  d1.loss_cls: 0.8704  d1.loss_mask: 0.2859  d1.loss_dice: 2.4968  d2.loss_cls: 0.8189  d2.loss_mask: 0.2736  d2.loss_dice: 2.4216  d3.loss_cls: 0.7875  d3.loss_mask: 0.2658  d3.loss_dice: 2.3864  d4.loss_cls: 0.7592  d4.loss_mask: 0.2664  d4.loss_dice: 2.3567  d5.loss_cls: 0.7414  d5.loss_mask: 0.2660  d5.loss_dice: 2.3632  d6.loss_cls: 0.7394  d6.loss_mask: 0.2646  d6.loss_dice: 2.3595  d7.loss_cls: 0.7246  d7.loss_mask: 0.2667  d7.loss_dice: 2.3635  d8.loss_cls: 0.7227  d8.loss_mask: 0.2643  d8.loss_dice: 2.3593
05/11 06:53:20 - mmengine - INFO - Iter(train) [ 1950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:25:53  time: 1.5449  data_time: 0.0292  memory: 38677  grad_norm: 50.2937  loss: 35.5965  loss_cls: 0.7278  loss_mask: 0.2661  loss_dice: 2.4158  d0.loss_cls: 0.7148  d0.loss_mask: 0.3315  d0.loss_dice: 3.0988  d1.loss_cls: 0.8813  d1.loss_mask: 0.2913  d1.loss_dice: 2.5888  d2.loss_cls: 0.8198  d2.loss_mask: 0.2784  d2.loss_dice: 2.5022  d3.loss_cls: 0.7868  d3.loss_mask: 0.2696  d3.loss_dice: 2.4598  d4.loss_cls: 0.7640  d4.loss_mask: 0.2680  d4.loss_dice: 2.4223  d5.loss_cls: 0.7460  d5.loss_mask: 0.2687  d5.loss_dice: 2.4246  d6.loss_cls: 0.7397  d6.loss_mask: 0.2684  d6.loss_dice: 2.4314  d7.loss_cls: 0.7265  d7.loss_mask: 0.2680  d7.loss_dice: 2.4295  d8.loss_cls: 0.7258  d8.loss_mask: 0.2677  d8.loss_dice: 2.4131
05/11 06:54:37 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 06:54:37 - mmengine - INFO - Iter(train) [ 2000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:24:39  time: 1.5462  data_time: 0.0293  memory: 38793  grad_norm: 46.7100  loss: 36.3359  loss_cls: 0.7286  loss_mask: 0.2533  loss_dice: 2.5080  d0.loss_cls: 0.7123  d0.loss_mask: 0.3144  d0.loss_dice: 3.1781  d1.loss_cls: 0.8868  d1.loss_mask: 0.2767  d1.loss_dice: 2.6562  d2.loss_cls: 0.8288  d2.loss_mask: 0.2647  d2.loss_dice: 2.5836  d3.loss_cls: 0.7897  d3.loss_mask: 0.2575  d3.loss_dice: 2.5426  d4.loss_cls: 0.7588  d4.loss_mask: 0.2558  d4.loss_dice: 2.5075  d5.loss_cls: 0.7448  d5.loss_mask: 0.2544  d5.loss_dice: 2.5166  d6.loss_cls: 0.7413  d6.loss_mask: 0.2551  d6.loss_dice: 2.5234  d7.loss_cls: 0.7322  d7.loss_mask: 0.2546  d7.loss_dice: 2.5206  d8.loss_cls: 0.7295  d8.loss_mask: 0.2531  d8.loss_dice: 2.5069
05/11 06:54:37 - mmengine - INFO - Saving checkpoint at 2000 iterations
05/11 06:55:20 - mmengine - INFO - Iter(val) [ 50/100]    eta: 0:00:36  time: 0.7353  data_time: 0.0158  memory: 5704  
05/11 06:55:56 - mmengine - INFO - Iter(val) [100/100]    eta: 0:00:00  time: 0.7167  data_time: 0.0134  memory: 5704  
05/11 06:56:19 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.42s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/11 06:56:28 - mmengine - INFO - start multi processing evaluation ...

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

  0%|          | 0/25552 [00:00<?, ?it/s]
 48%|████▊     | 12308/25552 [00:00<00:00, 123073.66it/s]
100%|██████████| 25552/25552 [00:00<00:00, 133879.88it/s]
DONE (t=50.76s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.239
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.500
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.100
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.347
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.489
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.440
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.812
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.430
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.280
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.517
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.883
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.440
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.440
05/11 06:57:18 - mmengine - INFO - segm_mAP_copypaste: 0.239 0.500 0.226 0.100 0.347 0.489
05/11 06:57:18 - mmengine - INFO - segm_mAR_copypaste: 0.440 0.812 0.430 0.280 0.517 0.883
05/11 06:57:19 - mmengine - INFO - Iter(val) [100/100]    coco/segm_mAP: 0.2390  coco/segm_mAP_50: 0.5000  coco/segm_mAP_75: 0.2260  coco/segm_mAP_s: 0.1000  coco/segm_mAP_m: 0.3470  coco/segm_mAP_l: 0.4890  data_time: 0.0146  time: 0.7260
05/11 06:57:22 - mmengine - INFO - The best checkpoint with 0.5000 coco/segm_mAP_50 at 2000 iter is saved to best_coco_segm_mAP_50_iter_2000.pth.
05/11 06:58:44 - mmengine - INFO - Iter(train) [ 2050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:39:38  time: 3.3621  data_time: 1.8635  memory: 39087  grad_norm: 43.6311  loss: 34.7238  loss_cls: 0.7029  loss_mask: 0.2561  loss_dice: 2.3585  d0.loss_cls: 0.6923  d0.loss_mask: 0.3274  d0.loss_dice: 3.0196  d1.loss_cls: 0.8806  d1.loss_mask: 0.2789  d1.loss_dice: 2.5220  d2.loss_cls: 0.8191  d2.loss_mask: 0.2657  d2.loss_dice: 2.4442  d3.loss_cls: 0.7832  d3.loss_mask: 0.2605  d3.loss_dice: 2.4018  d4.loss_cls: 0.7448  d4.loss_mask: 0.2589  d4.loss_dice: 2.3626  d5.loss_cls: 0.7254  d5.loss_mask: 0.2593  d5.loss_dice: 2.3602  d6.loss_cls: 0.7184  d6.loss_mask: 0.2581  d6.loss_dice: 2.3670  d7.loss_cls: 0.7061  d7.loss_mask: 0.2588  d7.loss_dice: 2.3730  d8.loss_cls: 0.7017  d8.loss_mask: 0.2574  d8.loss_dice: 2.3593
05/11 07:00:02 - mmengine - INFO - Iter(train) [ 2100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:37:59  time: 1.5476  data_time: 0.0295  memory: 39183  grad_norm: 49.7948  loss: 36.3094  loss_cls: 0.7107  loss_mask: 0.2597  loss_dice: 2.5081  d0.loss_cls: 0.7236  d0.loss_mask: 0.3229  d0.loss_dice: 3.1879  d1.loss_cls: 0.8822  d1.loss_mask: 0.2813  d1.loss_dice: 2.6572  d2.loss_cls: 0.8302  d2.loss_mask: 0.2669  d2.loss_dice: 2.5831  d3.loss_cls: 0.7937  d3.loss_mask: 0.2623  d3.loss_dice: 2.5412  d4.loss_cls: 0.7487  d4.loss_mask: 0.2599  d4.loss_dice: 2.5124  d5.loss_cls: 0.7247  d5.loss_mask: 0.2601  d5.loss_dice: 2.5162  d6.loss_cls: 0.7261  d6.loss_mask: 0.2598  d6.loss_dice: 2.5156  d7.loss_cls: 0.7116  d7.loss_mask: 0.2607  d7.loss_dice: 2.5199  d8.loss_cls: 0.7153  d8.loss_mask: 0.2598  d8.loss_dice: 2.5076
05/11 07:01:19 - mmengine - INFO - Iter(train) [ 2150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:36:20  time: 1.5445  data_time: 0.0291  memory: 38700  grad_norm: 44.4945  loss: 35.5281  loss_cls: 0.6930  loss_mask: 0.2572  loss_dice: 2.4426  d0.loss_cls: 0.6917  d0.loss_mask: 0.3268  d0.loss_dice: 3.1075  d1.loss_cls: 0.8891  d1.loss_mask: 0.2813  d1.loss_dice: 2.5946  d2.loss_cls: 0.8281  d2.loss_mask: 0.2661  d2.loss_dice: 2.5173  d3.loss_cls: 0.7951  d3.loss_mask: 0.2589  d3.loss_dice: 2.4801  d4.loss_cls: 0.7401  d4.loss_mask: 0.2573  d4.loss_dice: 2.4521  d5.loss_cls: 0.7128  d5.loss_mask: 0.2592  d5.loss_dice: 2.4548  d6.loss_cls: 0.7076  d6.loss_mask: 0.2582  d6.loss_dice: 2.4533  d7.loss_cls: 0.6924  d7.loss_mask: 0.2587  d7.loss_dice: 2.4557  d8.loss_cls: 0.6967  d8.loss_mask: 0.2571  d8.loss_dice: 2.4426
05/11 07:02:36 - mmengine - INFO - Iter(train) [ 2200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:34:37  time: 1.5336  data_time: 0.0360  memory: 38981  grad_norm: 42.3367  loss: 31.2867  loss_cls: 0.6640  loss_mask: 0.2387  loss_dice: 2.0811  d0.loss_cls: 0.7040  d0.loss_mask: 0.3039  d0.loss_dice: 2.6301  d1.loss_cls: 0.8679  d1.loss_mask: 0.2589  d1.loss_dice: 2.2105  d2.loss_cls: 0.7926  d2.loss_mask: 0.2469  d2.loss_dice: 2.1479  d3.loss_cls: 0.7565  d3.loss_mask: 0.2406  d3.loss_dice: 2.1169  d4.loss_cls: 0.7043  d4.loss_mask: 0.2409  d4.loss_dice: 2.0869  d5.loss_cls: 0.6856  d5.loss_mask: 0.2381  d5.loss_dice: 2.0882  d6.loss_cls: 0.6784  d6.loss_mask: 0.2402  d6.loss_dice: 2.0914  d7.loss_cls: 0.6691  d7.loss_mask: 0.2382  d7.loss_dice: 2.0877  d8.loss_cls: 0.6638  d8.loss_mask: 0.2385  d8.loss_dice: 2.0747
05/11 07:03:52 - mmengine - INFO - Iter(train) [ 2250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:32:56  time: 1.5373  data_time: 0.0285  memory: 38737  grad_norm: 40.7242  loss: 29.7537  loss_cls: 0.6378  loss_mask: 0.2114  loss_dice: 1.9791  d0.loss_cls: 0.6983  d0.loss_mask: 0.2745  d0.loss_dice: 2.5223  d1.loss_cls: 0.8559  d1.loss_mask: 0.2327  d1.loss_dice: 2.0932  d2.loss_cls: 0.7830  d2.loss_mask: 0.2196  d2.loss_dice: 2.0403  d3.loss_cls: 0.7424  d3.loss_mask: 0.2148  d3.loss_dice: 2.0024  d4.loss_cls: 0.6719  d4.loss_mask: 0.2141  d4.loss_dice: 1.9876  d5.loss_cls: 0.6522  d5.loss_mask: 0.2142  d5.loss_dice: 1.9842  d6.loss_cls: 0.6478  d6.loss_mask: 0.2148  d6.loss_dice: 1.9920  d7.loss_cls: 0.6382  d7.loss_mask: 0.2132  d7.loss_dice: 1.9898  d8.loss_cls: 0.6348  d8.loss_mask: 0.2122  d8.loss_dice: 1.9788
05/11 07:05:09 - mmengine - INFO - Iter(train) [ 2300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:31:16  time: 1.5345  data_time: 0.0289  memory: 39497  grad_norm: 37.4787  loss: 34.2056  loss_cls: 0.6697  loss_mask: 0.2312  loss_dice: 2.3663  d0.loss_cls: 0.6830  d0.loss_mask: 0.2914  d0.loss_dice: 3.0272  d1.loss_cls: 0.8766  d1.loss_mask: 0.2524  d1.loss_dice: 2.5081  d2.loss_cls: 0.8124  d2.loss_mask: 0.2409  d2.loss_dice: 2.4258  d3.loss_cls: 0.7705  d3.loss_mask: 0.2349  d3.loss_dice: 2.3956  d4.loss_cls: 0.7058  d4.loss_mask: 0.2341  d4.loss_dice: 2.3684  d5.loss_cls: 0.6816  d5.loss_mask: 0.2339  d5.loss_dice: 2.3799  d6.loss_cls: 0.6788  d6.loss_mask: 0.2325  d6.loss_dice: 2.3756  d7.loss_cls: 0.6669  d7.loss_mask: 0.2318  d7.loss_dice: 2.3709  d8.loss_cls: 0.6650  d8.loss_mask: 0.2315  d8.loss_dice: 2.3631
05/11 07:06:27 - mmengine - INFO - Iter(train) [ 2350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:29:46  time: 1.5553  data_time: 0.0315  memory: 38438  grad_norm: 43.1546  loss: 33.5367  loss_cls: 0.6576  loss_mask: 0.2392  loss_dice: 2.3068  d0.loss_cls: 0.6954  d0.loss_mask: 0.3073  d0.loss_dice: 2.9162  d1.loss_cls: 0.8795  d1.loss_mask: 0.2671  d1.loss_dice: 2.4326  d2.loss_cls: 0.8087  d2.loss_mask: 0.2495  d2.loss_dice: 2.3667  d3.loss_cls: 0.7655  d3.loss_mask: 0.2428  d3.loss_dice: 2.3257  d4.loss_cls: 0.6932  d4.loss_mask: 0.2382  d4.loss_dice: 2.3041  d5.loss_cls: 0.6743  d5.loss_mask: 0.2385  d5.loss_dice: 2.3042  d6.loss_cls: 0.6724  d6.loss_mask: 0.2386  d6.loss_dice: 2.3077  d7.loss_cls: 0.6593  d7.loss_mask: 0.2383  d7.loss_dice: 2.3049  d8.loss_cls: 0.6609  d8.loss_mask: 0.2375  d8.loss_dice: 2.3040
05/11 07:07:44 - mmengine - INFO - Iter(train) [ 2400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:28:08  time: 1.5381  data_time: 0.0243  memory: 38837  grad_norm: 39.1691  loss: 33.3095  loss_cls: 0.6556  loss_mask: 0.2354  loss_dice: 2.2837  d0.loss_cls: 0.6990  d0.loss_mask: 0.2984  d0.loss_dice: 2.9219  d1.loss_cls: 0.8697  d1.loss_mask: 0.2582  d1.loss_dice: 2.4211  d2.loss_cls: 0.8039  d2.loss_mask: 0.2431  d2.loss_dice: 2.3612  d3.loss_cls: 0.7709  d3.loss_mask: 0.2364  d3.loss_dice: 2.3093  d4.loss_cls: 0.6907  d4.loss_mask: 0.2351  d4.loss_dice: 2.2865  d5.loss_cls: 0.6709  d5.loss_mask: 0.2360  d5.loss_dice: 2.2848  d6.loss_cls: 0.6665  d6.loss_mask: 0.2352  d6.loss_dice: 2.2834  d7.loss_cls: 0.6580  d7.loss_mask: 0.2351  d7.loss_dice: 2.2880  d8.loss_cls: 0.6557  d8.loss_mask: 0.2348  d8.loss_dice: 2.2808
05/11 07:09:00 - mmengine - INFO - Iter(train) [ 2450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:26:29  time: 1.5315  data_time: 0.0265  memory: 39202  grad_norm: 39.8639  loss: 33.8974  loss_cls: 0.6447  loss_mask: 0.2486  loss_dice: 2.3289  d0.loss_cls: 0.6701  d0.loss_mask: 0.3216  d0.loss_dice: 2.9932  d1.loss_cls: 0.8901  d1.loss_mask: 0.2734  d1.loss_dice: 2.4648  d2.loss_cls: 0.8182  d2.loss_mask: 0.2590  d2.loss_dice: 2.3907  d3.loss_cls: 0.7715  d3.loss_mask: 0.2522  d3.loss_dice: 2.3567  d4.loss_cls: 0.6859  d4.loss_mask: 0.2504  d4.loss_dice: 2.3299  d5.loss_cls: 0.6632  d5.loss_mask: 0.2518  d5.loss_dice: 2.3317  d6.loss_cls: 0.6608  d6.loss_mask: 0.2502  d6.loss_dice: 2.3346  d7.loss_cls: 0.6488  d7.loss_mask: 0.2509  d7.loss_dice: 2.3336  d8.loss_cls: 0.6478  d8.loss_mask: 0.2487  d8.loss_dice: 2.3255
05/11 07:10:18 - mmengine - INFO - Iter(train) [ 2500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:24:55  time: 1.5427  data_time: 0.0257  memory: 38955  grad_norm: 43.3014  loss: 31.7397  loss_cls: 0.6470  loss_mask: 0.2220  loss_dice: 2.1551  d0.loss_cls: 0.6887  d0.loss_mask: 0.2837  d0.loss_dice: 2.7280  d1.loss_cls: 0.8800  d1.loss_mask: 0.2426  d1.loss_dice: 2.2771  d2.loss_cls: 0.8074  d2.loss_mask: 0.2292  d2.loss_dice: 2.2066  d3.loss_cls: 0.7639  d3.loss_mask: 0.2244  d3.loss_dice: 2.1734  d4.loss_cls: 0.6770  d4.loss_mask: 0.2235  d4.loss_dice: 2.1537  d5.loss_cls: 0.6594  d5.loss_mask: 0.2225  d5.loss_dice: 2.1644  d6.loss_cls: 0.6543  d6.loss_mask: 0.2238  d6.loss_dice: 2.1656  d7.loss_cls: 0.6459  d7.loss_mask: 0.2230  d7.loss_dice: 2.1694  d8.loss_cls: 0.6502  d8.loss_mask: 0.2220  d8.loss_dice: 2.1558
05/11 07:11:34 - mmengine - INFO - Iter(train) [ 2550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:23:18  time: 1.5324  data_time: 0.0315  memory: 38314  grad_norm: 40.6451  loss: 31.9644  loss_cls: 0.6493  loss_mask: 0.2205  loss_dice: 2.1745  d0.loss_cls: 0.6739  d0.loss_mask: 0.2830  d0.loss_dice: 2.7788  d1.loss_cls: 0.8838  d1.loss_mask: 0.2444  d1.loss_dice: 2.2992  d2.loss_cls: 0.8059  d2.loss_mask: 0.2314  d2.loss_dice: 2.2421  d3.loss_cls: 0.7618  d3.loss_mask: 0.2258  d3.loss_dice: 2.2085  d4.loss_cls: 0.6756  d4.loss_mask: 0.2210  d4.loss_dice: 2.1768  d5.loss_cls: 0.6576  d5.loss_mask: 0.2224  d5.loss_dice: 2.1787  d6.loss_cls: 0.6586  d6.loss_mask: 0.2229  d6.loss_dice: 2.1766  d7.loss_cls: 0.6459  d7.loss_mask: 0.2221  d7.loss_dice: 2.1798  d8.loss_cls: 0.6485  d8.loss_mask: 0.2207  d8.loss_dice: 2.1742
05/11 07:12:52 - mmengine - INFO - Iter(train) [ 2600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:21:49  time: 1.5518  data_time: 0.0292  memory: 38580  grad_norm: 46.9951  loss: 33.6300  loss_cls: 0.6551  loss_mask: 0.2381  loss_dice: 2.3066  d0.loss_cls: 0.6681  d0.loss_mask: 0.3063  d0.loss_dice: 2.9456  d1.loss_cls: 0.9025  d1.loss_mask: 0.2704  d1.loss_dice: 2.4500  d2.loss_cls: 0.8247  d2.loss_mask: 0.2526  d2.loss_dice: 2.3767  d3.loss_cls: 0.7798  d3.loss_mask: 0.2450  d3.loss_dice: 2.3305  d4.loss_cls: 0.6900  d4.loss_mask: 0.2412  d4.loss_dice: 2.3092  d5.loss_cls: 0.6640  d5.loss_mask: 0.2423  d5.loss_dice: 2.3055  d6.loss_cls: 0.6633  d6.loss_mask: 0.2429  d6.loss_dice: 2.3123  d7.loss_cls: 0.6542  d7.loss_mask: 0.2410  d7.loss_dice: 2.3127  d8.loss_cls: 0.6551  d8.loss_mask: 0.2384  d8.loss_dice: 2.3057
05/11 07:14:09 - mmengine - INFO - Iter(train) [ 2650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:20:21  time: 1.5522  data_time: 0.0293  memory: 39341  grad_norm: 46.7406  loss: 33.9587  loss_cls: 0.6460  loss_mask: 0.2287  loss_dice: 2.3623  d0.loss_cls: 0.7135  d0.loss_mask: 0.2963  d0.loss_dice: 2.9862  d1.loss_cls: 0.8942  d1.loss_mask: 0.2584  d1.loss_dice: 2.4891  d2.loss_cls: 0.8133  d2.loss_mask: 0.2416  d2.loss_dice: 2.4241  d3.loss_cls: 0.7650  d3.loss_mask: 0.2365  d3.loss_dice: 2.3897  d4.loss_cls: 0.6719  d4.loss_mask: 0.2324  d4.loss_dice: 2.3521  d5.loss_cls: 0.6539  d5.loss_mask: 0.2311  d5.loss_dice: 2.3619  d6.loss_cls: 0.6466  d6.loss_mask: 0.2321  d6.loss_dice: 2.3656  d7.loss_cls: 0.6423  d7.loss_mask: 0.2299  d7.loss_dice: 2.3663  d8.loss_cls: 0.6441  d8.loss_mask: 0.2290  d8.loss_dice: 2.3546
05/11 07:15:26 - mmengine - INFO - Iter(train) [ 2700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:18:49  time: 1.5423  data_time: 0.0288  memory: 39013  grad_norm: 36.8333  loss: 31.7977  loss_cls: 0.6258  loss_mask: 0.2305  loss_dice: 2.1658  d0.loss_cls: 0.6927  d0.loss_mask: 0.2975  d0.loss_dice: 2.7615  d1.loss_cls: 0.8761  d1.loss_mask: 0.2536  d1.loss_dice: 2.2896  d2.loss_cls: 0.7961  d2.loss_mask: 0.2409  d2.loss_dice: 2.2285  d3.loss_cls: 0.7494  d3.loss_mask: 0.2349  d3.loss_dice: 2.1836  d4.loss_cls: 0.6550  d4.loss_mask: 0.2326  d4.loss_dice: 2.1574  d5.loss_cls: 0.6373  d5.loss_mask: 0.2324  d5.loss_dice: 2.1654  d6.loss_cls: 0.6371  d6.loss_mask: 0.2322  d6.loss_dice: 2.1708  d7.loss_cls: 0.6251  d7.loss_mask: 0.2311  d7.loss_dice: 2.1737  d8.loss_cls: 0.6273  d8.loss_mask: 0.2293  d8.loss_dice: 2.1644
05/11 07:16:43 - mmengine - INFO - Iter(train) [ 2750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:17:13  time: 1.5282  data_time: 0.0289  memory: 38180  grad_norm: 38.7991  loss: 30.4329  loss_cls: 0.6264  loss_mask: 0.2299  loss_dice: 2.0333  d0.loss_cls: 0.6776  d0.loss_mask: 0.3038  d0.loss_dice: 2.6122  d1.loss_cls: 0.8792  d1.loss_mask: 0.2569  d1.loss_dice: 2.1550  d2.loss_cls: 0.7863  d2.loss_mask: 0.2413  d2.loss_dice: 2.0989  d3.loss_cls: 0.7383  d3.loss_mask: 0.2352  d3.loss_dice: 2.0579  d4.loss_cls: 0.6487  d4.loss_mask: 0.2317  d4.loss_dice: 2.0320  d5.loss_cls: 0.6262  d5.loss_mask: 0.2335  d5.loss_dice: 2.0453  d6.loss_cls: 0.6279  d6.loss_mask: 0.2321  d6.loss_dice: 2.0360  d7.loss_cls: 0.6223  d7.loss_mask: 0.2305  d7.loss_dice: 2.0452  d8.loss_cls: 0.6219  d8.loss_mask: 0.2306  d8.loss_dice: 2.0367
05/11 07:18:00 - mmengine - INFO - Iter(train) [ 2800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:15:45  time: 1.5499  data_time: 0.0351  memory: 39112  grad_norm: 35.9783  loss: 31.6693  loss_cls: 0.6346  loss_mask: 0.2201  loss_dice: 2.1525  d0.loss_cls: 0.6841  d0.loss_mask: 0.2852  d0.loss_dice: 2.7486  d1.loss_cls: 0.9011  d1.loss_mask: 0.2440  d1.loss_dice: 2.2839  d2.loss_cls: 0.8113  d2.loss_mask: 0.2333  d2.loss_dice: 2.2192  d3.loss_cls: 0.7573  d3.loss_mask: 0.2255  d3.loss_dice: 2.1702  d4.loss_cls: 0.6601  d4.loss_mask: 0.2232  d4.loss_dice: 2.1555  d5.loss_cls: 0.6430  d5.loss_mask: 0.2236  d5.loss_dice: 2.1527  d6.loss_cls: 0.6427  d6.loss_mask: 0.2229  d6.loss_dice: 2.1538  d7.loss_cls: 0.6316  d7.loss_mask: 0.2214  d7.loss_dice: 2.1629  d8.loss_cls: 0.6339  d8.loss_mask: 0.2203  d8.loss_dice: 2.1509
05/11 07:19:17 - mmengine - INFO - Iter(train) [ 2850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:14:13  time: 1.5348  data_time: 0.0312  memory: 38754  grad_norm: 40.0831  loss: 30.6034  loss_cls: 0.6178  loss_mask: 0.2171  loss_dice: 2.0806  d0.loss_cls: 0.7132  d0.loss_mask: 0.2775  d0.loss_dice: 2.6250  d1.loss_cls: 0.8682  d1.loss_mask: 0.2403  d1.loss_dice: 2.1881  d2.loss_cls: 0.7901  d2.loss_mask: 0.2251  d2.loss_dice: 2.1279  d3.loss_cls: 0.7315  d3.loss_mask: 0.2236  d3.loss_dice: 2.0878  d4.loss_cls: 0.6396  d4.loss_mask: 0.2181  d4.loss_dice: 2.0733  d5.loss_cls: 0.6234  d5.loss_mask: 0.2188  d5.loss_dice: 2.0773  d6.loss_cls: 0.6224  d6.loss_mask: 0.2190  d6.loss_dice: 2.0743  d7.loss_cls: 0.6171  d7.loss_mask: 0.2182  d7.loss_dice: 2.0842  d8.loss_cls: 0.6172  d8.loss_mask: 0.2156  d8.loss_dice: 2.0711
05/11 07:20:33 - mmengine - INFO - Iter(train) [ 2900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:12:38  time: 1.5271  data_time: 0.0305  memory: 38166  grad_norm: 36.5654  loss: 30.6487  loss_cls: 0.6295  loss_mask: 0.2226  loss_dice: 2.0493  d0.loss_cls: 0.7027  d0.loss_mask: 0.3001  d0.loss_dice: 2.6651  d1.loss_cls: 0.8936  d1.loss_mask: 0.2480  d1.loss_dice: 2.1755  d2.loss_cls: 0.8025  d2.loss_mask: 0.2359  d2.loss_dice: 2.1142  d3.loss_cls: 0.7480  d3.loss_mask: 0.2301  d3.loss_dice: 2.0627  d4.loss_cls: 0.6509  d4.loss_mask: 0.2257  d4.loss_dice: 2.0548  d5.loss_cls: 0.6318  d5.loss_mask: 0.2254  d5.loss_dice: 2.0559  d6.loss_cls: 0.6283  d6.loss_mask: 0.2260  d6.loss_dice: 2.0635  d7.loss_cls: 0.6280  d7.loss_mask: 0.2235  d7.loss_dice: 2.0617  d8.loss_cls: 0.6276  d8.loss_mask: 0.2224  d8.loss_dice: 2.0435
05/11 07:21:50 - mmengine - INFO - Iter(train) [ 2950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:11:05  time: 1.5317  data_time: 0.0310  memory: 38733  grad_norm: 37.2434  loss: 33.4891  loss_cls: 0.6274  loss_mask: 0.2459  loss_dice: 2.3109  d0.loss_cls: 0.6982  d0.loss_mask: 0.3104  d0.loss_dice: 2.9476  d1.loss_cls: 0.8943  d1.loss_mask: 0.2723  d1.loss_dice: 2.4407  d2.loss_cls: 0.8028  d2.loss_mask: 0.2591  d2.loss_dice: 2.3752  d3.loss_cls: 0.7484  d3.loss_mask: 0.2523  d3.loss_dice: 2.3246  d4.loss_cls: 0.6544  d4.loss_mask: 0.2484  d4.loss_dice: 2.3025  d5.loss_cls: 0.6379  d5.loss_mask: 0.2492  d5.loss_dice: 2.3154  d6.loss_cls: 0.6371  d6.loss_mask: 0.2482  d6.loss_dice: 2.3108  d7.loss_cls: 0.6281  d7.loss_mask: 0.2469  d7.loss_dice: 2.3170  d8.loss_cls: 0.6296  d8.loss_mask: 0.2468  d8.loss_dice: 2.3068
05/11 07:23:06 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 07:23:06 - mmengine - INFO - Iter(train) [ 3000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:09:29  time: 1.5211  data_time: 0.0300  memory: 38532  grad_norm: 38.0073  loss: 30.4720  loss_cls: 0.6223  loss_mask: 0.2122  loss_dice: 2.0551  d0.loss_cls: 0.7157  d0.loss_mask: 0.2829  d0.loss_dice: 2.6428  d1.loss_cls: 0.8857  d1.loss_mask: 0.2403  d1.loss_dice: 2.1856  d2.loss_cls: 0.7930  d2.loss_mask: 0.2250  d2.loss_dice: 2.1217  d3.loss_cls: 0.7269  d3.loss_mask: 0.2197  d3.loss_dice: 2.0736  d4.loss_cls: 0.6359  d4.loss_mask: 0.2147  d4.loss_dice: 2.0560  d5.loss_cls: 0.6224  d5.loss_mask: 0.2153  d5.loss_dice: 2.0570  d6.loss_cls: 0.6188  d6.loss_mask: 0.2138  d6.loss_dice: 2.0635  d7.loss_cls: 0.6155  d7.loss_mask: 0.2124  d7.loss_dice: 2.0644  d8.loss_cls: 0.6193  d8.loss_mask: 0.2121  d8.loss_dice: 2.0486
05/11 07:24:23 - mmengine - INFO - Iter(train) [ 3050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:08:00  time: 1.5396  data_time: 0.0292  memory: 39185  grad_norm: 38.0734  loss: 34.3652  loss_cls: 0.6407  loss_mask: 0.2416  loss_dice: 2.3864  d0.loss_cls: 0.7291  d0.loss_mask: 0.3006  d0.loss_dice: 3.0179  d1.loss_cls: 0.8986  d1.loss_mask: 0.2719  d1.loss_dice: 2.5269  d2.loss_cls: 0.8084  d2.loss_mask: 0.2577  d2.loss_dice: 2.4619  d3.loss_cls: 0.7495  d3.loss_mask: 0.2503  d3.loss_dice: 2.4159  d4.loss_cls: 0.6605  d4.loss_mask: 0.2455  d4.loss_dice: 2.3944  d5.loss_cls: 0.6382  d5.loss_mask: 0.2453  d5.loss_dice: 2.3999  d6.loss_cls: 0.6399  d6.loss_mask: 0.2446  d6.loss_dice: 2.3995  d7.loss_cls: 0.6376  d7.loss_mask: 0.2424  d7.loss_dice: 2.3962  d8.loss_cls: 0.6412  d8.loss_mask: 0.2414  d8.loss_dice: 2.3813
05/11 07:25:40 - mmengine - INFO - Iter(train) [ 3100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:06:32  time: 1.5407  data_time: 0.0272  memory: 38816  grad_norm: 40.7767  loss: 30.1985  loss_cls: 0.6185  loss_mask: 0.2122  loss_dice: 2.0302  d0.loss_cls: 0.7155  d0.loss_mask: 0.2738  d0.loss_dice: 2.5770  d1.loss_cls: 0.8893  d1.loss_mask: 0.2387  d1.loss_dice: 2.1534  d2.loss_cls: 0.7958  d2.loss_mask: 0.2275  d2.loss_dice: 2.0868  d3.loss_cls: 0.7302  d3.loss_mask: 0.2188  d3.loss_dice: 2.0478  d4.loss_cls: 0.6371  d4.loss_mask: 0.2161  d4.loss_dice: 2.0361  d5.loss_cls: 0.6205  d5.loss_mask: 0.2165  d5.loss_dice: 2.0464  d6.loss_cls: 0.6207  d6.loss_mask: 0.2157  d6.loss_dice: 2.0407  d7.loss_cls: 0.6160  d7.loss_mask: 0.2148  d7.loss_dice: 2.0405  d8.loss_cls: 0.6209  d8.loss_mask: 0.2132  d8.loss_dice: 2.0280
05/11 07:26:57 - mmengine - INFO - Iter(train) [ 3150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:05:02  time: 1.5332  data_time: 0.0277  memory: 38348  grad_norm: 35.8433  loss: 30.4478  loss_cls: 0.6128  loss_mask: 0.2052  loss_dice: 2.0747  d0.loss_cls: 0.7208  d0.loss_mask: 0.2592  d0.loss_dice: 2.6193  d1.loss_cls: 0.8771  d1.loss_mask: 0.2256  d1.loss_dice: 2.2062  d2.loss_cls: 0.7834  d2.loss_mask: 0.2144  d2.loss_dice: 2.1438  d3.loss_cls: 0.7125  d3.loss_mask: 0.2101  d3.loss_dice: 2.0960  d4.loss_cls: 0.6248  d4.loss_mask: 0.2071  d4.loss_dice: 2.0775  d5.loss_cls: 0.6145  d5.loss_mask: 0.2053  d5.loss_dice: 2.0800  d6.loss_cls: 0.6150  d6.loss_mask: 0.2059  d6.loss_dice: 2.0744  d7.loss_cls: 0.6096  d7.loss_mask: 0.2051  d7.loss_dice: 2.0829  d8.loss_cls: 0.6155  d8.loss_mask: 0.2033  d8.loss_dice: 2.0655
05/11 07:28:13 - mmengine - INFO - Iter(train) [ 3200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:03:30  time: 1.5277  data_time: 0.0307  memory: 38850  grad_norm: 35.3119  loss: 33.7253  loss_cls: 0.6406  loss_mask: 0.2332  loss_dice: 2.3364  d0.loss_cls: 0.7402  d0.loss_mask: 0.2925  d0.loss_dice: 2.9484  d1.loss_cls: 0.8948  d1.loss_mask: 0.2598  d1.loss_dice: 2.4805  d2.loss_cls: 0.8112  d2.loss_mask: 0.2461  d2.loss_dice: 2.4081  d3.loss_cls: 0.7428  d3.loss_mask: 0.2390  d3.loss_dice: 2.3513  d4.loss_cls: 0.6554  d4.loss_mask: 0.2334  d4.loss_dice: 2.3490  d5.loss_cls: 0.6401  d5.loss_mask: 0.2336  d5.loss_dice: 2.3492  d6.loss_cls: 0.6408  d6.loss_mask: 0.2331  d6.loss_dice: 2.3470  d7.loss_cls: 0.6333  d7.loss_mask: 0.2323  d7.loss_dice: 2.3507  d8.loss_cls: 0.6376  d8.loss_mask: 0.2316  d8.loss_dice: 2.3335
05/11 07:29:04 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 07:29:30 - mmengine - INFO - Iter(train) [ 3250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:02:02  time: 1.5396  data_time: 0.0310  memory: 39144  grad_norm: 38.6960  loss: 31.7835  loss_cls: 0.6105  loss_mask: 0.2237  loss_dice: 2.1873  d0.loss_cls: 0.7343  d0.loss_mask: 0.2926  d0.loss_dice: 2.7395  d1.loss_cls: 0.8770  d1.loss_mask: 0.2526  d1.loss_dice: 2.3146  d2.loss_cls: 0.7892  d2.loss_mask: 0.2368  d2.loss_dice: 2.2489  d3.loss_cls: 0.7047  d3.loss_mask: 0.2315  d3.loss_dice: 2.1919  d4.loss_cls: 0.6211  d4.loss_mask: 0.2271  d4.loss_dice: 2.1948  d5.loss_cls: 0.6063  d5.loss_mask: 0.2267  d5.loss_dice: 2.2015  d6.loss_cls: 0.6102  d6.loss_mask: 0.2257  d6.loss_dice: 2.1960  d7.loss_cls: 0.6055  d7.loss_mask: 0.2236  d7.loss_dice: 2.1945  d8.loss_cls: 0.6092  d8.loss_mask: 0.2234  d8.loss_dice: 2.1830
05/11 07:30:47 - mmengine - INFO - Iter(train) [ 3300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 9:00:37  time: 1.5448  data_time: 0.0301  memory: 39350  grad_norm: 41.1685  loss: 32.0602  loss_cls: 0.6484  loss_mask: 0.2367  loss_dice: 2.1475  d0.loss_cls: 0.7147  d0.loss_mask: 0.3054  d0.loss_dice: 2.7846  d1.loss_cls: 0.9218  d1.loss_mask: 0.2630  d1.loss_dice: 2.3079  d2.loss_cls: 0.8202  d2.loss_mask: 0.2525  d2.loss_dice: 2.2320  d3.loss_cls: 0.7368  d3.loss_mask: 0.2460  d3.loss_dice: 2.1842  d4.loss_cls: 0.6607  d4.loss_mask: 0.2390  d4.loss_dice: 2.1729  d5.loss_cls: 0.6470  d5.loss_mask: 0.2382  d5.loss_dice: 2.1746  d6.loss_cls: 0.6444  d6.loss_mask: 0.2393  d6.loss_dice: 2.1757  d7.loss_cls: 0.6387  d7.loss_mask: 0.2376  d7.loss_dice: 2.1650  d8.loss_cls: 0.6450  d8.loss_mask: 0.2363  d8.loss_dice: 2.1442
05/11 07:32:04 - mmengine - INFO - Iter(train) [ 3350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:59:05  time: 1.5227  data_time: 0.0253  memory: 38846  grad_norm: 42.4841  loss: 32.4847  loss_cls: 0.6153  loss_mask: 0.2278  loss_dice: 2.2328  d0.loss_cls: 0.7631  d0.loss_mask: 0.2937  d0.loss_dice: 2.8252  d1.loss_cls: 0.8821  d1.loss_mask: 0.2597  d1.loss_dice: 2.3878  d2.loss_cls: 0.7857  d2.loss_mask: 0.2434  d2.loss_dice: 2.3161  d3.loss_cls: 0.7047  d3.loss_mask: 0.2347  d3.loss_dice: 2.2713  d4.loss_cls: 0.6274  d4.loss_mask: 0.2294  d4.loss_dice: 2.2504  d5.loss_cls: 0.6107  d5.loss_mask: 0.2300  d5.loss_dice: 2.2523  d6.loss_cls: 0.6110  d6.loss_mask: 0.2289  d6.loss_dice: 2.2479  d7.loss_cls: 0.6074  d7.loss_mask: 0.2288  d7.loss_dice: 2.2505  d8.loss_cls: 0.6128  d8.loss_mask: 0.2275  d8.loss_dice: 2.2264
05/11 07:33:21 - mmengine - INFO - Iter(train) [ 3400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:57:40  time: 1.5441  data_time: 0.0261  memory: 38837  grad_norm: 42.8324  loss: 32.9244  loss_cls: 0.6348  loss_mask: 0.2348  loss_dice: 2.2526  d0.loss_cls: 0.7507  d0.loss_mask: 0.2996  d0.loss_dice: 2.8488  d1.loss_cls: 0.8895  d1.loss_mask: 0.2657  d1.loss_dice: 2.4043  d2.loss_cls: 0.7938  d2.loss_mask: 0.2499  d2.loss_dice: 2.3322  d3.loss_cls: 0.7150  d3.loss_mask: 0.2428  d3.loss_dice: 2.2812  d4.loss_cls: 0.6439  d4.loss_mask: 0.2363  d4.loss_dice: 2.2794  d5.loss_cls: 0.6335  d5.loss_mask: 0.2358  d5.loss_dice: 2.2829  d6.loss_cls: 0.6377  d6.loss_mask: 0.2360  d6.loss_dice: 2.2822  d7.loss_cls: 0.6347  d7.loss_mask: 0.2344  d7.loss_dice: 2.2688  d8.loss_cls: 0.6385  d8.loss_mask: 0.2350  d8.loss_dice: 2.2496
05/11 07:34:39 - mmengine - INFO - Iter(train) [ 3450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:56:19  time: 1.5584  data_time: 0.0315  memory: 38727  grad_norm: 40.8226  loss: 31.7519  loss_cls: 0.6400  loss_mask: 0.2170  loss_dice: 2.1662  d0.loss_cls: 0.7431  d0.loss_mask: 0.2802  d0.loss_dice: 2.6945  d1.loss_cls: 0.8973  d1.loss_mask: 0.2437  d1.loss_dice: 2.2904  d2.loss_cls: 0.7951  d2.loss_mask: 0.2317  d2.loss_dice: 2.2253  d3.loss_cls: 0.7124  d3.loss_mask: 0.2249  d3.loss_dice: 2.1831  d4.loss_cls: 0.6496  d4.loss_mask: 0.2197  d4.loss_dice: 2.1849  d5.loss_cls: 0.6354  d5.loss_mask: 0.2189  d5.loss_dice: 2.1963  d6.loss_cls: 0.6349  d6.loss_mask: 0.2197  d6.loss_dice: 2.1859  d7.loss_cls: 0.6345  d7.loss_mask: 0.2188  d7.loss_dice: 2.1866  d8.loss_cls: 0.6410  d8.loss_mask: 0.2169  d8.loss_dice: 2.1641
05/11 07:35:56 - mmengine - INFO - Iter(train) [ 3500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:54:54  time: 1.5428  data_time: 0.0317  memory: 38940  grad_norm: 39.0085  loss: 31.1103  loss_cls: 0.6217  loss_mask: 0.2301  loss_dice: 2.0965  d0.loss_cls: 0.7259  d0.loss_mask: 0.2990  d0.loss_dice: 2.6570  d1.loss_cls: 0.8905  d1.loss_mask: 0.2599  d1.loss_dice: 2.2279  d2.loss_cls: 0.7974  d2.loss_mask: 0.2441  d2.loss_dice: 2.1612  d3.loss_cls: 0.7081  d3.loss_mask: 0.2381  d3.loss_dice: 2.1186  d4.loss_cls: 0.6320  d4.loss_mask: 0.2337  d4.loss_dice: 2.1151  d5.loss_cls: 0.6273  d5.loss_mask: 0.2330  d5.loss_dice: 2.1177  d6.loss_cls: 0.6261  d6.loss_mask: 0.2322  d6.loss_dice: 2.1095  d7.loss_cls: 0.6226  d7.loss_mask: 0.2315  d7.loss_dice: 2.1077  d8.loss_cls: 0.6253  d8.loss_mask: 0.2298  d8.loss_dice: 2.0910
05/11 07:37:13 - mmengine - INFO - Iter(train) [ 3550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:53:28  time: 1.5369  data_time: 0.0284  memory: 39670  grad_norm: 41.6155  loss: 33.2104  loss_cls: 0.6330  loss_mask: 0.2227  loss_dice: 2.2948  d0.loss_cls: 0.7645  d0.loss_mask: 0.2784  d0.loss_dice: 2.8821  d1.loss_cls: 0.9049  d1.loss_mask: 0.2476  d1.loss_dice: 2.4567  d2.loss_cls: 0.7972  d2.loss_mask: 0.2351  d2.loss_dice: 2.3791  d3.loss_cls: 0.7028  d3.loss_mask: 0.2310  d3.loss_dice: 2.3360  d4.loss_cls: 0.6398  d4.loss_mask: 0.2264  d4.loss_dice: 2.3168  d5.loss_cls: 0.6303  d5.loss_mask: 0.2241  d5.loss_dice: 2.3179  d6.loss_cls: 0.6306  d6.loss_mask: 0.2243  d6.loss_dice: 2.3199  d7.loss_cls: 0.6294  d7.loss_mask: 0.2220  d7.loss_dice: 2.3133  d8.loss_cls: 0.6335  d8.loss_mask: 0.2216  d8.loss_dice: 2.2944
05/11 07:38:29 - mmengine - INFO - Iter(train) [ 3600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:52:01  time: 1.5350  data_time: 0.0283  memory: 38404  grad_norm: 38.3889  loss: 28.7437  loss_cls: 0.5869  loss_mask: 0.2056  loss_dice: 1.9216  d0.loss_cls: 0.7464  d0.loss_mask: 0.2694  d0.loss_dice: 2.4268  d1.loss_cls: 0.8663  d1.loss_mask: 0.2357  d1.loss_dice: 2.0539  d2.loss_cls: 0.7560  d2.loss_mask: 0.2207  d2.loss_dice: 1.9986  d3.loss_cls: 0.6590  d3.loss_mask: 0.2125  d3.loss_dice: 1.9533  d4.loss_cls: 0.5945  d4.loss_mask: 0.2073  d4.loss_dice: 1.9389  d5.loss_cls: 0.5846  d5.loss_mask: 0.2071  d5.loss_dice: 1.9372  d6.loss_cls: 0.5811  d6.loss_mask: 0.2079  d6.loss_dice: 1.9395  d7.loss_cls: 0.5822  d7.loss_mask: 0.2064  d7.loss_dice: 1.9357  d8.loss_cls: 0.5858  d8.loss_mask: 0.2053  d8.loss_dice: 1.9174
05/11 07:39:47 - mmengine - INFO - Iter(train) [ 3650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:50:41  time: 1.5576  data_time: 0.0288  memory: 38451  grad_norm: 33.8456  loss: 30.5378  loss_cls: 0.6178  loss_mask: 0.2095  loss_dice: 2.0751  d0.loss_cls: 0.7705  d0.loss_mask: 0.2643  d0.loss_dice: 2.5725  d1.loss_cls: 0.8857  d1.loss_mask: 0.2347  d1.loss_dice: 2.1910  d2.loss_cls: 0.7807  d2.loss_mask: 0.2216  d2.loss_dice: 2.1354  d3.loss_cls: 0.6882  d3.loss_mask: 0.2159  d3.loss_dice: 2.0931  d4.loss_cls: 0.6266  d4.loss_mask: 0.2130  d4.loss_dice: 2.0918  d5.loss_cls: 0.6144  d5.loss_mask: 0.2128  d5.loss_dice: 2.0993  d6.loss_cls: 0.6174  d6.loss_mask: 0.2119  d6.loss_dice: 2.0915  d7.loss_cls: 0.6138  d7.loss_mask: 0.2103  d7.loss_dice: 2.0860  d8.loss_cls: 0.6194  d8.loss_mask: 0.2093  d8.loss_dice: 2.0643
05/11 07:41:04 - mmengine - INFO - Iter(train) [ 3700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:49:15  time: 1.5359  data_time: 0.0301  memory: 39495  grad_norm: 38.7230  loss: 31.1576  loss_cls: 0.6142  loss_mask: 0.2078  loss_dice: 2.1291  d0.loss_cls: 0.7672  d0.loss_mask: 0.2635  d0.loss_dice: 2.6459  d1.loss_cls: 0.8896  d1.loss_mask: 0.2362  d1.loss_dice: 2.2740  d2.loss_cls: 0.7813  d2.loss_mask: 0.2222  d2.loss_dice: 2.2068  d3.loss_cls: 0.6865  d3.loss_mask: 0.2148  d3.loss_dice: 2.1538  d4.loss_cls: 0.6289  d4.loss_mask: 0.2111  d4.loss_dice: 2.1503  d5.loss_cls: 0.6168  d5.loss_mask: 0.2105  d5.loss_dice: 2.1499  d6.loss_cls: 0.6172  d6.loss_mask: 0.2099  d6.loss_dice: 2.1526  d7.loss_cls: 0.6168  d7.loss_mask: 0.2088  d7.loss_dice: 2.1398  d8.loss_cls: 0.6163  d8.loss_mask: 0.2090  d8.loss_dice: 2.1269
05/11 07:42:21 - mmengine - INFO - Iter(train) [ 3750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:47:49  time: 1.5378  data_time: 0.0320  memory: 38990  grad_norm: 38.6803  loss: 32.0296  loss_cls: 0.6286  loss_mask: 0.2187  loss_dice: 2.1830  d0.loss_cls: 0.8033  d0.loss_mask: 0.2837  d0.loss_dice: 2.7382  d1.loss_cls: 0.8901  d1.loss_mask: 0.2525  d1.loss_dice: 2.3532  d2.loss_cls: 0.7790  d2.loss_mask: 0.2359  d2.loss_dice: 2.2741  d3.loss_cls: 0.6982  d3.loss_mask: 0.2256  d3.loss_dice: 2.2131  d4.loss_cls: 0.6389  d4.loss_mask: 0.2212  d4.loss_dice: 2.2069  d5.loss_cls: 0.6285  d5.loss_mask: 0.2216  d5.loss_dice: 2.2084  d6.loss_cls: 0.6275  d6.loss_mask: 0.2208  d6.loss_dice: 2.2063  d7.loss_cls: 0.6239  d7.loss_mask: 0.2199  d7.loss_dice: 2.2008  d8.loss_cls: 0.6266  d8.loss_mask: 0.2184  d8.loss_dice: 2.1828
05/11 07:43:38 - mmengine - INFO - Iter(train) [ 3800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:46:23  time: 1.5332  data_time: 0.0312  memory: 39550  grad_norm: 38.0959  loss: 32.5820  loss_cls: 0.6394  loss_mask: 0.2226  loss_dice: 2.2243  d0.loss_cls: 0.7950  d0.loss_mask: 0.2798  d0.loss_dice: 2.7755  d1.loss_cls: 0.9147  d1.loss_mask: 0.2577  d1.loss_dice: 2.3925  d2.loss_cls: 0.8001  d2.loss_mask: 0.2420  d2.loss_dice: 2.3214  d3.loss_cls: 0.7061  d3.loss_mask: 0.2326  d3.loss_dice: 2.2609  d4.loss_cls: 0.6531  d4.loss_mask: 0.2267  d4.loss_dice: 2.2449  d5.loss_cls: 0.6353  d5.loss_mask: 0.2254  d5.loss_dice: 2.2497  d6.loss_cls: 0.6396  d6.loss_mask: 0.2239  d6.loss_dice: 2.2441  d7.loss_cls: 0.6348  d7.loss_mask: 0.2221  d7.loss_dice: 2.2355  d8.loss_cls: 0.6414  d8.loss_mask: 0.2227  d8.loss_dice: 2.2181
05/11 07:44:55 - mmengine - INFO - Iter(train) [ 3850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:45:01  time: 1.5485  data_time: 0.0307  memory: 38550  grad_norm: 36.9505  loss: 31.0135  loss_cls: 0.6273  loss_mask: 0.2254  loss_dice: 2.0867  d0.loss_cls: 0.7888  d0.loss_mask: 0.2822  d0.loss_dice: 2.5845  d1.loss_cls: 0.8956  d1.loss_mask: 0.2505  d1.loss_dice: 2.2366  d2.loss_cls: 0.7740  d2.loss_mask: 0.2380  d2.loss_dice: 2.1702  d3.loss_cls: 0.6900  d3.loss_mask: 0.2316  d3.loss_dice: 2.1189  d4.loss_cls: 0.6332  d4.loss_mask: 0.2276  d4.loss_dice: 2.1112  d5.loss_cls: 0.6274  d5.loss_mask: 0.2282  d5.loss_dice: 2.1171  d6.loss_cls: 0.6305  d6.loss_mask: 0.2276  d6.loss_dice: 2.1150  d7.loss_cls: 0.6236  d7.loss_mask: 0.2263  d7.loss_dice: 2.1033  d8.loss_cls: 0.6308  d8.loss_mask: 0.2252  d8.loss_dice: 2.0860
05/11 07:46:12 - mmengine - INFO - Iter(train) [ 3900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:43:38  time: 1.5465  data_time: 0.0298  memory: 38372  grad_norm: 40.8387  loss: 29.4910  loss_cls: 0.6039  loss_mask: 0.2028  loss_dice: 1.9767  d0.loss_cls: 0.7758  d0.loss_mask: 0.2693  d0.loss_dice: 2.4597  d1.loss_cls: 0.8821  d1.loss_mask: 0.2354  d1.loss_dice: 2.1181  d2.loss_cls: 0.7706  d2.loss_mask: 0.2185  d2.loss_dice: 2.0519  d3.loss_cls: 0.6723  d3.loss_mask: 0.2121  d3.loss_dice: 2.0099  d4.loss_cls: 0.6130  d4.loss_mask: 0.2078  d4.loss_dice: 2.0050  d5.loss_cls: 0.6037  d5.loss_mask: 0.2072  d5.loss_dice: 2.0048  d6.loss_cls: 0.6065  d6.loss_mask: 0.2065  d6.loss_dice: 1.9999  d7.loss_cls: 0.6008  d7.loss_mask: 0.2054  d7.loss_dice: 1.9888  d8.loss_cls: 0.6035  d8.loss_mask: 0.2046  d8.loss_dice: 1.9742
05/11 07:47:29 - mmengine - INFO - Iter(train) [ 3950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:42:13  time: 1.5363  data_time: 0.0315  memory: 38930  grad_norm: 39.3838  loss: 31.0779  loss_cls: 0.6287  loss_mask: 0.2120  loss_dice: 2.1004  d0.loss_cls: 0.7737  d0.loss_mask: 0.2713  d0.loss_dice: 2.6181  d1.loss_cls: 0.8998  d1.loss_mask: 0.2425  d1.loss_dice: 2.2659  d2.loss_cls: 0.7709  d2.loss_mask: 0.2288  d2.loss_dice: 2.1930  d3.loss_cls: 0.6810  d3.loss_mask: 0.2237  d3.loss_dice: 2.1479  d4.loss_cls: 0.6319  d4.loss_mask: 0.2175  d4.loss_dice: 2.1346  d5.loss_cls: 0.6284  d5.loss_mask: 0.2162  d5.loss_dice: 2.1246  d6.loss_cls: 0.6295  d6.loss_mask: 0.2156  d6.loss_dice: 2.1252  d7.loss_cls: 0.6233  d7.loss_mask: 0.2137  d7.loss_dice: 2.1180  d8.loss_cls: 0.6299  d8.loss_mask: 0.2131  d8.loss_dice: 2.0987
05/11 07:48:46 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 07:48:46 - mmengine - INFO - Iter(train) [ 4000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:40:47  time: 1.5319  data_time: 0.0310  memory: 38990  grad_norm: 39.6821  loss: 33.2380  loss_cls: 0.6422  loss_mask: 0.2386  loss_dice: 2.2605  d0.loss_cls: 0.7906  d0.loss_mask: 0.3145  d0.loss_dice: 2.8363  d1.loss_cls: 0.9204  d1.loss_mask: 0.2756  d1.loss_dice: 2.4393  d2.loss_cls: 0.8048  d2.loss_mask: 0.2572  d2.loss_dice: 2.3588  d3.loss_cls: 0.7120  d3.loss_mask: 0.2462  d3.loss_dice: 2.3050  d4.loss_cls: 0.6625  d4.loss_mask: 0.2430  d4.loss_dice: 2.2845  d5.loss_cls: 0.6479  d5.loss_mask: 0.2418  d5.loss_dice: 2.2865  d6.loss_cls: 0.6481  d6.loss_mask: 0.2407  d6.loss_dice: 2.2856  d7.loss_cls: 0.6428  d7.loss_mask: 0.2386  d7.loss_dice: 2.2769  d8.loss_cls: 0.6445  d8.loss_mask: 0.2394  d8.loss_dice: 2.2531
05/11 07:48:46 - mmengine - INFO - Saving checkpoint at 4000 iterations
05/11 07:49:30 - mmengine - INFO - Iter(val) [ 50/100]    eta: 0:00:36  time: 0.7345  data_time: 0.0145  memory: 5704  
05/11 07:50:06 - mmengine - INFO - Iter(val) [100/100]    eta: 0:00:00  time: 0.7269  data_time: 0.0137  memory: 5704  
05/11 07:50:29 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.27s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/11 07:50:37 - mmengine - INFO - start multi processing evaluation ...

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

  0%|          | 0/25552 [00:00<?, ?it/s]
 50%|█████     | 12777/25552 [00:00<00:00, 125460.60it/s]
100%|██████████| 25552/25552 [00:00<00:00, 136056.92it/s]
DONE (t=52.30s).
Accumulating evaluation results...
DONE (t=0.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.302
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.632
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.240
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.153
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.407
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.784
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.460
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.812
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.422
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.324
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.521
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.460
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.460
05/11 07:51:29 - mmengine - INFO - segm_mAP_copypaste: 0.302 0.632 0.240 0.153 0.407 0.784
05/11 07:51:29 - mmengine - INFO - segm_mAR_copypaste: 0.460 0.812 0.422 0.324 0.521 0.900
05/11 07:51:30 - mmengine - INFO - Iter(val) [100/100]    coco/segm_mAP: 0.3020  coco/segm_mAP_50: 0.6320  coco/segm_mAP_75: 0.2400  coco/segm_mAP_s: 0.1530  coco/segm_mAP_m: 0.4070  coco/segm_mAP_l: 0.7840  data_time: 0.0141  time: 0.7292
05/11 07:51:30 - mmengine - INFO - The previous best checkpoint /data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-L_8xb32-24k_coco/best_coco_segm_mAP_50_iter_2000.pth is removed
05/11 07:51:32 - mmengine - INFO - The best checkpoint with 0.6320 coco/segm_mAP_50 at 4000 iter is saved to best_coco_segm_mAP_50_iter_4000.pth.
05/11 07:52:56 - mmengine - INFO - Iter(train) [ 4050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:47:00  time: 3.3936  data_time: 1.8887  memory: 39408  grad_norm: 43.4065  loss: 32.2780  loss_cls: 0.6293  loss_mask: 0.2334  loss_dice: 2.1826  d0.loss_cls: 0.8003  d0.loss_mask: 0.3085  d0.loss_dice: 2.7061  d1.loss_cls: 0.9035  d1.loss_mask: 0.2736  d1.loss_dice: 2.3731  d2.loss_cls: 0.7770  d2.loss_mask: 0.2542  d2.loss_dice: 2.2895  d3.loss_cls: 0.6835  d3.loss_mask: 0.2458  d3.loss_dice: 2.2352  d4.loss_cls: 0.6361  d4.loss_mask: 0.2386  d4.loss_dice: 2.2216  d5.loss_cls: 0.6301  d5.loss_mask: 0.2385  d5.loss_dice: 2.2213  d6.loss_cls: 0.6315  d6.loss_mask: 0.2373  d6.loss_dice: 2.2136  d7.loss_cls: 0.6266  d7.loss_mask: 0.2362  d7.loss_dice: 2.2060  d8.loss_cls: 0.6318  d8.loss_mask: 0.2332  d8.loss_dice: 2.1800
05/11 07:54:13 - mmengine - INFO - Iter(train) [ 4100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:45:30  time: 1.5410  data_time: 0.0285  memory: 39023  grad_norm: 39.5656  loss: 28.3811  loss_cls: 0.5824  loss_mask: 0.1922  loss_dice: 1.8878  d0.loss_cls: 0.7898  d0.loss_mask: 0.2529  d0.loss_dice: 2.3778  d1.loss_cls: 0.8683  d1.loss_mask: 0.2236  d1.loss_dice: 2.0569  d2.loss_cls: 0.7300  d2.loss_mask: 0.2109  d2.loss_dice: 1.9855  d3.loss_cls: 0.6438  d3.loss_mask: 0.2018  d3.loss_dice: 1.9302  d4.loss_cls: 0.5974  d4.loss_mask: 0.1959  d4.loss_dice: 1.9193  d5.loss_cls: 0.5808  d5.loss_mask: 0.1961  d5.loss_dice: 1.9213  d6.loss_cls: 0.5848  d6.loss_mask: 0.1953  d6.loss_dice: 1.9168  d7.loss_cls: 0.5801  d7.loss_mask: 0.1932  d7.loss_dice: 1.9092  d8.loss_cls: 0.5805  d8.loss_mask: 0.1917  d8.loss_dice: 1.8847
05/11 07:55:30 - mmengine - INFO - Iter(train) [ 4150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:44:04  time: 1.5529  data_time: 0.0296  memory: 38440  grad_norm: 49.1948  loss: 29.7208  loss_cls: 0.6171  loss_mask: 0.2114  loss_dice: 1.9770  d0.loss_cls: 0.8143  d0.loss_mask: 0.2846  d0.loss_dice: 2.4455  d1.loss_cls: 0.8845  d1.loss_mask: 0.2448  d1.loss_dice: 2.1309  d2.loss_cls: 0.7563  d2.loss_mask: 0.2280  d2.loss_dice: 2.0593  d3.loss_cls: 0.6657  d3.loss_mask: 0.2214  d3.loss_dice: 2.0246  d4.loss_cls: 0.6254  d4.loss_mask: 0.2169  d4.loss_dice: 2.0133  d5.loss_cls: 0.6179  d5.loss_mask: 0.2157  d5.loss_dice: 2.0057  d6.loss_cls: 0.6169  d6.loss_mask: 0.2177  d6.loss_dice: 2.0046  d7.loss_cls: 0.6100  d7.loss_mask: 0.2141  d7.loss_dice: 1.9926  d8.loss_cls: 0.6160  d8.loss_mask: 0.2116  d8.loss_dice: 1.9772
05/11 07:56:48 - mmengine - INFO - Iter(train) [ 4200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:42:35  time: 1.5445  data_time: 0.0298  memory: 38733  grad_norm: 38.2241  loss: 30.1615  loss_cls: 0.6057  loss_mask: 0.1995  loss_dice: 2.0462  d0.loss_cls: 0.8065  d0.loss_mask: 0.2622  d0.loss_dice: 2.4987  d1.loss_cls: 0.8714  d1.loss_mask: 0.2335  d1.loss_dice: 2.2046  d2.loss_cls: 0.7542  d2.loss_mask: 0.2163  d2.loss_dice: 2.1234  d3.loss_cls: 0.6577  d3.loss_mask: 0.2079  d3.loss_dice: 2.0974  d4.loss_cls: 0.6236  d4.loss_mask: 0.2038  d4.loss_dice: 2.0730  d5.loss_cls: 0.6054  d5.loss_mask: 0.2031  d5.loss_dice: 2.0736  d6.loss_cls: 0.6068  d6.loss_mask: 0.2025  d6.loss_dice: 2.0724  d7.loss_cls: 0.6033  d7.loss_mask: 0.2004  d7.loss_dice: 2.0637  d8.loss_cls: 0.6071  d8.loss_mask: 0.2000  d8.loss_dice: 2.0375
05/11 07:58:05 - mmengine - INFO - Iter(train) [ 4250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:41:07  time: 1.5454  data_time: 0.0301  memory: 39049  grad_norm: 32.7698  loss: 33.6817  loss_cls: 0.6385  loss_mask: 0.2216  loss_dice: 2.3388  d0.loss_cls: 0.8439  d0.loss_mask: 0.2796  d0.loss_dice: 2.8615  d1.loss_cls: 0.9038  d1.loss_mask: 0.2561  d1.loss_dice: 2.4976  d2.loss_cls: 0.7774  d2.loss_mask: 0.2388  d2.loss_dice: 2.4145  d3.loss_cls: 0.6883  d3.loss_mask: 0.2304  d3.loss_dice: 2.3756  d4.loss_cls: 0.6484  d4.loss_mask: 0.2256  d4.loss_dice: 2.3715  d5.loss_cls: 0.6340  d5.loss_mask: 0.2268  d5.loss_dice: 2.3686  d6.loss_cls: 0.6382  d6.loss_mask: 0.2258  d6.loss_dice: 2.3668  d7.loss_cls: 0.6335  d7.loss_mask: 0.2246  d7.loss_dice: 2.3568  d8.loss_cls: 0.6386  d8.loss_mask: 0.2226  d8.loss_dice: 2.3336
05/11 07:59:22 - mmengine - INFO - Iter(train) [ 4300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:39:36  time: 1.5331  data_time: 0.0298  memory: 38723  grad_norm: 32.2970  loss: 30.4192  loss_cls: 0.6078  loss_mask: 0.2180  loss_dice: 2.0499  d0.loss_cls: 0.7941  d0.loss_mask: 0.2807  d0.loss_dice: 2.5123  d1.loss_cls: 0.8946  d1.loss_mask: 0.2525  d1.loss_dice: 2.2094  d2.loss_cls: 0.7434  d2.loss_mask: 0.2373  d2.loss_dice: 2.1344  d3.loss_cls: 0.6591  d3.loss_mask: 0.2265  d3.loss_dice: 2.1001  d4.loss_cls: 0.6175  d4.loss_mask: 0.2232  d4.loss_dice: 2.0806  d5.loss_cls: 0.6103  d5.loss_mask: 0.2218  d5.loss_dice: 2.0758  d6.loss_cls: 0.6100  d6.loss_mask: 0.2211  d6.loss_dice: 2.0747  d7.loss_cls: 0.6089  d7.loss_mask: 0.2189  d7.loss_dice: 2.0634  d8.loss_cls: 0.6063  d8.loss_mask: 0.2172  d8.loss_dice: 2.0494
05/11 08:00:39 - mmengine - INFO - Iter(train) [ 4350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:38:12  time: 1.5584  data_time: 0.0370  memory: 39018  grad_norm: 33.7071  loss: 29.2092  loss_cls: 0.6081  loss_mask: 0.1963  loss_dice: 1.9594  d0.loss_cls: 0.8029  d0.loss_mask: 0.2578  d0.loss_dice: 2.4069  d1.loss_cls: 0.8822  d1.loss_mask: 0.2269  d1.loss_dice: 2.1109  d2.loss_cls: 0.7363  d2.loss_mask: 0.2097  d2.loss_dice: 2.0368  d3.loss_cls: 0.6569  d3.loss_mask: 0.2027  d3.loss_dice: 2.0011  d4.loss_cls: 0.6136  d4.loss_mask: 0.1984  d4.loss_dice: 1.9882  d5.loss_cls: 0.6064  d5.loss_mask: 0.1994  d5.loss_dice: 1.9854  d6.loss_cls: 0.6010  d6.loss_mask: 0.1994  d6.loss_dice: 1.9853  d7.loss_cls: 0.5993  d7.loss_mask: 0.1982  d7.loss_dice: 1.9790  d8.loss_cls: 0.6048  d8.loss_mask: 0.1964  d8.loss_dice: 1.9595
05/11 08:01:56 - mmengine - INFO - Iter(train) [ 4400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:36:41  time: 1.5279  data_time: 0.0314  memory: 38475  grad_norm: 34.4051  loss: 27.9187  loss_cls: 0.5816  loss_mask: 0.1999  loss_dice: 1.8325  d0.loss_cls: 0.8325  d0.loss_mask: 0.2673  d0.loss_dice: 2.3051  d1.loss_cls: 0.8682  d1.loss_mask: 0.2319  d1.loss_dice: 2.0075  d2.loss_cls: 0.7228  d2.loss_mask: 0.2129  d2.loss_dice: 1.9315  d3.loss_cls: 0.6353  d3.loss_mask: 0.2071  d3.loss_dice: 1.8891  d4.loss_cls: 0.5949  d4.loss_mask: 0.2038  d4.loss_dice: 1.8662  d5.loss_cls: 0.5823  d5.loss_mask: 0.2033  d5.loss_dice: 1.8633  d6.loss_cls: 0.5841  d6.loss_mask: 0.2029  d6.loss_dice: 1.8557  d7.loss_cls: 0.5831  d7.loss_mask: 0.2004  d7.loss_dice: 1.8425  d8.loss_cls: 0.5823  d8.loss_mask: 0.1997  d8.loss_dice: 1.8286
05/11 08:03:13 - mmengine - INFO - Iter(train) [ 4450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:35:14  time: 1.5458  data_time: 0.0321  memory: 38548  grad_norm: 37.8401  loss: 31.4169  loss_cls: 0.6275  loss_mask: 0.2145  loss_dice: 2.1335  d0.loss_cls: 0.8086  d0.loss_mask: 0.2718  d0.loss_dice: 2.6142  d1.loss_cls: 0.8979  d1.loss_mask: 0.2500  d1.loss_dice: 2.3053  d2.loss_cls: 0.7633  d2.loss_mask: 0.2309  d2.loss_dice: 2.2376  d3.loss_cls: 0.6698  d3.loss_mask: 0.2230  d3.loss_dice: 2.1918  d4.loss_cls: 0.6348  d4.loss_mask: 0.2184  d4.loss_dice: 2.1647  d5.loss_cls: 0.6247  d5.loss_mask: 0.2180  d5.loss_dice: 2.1589  d6.loss_cls: 0.6227  d6.loss_mask: 0.2183  d6.loss_dice: 2.1545  d7.loss_cls: 0.6215  d7.loss_mask: 0.2168  d7.loss_dice: 2.1506  d8.loss_cls: 0.6278  d8.loss_mask: 0.2141  d8.loss_dice: 2.1314
05/11 08:04:30 - mmengine - INFO - Iter(train) [ 4500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:33:45  time: 1.5357  data_time: 0.0318  memory: 38764  grad_norm: 42.7190  loss: 30.4671  loss_cls: 0.6201  loss_mask: 0.2055  loss_dice: 2.0485  d0.loss_cls: 0.8380  d0.loss_mask: 0.2702  d0.loss_dice: 2.5103  d1.loss_cls: 0.8993  d1.loss_mask: 0.2399  d1.loss_dice: 2.2106  d2.loss_cls: 0.7535  d2.loss_mask: 0.2222  d2.loss_dice: 2.1553  d3.loss_cls: 0.6709  d3.loss_mask: 0.2134  d3.loss_dice: 2.1109  d4.loss_cls: 0.6275  d4.loss_mask: 0.2094  d4.loss_dice: 2.0919  d5.loss_cls: 0.6170  d5.loss_mask: 0.2090  d5.loss_dice: 2.0793  d6.loss_cls: 0.6173  d6.loss_mask: 0.2095  d6.loss_dice: 2.0798  d7.loss_cls: 0.6166  d7.loss_mask: 0.2077  d7.loss_dice: 2.0676  d8.loss_cls: 0.6191  d8.loss_mask: 0.2061  d8.loss_dice: 2.0407
05/11 08:05:47 - mmengine - INFO - Iter(train) [ 4550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:32:19  time: 1.5505  data_time: 0.0368  memory: 39405  grad_norm: 32.0047  loss: 29.4347  loss_cls: 0.5967  loss_mask: 0.1990  loss_dice: 1.9627  d0.loss_cls: 0.8410  d0.loss_mask: 0.2635  d0.loss_dice: 2.4681  d1.loss_cls: 0.8842  d1.loss_mask: 0.2378  d1.loss_dice: 2.1463  d2.loss_cls: 0.7250  d2.loss_mask: 0.2165  d2.loss_dice: 2.0816  d3.loss_cls: 0.6433  d3.loss_mask: 0.2085  d3.loss_dice: 2.0366  d4.loss_cls: 0.6088  d4.loss_mask: 0.2047  d4.loss_dice: 1.9957  d5.loss_cls: 0.5963  d5.loss_mask: 0.2044  d5.loss_dice: 1.9940  d6.loss_cls: 0.5981  d6.loss_mask: 0.2038  d6.loss_dice: 1.9925  d7.loss_cls: 0.5950  d7.loss_mask: 0.2012  d7.loss_dice: 1.9761  d8.loss_cls: 0.5965  d8.loss_mask: 0.1995  d8.loss_dice: 1.9574
05/11 08:07:05 - mmengine - INFO - Iter(train) [ 4600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:30:53  time: 1.5465  data_time: 0.0308  memory: 38926  grad_norm: 35.3320  loss: 28.7454  loss_cls: 0.6104  loss_mask: 0.1974  loss_dice: 1.8987  d0.loss_cls: 0.8361  d0.loss_mask: 0.2560  d0.loss_dice: 2.3421  d1.loss_cls: 0.8908  d1.loss_mask: 0.2311  d1.loss_dice: 2.0561  d2.loss_cls: 0.7438  d2.loss_mask: 0.2133  d2.loss_dice: 2.0038  d3.loss_cls: 0.6555  d3.loss_mask: 0.2062  d3.loss_dice: 1.9613  d4.loss_cls: 0.6185  d4.loss_mask: 0.2022  d4.loss_dice: 1.9302  d5.loss_cls: 0.6062  d5.loss_mask: 0.2019  d5.loss_dice: 1.9258  d6.loss_cls: 0.6102  d6.loss_mask: 0.2012  d6.loss_dice: 1.9217  d7.loss_cls: 0.6071  d7.loss_mask: 0.1985  d7.loss_dice: 1.9136  d8.loss_cls: 0.6127  d8.loss_mask: 0.1976  d8.loss_dice: 1.8953
05/11 08:08:21 - mmengine - INFO - Iter(train) [ 4650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:29:22  time: 1.5247  data_time: 0.0291  memory: 38766  grad_norm: 34.8039  loss: 30.4631  loss_cls: 0.6177  loss_mask: 0.2186  loss_dice: 2.0428  d0.loss_cls: 0.8169  d0.loss_mask: 0.2850  d0.loss_dice: 2.5029  d1.loss_cls: 0.8846  d1.loss_mask: 0.2516  d1.loss_dice: 2.2073  d2.loss_cls: 0.7315  d2.loss_mask: 0.2346  d2.loss_dice: 2.1502  d3.loss_cls: 0.6600  d3.loss_mask: 0.2278  d3.loss_dice: 2.1053  d4.loss_cls: 0.6264  d4.loss_mask: 0.2227  d4.loss_dice: 2.0740  d5.loss_cls: 0.6143  d5.loss_mask: 0.2228  d5.loss_dice: 2.0773  d6.loss_cls: 0.6150  d6.loss_mask: 0.2222  d6.loss_dice: 2.0713  d7.loss_cls: 0.6168  d7.loss_mask: 0.2188  d7.loss_dice: 2.0597  d8.loss_cls: 0.6185  d8.loss_mask: 0.2197  d8.loss_dice: 2.0465
05/11 08:09:38 - mmengine - INFO - Iter(train) [ 4700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:27:56  time: 1.5437  data_time: 0.0287  memory: 38820  grad_norm: 30.1396  loss: 30.6725  loss_cls: 0.5953  loss_mask: 0.2056  loss_dice: 2.0982  d0.loss_cls: 0.8283  d0.loss_mask: 0.2628  d0.loss_dice: 2.5382  d1.loss_cls: 0.8937  d1.loss_mask: 0.2386  d1.loss_dice: 2.2382  d2.loss_cls: 0.7368  d2.loss_mask: 0.2224  d2.loss_dice: 2.1843  d3.loss_cls: 0.6517  d3.loss_mask: 0.2141  d3.loss_dice: 2.1458  d4.loss_cls: 0.6101  d4.loss_mask: 0.2101  d4.loss_dice: 2.1257  d5.loss_cls: 0.6018  d5.loss_mask: 0.2083  d5.loss_dice: 2.1219  d6.loss_cls: 0.5986  d6.loss_mask: 0.2093  d6.loss_dice: 2.1191  d7.loss_cls: 0.5978  d7.loss_mask: 0.2064  d7.loss_dice: 2.1150  d8.loss_cls: 0.5978  d8.loss_mask: 0.2058  d8.loss_dice: 2.0908
05/11 08:10:55 - mmengine - INFO - Iter(train) [ 4750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:26:28  time: 1.5364  data_time: 0.0296  memory: 38657  grad_norm: 36.9974  loss: 29.8037  loss_cls: 0.6059  loss_mask: 0.1865  loss_dice: 2.0171  d0.loss_cls: 0.8269  d0.loss_mask: 0.2439  d0.loss_dice: 2.4596  d1.loss_cls: 0.8860  d1.loss_mask: 0.2213  d1.loss_dice: 2.1847  d2.loss_cls: 0.7337  d2.loss_mask: 0.2012  d2.loss_dice: 2.1280  d3.loss_cls: 0.6554  d3.loss_mask: 0.1940  d3.loss_dice: 2.0845  d4.loss_cls: 0.6164  d4.loss_mask: 0.1901  d4.loss_dice: 2.0459  d5.loss_cls: 0.6083  d5.loss_mask: 0.1908  d5.loss_dice: 2.0425  d6.loss_cls: 0.6049  d6.loss_mask: 0.1894  d6.loss_dice: 2.0491  d7.loss_cls: 0.6063  d7.loss_mask: 0.1872  d7.loss_dice: 2.0342  d8.loss_cls: 0.6069  d8.loss_mask: 0.1870  d8.loss_dice: 2.0162
05/11 08:12:12 - mmengine - INFO - Iter(train) [ 4800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:25:01  time: 1.5368  data_time: 0.0322  memory: 38726  grad_norm: 31.0561  loss: 29.7593  loss_cls: 0.5915  loss_mask: 0.2173  loss_dice: 2.0015  d0.loss_cls: 0.8387  d0.loss_mask: 0.2787  d0.loss_dice: 2.4374  d1.loss_cls: 0.8760  d1.loss_mask: 0.2513  d1.loss_dice: 2.1677  d2.loss_cls: 0.7090  d2.loss_mask: 0.2350  d2.loss_dice: 2.1035  d3.loss_cls: 0.6301  d3.loss_mask: 0.2274  d3.loss_dice: 2.0586  d4.loss_cls: 0.6039  d4.loss_mask: 0.2231  d4.loss_dice: 2.0226  d5.loss_cls: 0.5915  d5.loss_mask: 0.2209  d5.loss_dice: 2.0231  d6.loss_cls: 0.5892  d6.loss_mask: 0.2225  d6.loss_dice: 2.0199  d7.loss_cls: 0.5870  d7.loss_mask: 0.2204  d7.loss_dice: 2.0097  d8.loss_cls: 0.5942  d8.loss_mask: 0.2180  d8.loss_dice: 1.9897
05/11 08:13:29 - mmengine - INFO - Iter(train) [ 4850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:23:36  time: 1.5470  data_time: 0.0323  memory: 39274  grad_norm: 33.0780  loss: 30.2424  loss_cls: 0.6067  loss_mask: 0.1975  loss_dice: 2.0498  d0.loss_cls: 0.8511  d0.loss_mask: 0.2600  d0.loss_dice: 2.4800  d1.loss_cls: 0.8912  d1.loss_mask: 0.2315  d1.loss_dice: 2.2062  d2.loss_cls: 0.7372  d2.loss_mask: 0.2128  d2.loss_dice: 2.1581  d3.loss_cls: 0.6603  d3.loss_mask: 0.2054  d3.loss_dice: 2.1042  d4.loss_cls: 0.6248  d4.loss_mask: 0.2009  d4.loss_dice: 2.0739  d5.loss_cls: 0.6122  d5.loss_mask: 0.2007  d5.loss_dice: 2.0747  d6.loss_cls: 0.6127  d6.loss_mask: 0.1994  d6.loss_dice: 2.0709  d7.loss_cls: 0.6086  d7.loss_mask: 0.1977  d7.loss_dice: 2.0617  d8.loss_cls: 0.6098  d8.loss_mask: 0.1978  d8.loss_dice: 2.0445
05/11 08:14:47 - mmengine - INFO - Iter(train) [ 4900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:22:11  time: 1.5480  data_time: 0.0318  memory: 38459  grad_norm: 36.7892  loss: 28.6040  loss_cls: 0.5870  loss_mask: 0.1996  loss_dice: 1.8994  d0.loss_cls: 0.8657  d0.loss_mask: 0.2629  d0.loss_dice: 2.3183  d1.loss_cls: 0.8557  d1.loss_mask: 0.2362  d1.loss_dice: 2.0687  d2.loss_cls: 0.6975  d2.loss_mask: 0.2136  d2.loss_dice: 2.0228  d3.loss_cls: 0.6280  d3.loss_mask: 0.2059  d3.loss_dice: 1.9682  d4.loss_cls: 0.5984  d4.loss_mask: 0.2046  d4.loss_dice: 1.9338  d5.loss_cls: 0.5861  d5.loss_mask: 0.2026  d5.loss_dice: 1.9304  d6.loss_cls: 0.5857  d6.loss_mask: 0.2030  d6.loss_dice: 1.9314  d7.loss_cls: 0.5899  d7.loss_mask: 0.2013  d7.loss_dice: 1.9194  d8.loss_cls: 0.5874  d8.loss_mask: 0.1985  d8.loss_dice: 1.9017
05/11 08:16:04 - mmengine - INFO - Iter(train) [ 4950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:20:48  time: 1.5530  data_time: 0.0292  memory: 38806  grad_norm: 33.4153  loss: 30.3276  loss_cls: 0.6245  loss_mask: 0.2083  loss_dice: 2.0409  d0.loss_cls: 0.8543  d0.loss_mask: 0.2672  d0.loss_dice: 2.4577  d1.loss_cls: 0.8960  d1.loss_mask: 0.2409  d1.loss_dice: 2.2031  d2.loss_cls: 0.7286  d2.loss_mask: 0.2251  d2.loss_dice: 2.1452  d3.loss_cls: 0.6639  d3.loss_mask: 0.2176  d3.loss_dice: 2.0965  d4.loss_cls: 0.6314  d4.loss_mask: 0.2112  d4.loss_dice: 2.0659  d5.loss_cls: 0.6206  d5.loss_mask: 0.2121  d5.loss_dice: 2.0691  d6.loss_cls: 0.6226  d6.loss_mask: 0.2110  d6.loss_dice: 2.0623  d7.loss_cls: 0.6159  d7.loss_mask: 0.2100  d7.loss_dice: 2.0624  d8.loss_cls: 0.6195  d8.loss_mask: 0.2079  d8.loss_dice: 2.0360
05/11 08:17:20 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 08:17:20 - mmengine - INFO - Iter(train) [ 5000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:19:19  time: 1.5245  data_time: 0.0317  memory: 38673  grad_norm: 35.3118  loss: 28.1258  loss_cls: 0.5872  loss_mask: 0.2005  loss_dice: 1.8711  d0.loss_cls: 0.8201  d0.loss_mask: 0.2600  d0.loss_dice: 2.2446  d1.loss_cls: 0.8697  d1.loss_mask: 0.2298  d1.loss_dice: 2.0064  d2.loss_cls: 0.7034  d2.loss_mask: 0.2129  d2.loss_dice: 1.9646  d3.loss_cls: 0.6284  d3.loss_mask: 0.2072  d3.loss_dice: 1.9237  d4.loss_cls: 0.5955  d4.loss_mask: 0.2031  d4.loss_dice: 1.8996  d5.loss_cls: 0.5859  d5.loss_mask: 0.2031  d5.loss_dice: 1.8985  d6.loss_cls: 0.5895  d6.loss_mask: 0.2020  d6.loss_dice: 1.8913  d7.loss_cls: 0.5859  d7.loss_mask: 0.2019  d7.loss_dice: 1.8844  d8.loss_cls: 0.5870  d8.loss_mask: 0.2002  d8.loss_dice: 1.8682
05/11 08:18:37 - mmengine - INFO - Iter(train) [ 5050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:17:50  time: 1.5221  data_time: 0.0307  memory: 38758  grad_norm: 42.8860  loss: 29.8670  loss_cls: 0.6049  loss_mask: 0.2145  loss_dice: 1.9876  d0.loss_cls: 0.8532  d0.loss_mask: 0.2825  d0.loss_dice: 2.4358  d1.loss_cls: 0.8906  d1.loss_mask: 0.2522  d1.loss_dice: 2.1702  d2.loss_cls: 0.7167  d2.loss_mask: 0.2337  d2.loss_dice: 2.1194  d3.loss_cls: 0.6507  d3.loss_mask: 0.2258  d3.loss_dice: 2.0652  d4.loss_cls: 0.6124  d4.loss_mask: 0.2206  d4.loss_dice: 2.0218  d5.loss_cls: 0.6056  d5.loss_mask: 0.2188  d5.loss_dice: 2.0180  d6.loss_cls: 0.6042  d6.loss_mask: 0.2191  d6.loss_dice: 2.0117  d7.loss_cls: 0.6050  d7.loss_mask: 0.2169  d7.loss_dice: 1.9994  d8.loss_cls: 0.6083  d8.loss_mask: 0.2145  d8.loss_dice: 1.9878
05/11 08:19:53 - mmengine - INFO - Iter(train) [ 5100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:16:22  time: 1.5261  data_time: 0.0320  memory: 38830  grad_norm: 30.9021  loss: 29.0075  loss_cls: 0.6015  loss_mask: 0.2050  loss_dice: 1.9285  d0.loss_cls: 0.8472  d0.loss_mask: 0.2709  d0.loss_dice: 2.3335  d1.loss_cls: 0.8809  d1.loss_mask: 0.2415  d1.loss_dice: 2.0881  d2.loss_cls: 0.7119  d2.loss_mask: 0.2221  d2.loss_dice: 2.0333  d3.loss_cls: 0.6400  d3.loss_mask: 0.2139  d3.loss_dice: 1.9926  d4.loss_cls: 0.6084  d4.loss_mask: 0.2111  d4.loss_dice: 1.9607  d5.loss_cls: 0.5995  d5.loss_mask: 0.2100  d5.loss_dice: 1.9595  d6.loss_cls: 0.6006  d6.loss_mask: 0.2090  d6.loss_dice: 1.9571  d7.loss_cls: 0.5989  d7.loss_mask: 0.2061  d7.loss_dice: 1.9448  d8.loss_cls: 0.5996  d8.loss_mask: 0.2057  d8.loss_dice: 1.9254
05/11 08:21:10 - mmengine - INFO - Iter(train) [ 5150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:14:58  time: 1.5469  data_time: 0.0320  memory: 39403  grad_norm: 35.8325  loss: 29.5559  loss_cls: 0.6074  loss_mask: 0.2015  loss_dice: 1.9719  d0.loss_cls: 0.8552  d0.loss_mask: 0.2603  d0.loss_dice: 2.4142  d1.loss_cls: 0.8903  d1.loss_mask: 0.2332  d1.loss_dice: 2.1539  d2.loss_cls: 0.7237  d2.loss_mask: 0.2170  d2.loss_dice: 2.0907  d3.loss_cls: 0.6494  d3.loss_mask: 0.2098  d3.loss_dice: 2.0444  d4.loss_cls: 0.6132  d4.loss_mask: 0.2061  d4.loss_dice: 2.0101  d5.loss_cls: 0.6062  d5.loss_mask: 0.2046  d5.loss_dice: 2.0046  d6.loss_cls: 0.6085  d6.loss_mask: 0.2036  d6.loss_dice: 1.9974  d7.loss_cls: 0.6108  d7.loss_mask: 0.2012  d7.loss_dice: 1.9867  d8.loss_cls: 0.6074  d8.loss_mask: 0.2013  d8.loss_dice: 1.9713
05/11 08:22:27 - mmengine - INFO - Iter(train) [ 5200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:13:30  time: 1.5277  data_time: 0.0321  memory: 38768  grad_norm: 38.2194  loss: 31.6131  loss_cls: 0.6274  loss_mask: 0.2077  loss_dice: 2.1706  d0.loss_cls: 0.8558  d0.loss_mask: 0.2676  d0.loss_dice: 2.5991  d1.loss_cls: 0.8900  d1.loss_mask: 0.2461  d1.loss_dice: 2.3311  d2.loss_cls: 0.7221  d2.loss_mask: 0.2265  d2.loss_dice: 2.2636  d3.loss_cls: 0.6588  d3.loss_mask: 0.2191  d3.loss_dice: 2.2184  d4.loss_cls: 0.6301  d4.loss_mask: 0.2136  d4.loss_dice: 2.1979  d5.loss_cls: 0.6241  d5.loss_mask: 0.2112  d5.loss_dice: 2.1940  d6.loss_cls: 0.6190  d6.loss_mask: 0.2113  d6.loss_dice: 2.1945  d7.loss_cls: 0.6211  d7.loss_mask: 0.2105  d7.loss_dice: 2.1845  d8.loss_cls: 0.6263  d8.loss_mask: 0.2084  d8.loss_dice: 2.1629
05/11 08:23:43 - mmengine - INFO - Iter(train) [ 5250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:12:04  time: 1.5312  data_time: 0.0319  memory: 38242  grad_norm: 34.8534  loss: 28.5756  loss_cls: 0.5986  loss_mask: 0.1922  loss_dice: 1.9107  d0.loss_cls: 0.8229  d0.loss_mask: 0.2456  d0.loss_dice: 2.3195  d1.loss_cls: 0.8794  d1.loss_mask: 0.2213  d1.loss_dice: 2.0636  d2.loss_cls: 0.7106  d2.loss_mask: 0.2071  d2.loss_dice: 2.0077  d3.loss_cls: 0.6381  d3.loss_mask: 0.1992  d3.loss_dice: 1.9616  d4.loss_cls: 0.6088  d4.loss_mask: 0.1944  d4.loss_dice: 1.9299  d5.loss_cls: 0.5981  d5.loss_mask: 0.1940  d5.loss_dice: 1.9344  d6.loss_cls: 0.5984  d6.loss_mask: 0.1939  d6.loss_dice: 1.9276  d7.loss_cls: 0.5950  d7.loss_mask: 0.1928  d7.loss_dice: 1.9245  d8.loss_cls: 0.6009  d8.loss_mask: 0.1921  d8.loss_dice: 1.9127
05/11 08:25:00 - mmengine - INFO - Iter(train) [ 5300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:10:39  time: 1.5421  data_time: 0.0365  memory: 39092  grad_norm: 34.6984  loss: 29.4947  loss_cls: 0.6091  loss_mask: 0.1981  loss_dice: 1.9718  d0.loss_cls: 0.8720  d0.loss_mask: 0.2594  d0.loss_dice: 2.4082  d1.loss_cls: 0.8792  d1.loss_mask: 0.2349  d1.loss_dice: 2.1458  d2.loss_cls: 0.7235  d2.loss_mask: 0.2138  d2.loss_dice: 2.0772  d3.loss_cls: 0.6585  d3.loss_mask: 0.2051  d3.loss_dice: 2.0309  d4.loss_cls: 0.6219  d4.loss_mask: 0.2028  d4.loss_dice: 1.9971  d5.loss_cls: 0.6107  d5.loss_mask: 0.2022  d5.loss_dice: 1.9948  d6.loss_cls: 0.6113  d6.loss_mask: 0.2014  d6.loss_dice: 1.9948  d7.loss_cls: 0.6055  d7.loss_mask: 0.2005  d7.loss_dice: 1.9858  d8.loss_cls: 0.6105  d8.loss_mask: 0.1983  d8.loss_dice: 1.9699
05/11 08:26:17 - mmengine - INFO - Iter(train) [ 5350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:09:14  time: 1.5361  data_time: 0.0298  memory: 38852  grad_norm: 37.9842  loss: 29.5652  loss_cls: 0.6117  loss_mask: 0.2162  loss_dice: 1.9514  d0.loss_cls: 0.8609  d0.loss_mask: 0.2862  d0.loss_dice: 2.3964  d1.loss_cls: 0.8898  d1.loss_mask: 0.2559  d1.loss_dice: 2.1347  d2.loss_cls: 0.7215  d2.loss_mask: 0.2348  d2.loss_dice: 2.0701  d3.loss_cls: 0.6555  d3.loss_mask: 0.2282  d3.loss_dice: 2.0215  d4.loss_cls: 0.6241  d4.loss_mask: 0.2201  d4.loss_dice: 1.9824  d5.loss_cls: 0.6111  d5.loss_mask: 0.2206  d5.loss_dice: 1.9801  d6.loss_cls: 0.6136  d6.loss_mask: 0.2203  d6.loss_dice: 1.9775  d7.loss_cls: 0.6134  d7.loss_mask: 0.2174  d7.loss_dice: 1.9674  d8.loss_cls: 0.6143  d8.loss_mask: 0.2162  d8.loss_dice: 1.9517
05/11 08:27:34 - mmengine - INFO - Iter(train) [ 5400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:07:48  time: 1.5330  data_time: 0.0290  memory: 39731  grad_norm: 34.1770  loss: 32.6734  loss_cls: 0.6425  loss_mask: 0.2220  loss_dice: 2.2433  d0.loss_cls: 0.8950  d0.loss_mask: 0.2868  d0.loss_dice: 2.6700  d1.loss_cls: 0.8801  d1.loss_mask: 0.2570  d1.loss_dice: 2.4083  d2.loss_cls: 0.7309  d2.loss_mask: 0.2399  d2.loss_dice: 2.3468  d3.loss_cls: 0.6722  d3.loss_mask: 0.2325  d3.loss_dice: 2.3024  d4.loss_cls: 0.6467  d4.loss_mask: 0.2272  d4.loss_dice: 2.2735  d5.loss_cls: 0.6405  d5.loss_mask: 0.2254  d5.loss_dice: 2.2695  d6.loss_cls: 0.6432  d6.loss_mask: 0.2241  d6.loss_dice: 2.2720  d7.loss_cls: 0.6337  d7.loss_mask: 0.2242  d7.loss_dice: 2.2609  d8.loss_cls: 0.6405  d8.loss_mask: 0.2221  d8.loss_dice: 2.2403
05/11 08:28:50 - mmengine - INFO - Iter(train) [ 5450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:06:20  time: 1.5177  data_time: 0.0279  memory: 38878  grad_norm: 35.3481  loss: 32.2940  loss_cls: 0.6217  loss_mask: 0.2140  loss_dice: 2.2217  d0.loss_cls: 0.8501  d0.loss_mask: 0.2777  d0.loss_dice: 2.6782  d1.loss_cls: 0.8884  d1.loss_mask: 0.2523  d1.loss_dice: 2.4161  d2.loss_cls: 0.7179  d2.loss_mask: 0.2302  d2.loss_dice: 2.3454  d3.loss_cls: 0.6552  d3.loss_mask: 0.2257  d3.loss_dice: 2.2933  d4.loss_cls: 0.6267  d4.loss_mask: 0.2188  d4.loss_dice: 2.2528  d5.loss_cls: 0.6188  d5.loss_mask: 0.2171  d5.loss_dice: 2.2499  d6.loss_cls: 0.6251  d6.loss_mask: 0.2176  d6.loss_dice: 2.2465  d7.loss_cls: 0.6190  d7.loss_mask: 0.2154  d7.loss_dice: 2.2429  d8.loss_cls: 0.6223  d8.loss_mask: 0.2141  d8.loss_dice: 2.2194
05/11 08:30:07 - mmengine - INFO - Iter(train) [ 5500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:04:56  time: 1.5396  data_time: 0.0330  memory: 39224  grad_norm: 32.5036  loss: 29.5179  loss_cls: 0.6113  loss_mask: 0.2013  loss_dice: 1.9692  d0.loss_cls: 0.8768  d0.loss_mask: 0.2678  d0.loss_dice: 2.3872  d1.loss_cls: 0.8652  d1.loss_mask: 0.2398  d1.loss_dice: 2.1431  d2.loss_cls: 0.7084  d2.loss_mask: 0.2174  d2.loss_dice: 2.0901  d3.loss_cls: 0.6479  d3.loss_mask: 0.2095  d3.loss_dice: 2.0341  d4.loss_cls: 0.6194  d4.loss_mask: 0.2051  d4.loss_dice: 2.0070  d5.loss_cls: 0.6105  d5.loss_mask: 0.2041  d5.loss_dice: 2.0010  d6.loss_cls: 0.6099  d6.loss_mask: 0.2044  d6.loss_dice: 1.9982  d7.loss_cls: 0.6104  d7.loss_mask: 0.2023  d7.loss_dice: 1.9908  d8.loss_cls: 0.6156  d8.loss_mask: 0.2006  d8.loss_dice: 1.9696
05/11 08:31:23 - mmengine - INFO - Iter(train) [ 5550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:03:30  time: 1.5290  data_time: 0.0279  memory: 38258  grad_norm: 32.7810  loss: 29.3846  loss_cls: 0.6143  loss_mask: 0.1982  loss_dice: 1.9551  d0.loss_cls: 0.8503  d0.loss_mask: 0.2653  d0.loss_dice: 2.3930  d1.loss_cls: 0.8805  d1.loss_mask: 0.2344  d1.loss_dice: 2.1357  d2.loss_cls: 0.7174  d2.loss_mask: 0.2140  d2.loss_dice: 2.0776  d3.loss_cls: 0.6516  d3.loss_mask: 0.2068  d3.loss_dice: 2.0201  d4.loss_cls: 0.6229  d4.loss_mask: 0.2054  d4.loss_dice: 1.9888  d5.loss_cls: 0.6147  d5.loss_mask: 0.2037  d5.loss_dice: 1.9835  d6.loss_cls: 0.6130  d6.loss_mask: 0.2034  d6.loss_dice: 1.9811  d7.loss_cls: 0.6149  d7.loss_mask: 0.1996  d7.loss_dice: 1.9680  d8.loss_cls: 0.6179  d8.loss_mask: 0.1996  d8.loss_dice: 1.9538
05/11 08:32:39 - mmengine - INFO - Iter(train) [ 5600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:02:03  time: 1.5243  data_time: 0.0298  memory: 39080  grad_norm: 34.3206  loss: 28.2952  loss_cls: 0.5985  loss_mask: 0.1952  loss_dice: 1.8711  d0.loss_cls: 0.8508  d0.loss_mask: 0.2598  d0.loss_dice: 2.2882  d1.loss_cls: 0.8817  d1.loss_mask: 0.2298  d1.loss_dice: 2.0400  d2.loss_cls: 0.6999  d2.loss_mask: 0.2117  d2.loss_dice: 1.9846  d3.loss_cls: 0.6327  d3.loss_mask: 0.2055  d3.loss_dice: 1.9370  d4.loss_cls: 0.6054  d4.loss_mask: 0.2003  d4.loss_dice: 1.8927  d5.loss_cls: 0.5948  d5.loss_mask: 0.1995  d5.loss_dice: 1.8955  d6.loss_cls: 0.5940  d6.loss_mask: 0.1987  d6.loss_dice: 1.8919  d7.loss_cls: 0.5925  d7.loss_mask: 0.1969  d7.loss_dice: 1.8868  d8.loss_cls: 0.5993  d8.loss_mask: 0.1955  d8.loss_dice: 1.8649
05/11 08:33:56 - mmengine - INFO - Iter(train) [ 5650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:00:38  time: 1.5299  data_time: 0.0304  memory: 39007  grad_norm: 35.1402  loss: 29.5751  loss_cls: 0.6028  loss_mask: 0.1979  loss_dice: 1.9868  d0.loss_cls: 0.8548  d0.loss_mask: 0.2628  d0.loss_dice: 2.4266  d1.loss_cls: 0.8732  d1.loss_mask: 0.2351  d1.loss_dice: 2.1813  d2.loss_cls: 0.6978  d2.loss_mask: 0.2163  d2.loss_dice: 2.1010  d3.loss_cls: 0.6346  d3.loss_mask: 0.2067  d3.loss_dice: 2.0524  d4.loss_cls: 0.6093  d4.loss_mask: 0.2032  d4.loss_dice: 2.0205  d5.loss_cls: 0.5991  d5.loss_mask: 0.2030  d5.loss_dice: 2.0136  d6.loss_cls: 0.5980  d6.loss_mask: 0.2015  d6.loss_dice: 2.0105  d7.loss_cls: 0.6015  d7.loss_mask: 0.1985  d7.loss_dice: 2.0002  d8.loss_cls: 0.6046  d8.loss_mask: 0.1979  d8.loss_dice: 1.9837
05/11 08:35:12 - mmengine - INFO - Iter(train) [ 5700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:59:12  time: 1.5260  data_time: 0.0277  memory: 39192  grad_norm: 37.8765  loss: 28.9564  loss_cls: 0.5915  loss_mask: 0.2050  loss_dice: 1.9315  d0.loss_cls: 0.8654  d0.loss_mask: 0.2774  d0.loss_dice: 2.3382  d1.loss_cls: 0.8735  d1.loss_mask: 0.2425  d1.loss_dice: 2.1118  d2.loss_cls: 0.6955  d2.loss_mask: 0.2231  d2.loss_dice: 2.0438  d3.loss_cls: 0.6318  d3.loss_mask: 0.2148  d3.loss_dice: 1.9920  d4.loss_cls: 0.5982  d4.loss_mask: 0.2098  d4.loss_dice: 1.9562  d5.loss_cls: 0.5874  d5.loss_mask: 0.2087  d5.loss_dice: 1.9569  d6.loss_cls: 0.5919  d6.loss_mask: 0.2092  d6.loss_dice: 1.9489  d7.loss_cls: 0.5856  d7.loss_mask: 0.2063  d7.loss_dice: 1.9383  d8.loss_cls: 0.5929  d8.loss_mask: 0.2045  d8.loss_dice: 1.9236
05/11 08:36:29 - mmengine - INFO - Iter(train) [ 5750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:57:48  time: 1.5363  data_time: 0.0336  memory: 38658  grad_norm: 32.7008  loss: 29.9537  loss_cls: 0.6149  loss_mask: 0.2035  loss_dice: 2.0080  d0.loss_cls: 0.8626  d0.loss_mask: 0.2671  d0.loss_dice: 2.4289  d1.loss_cls: 0.8694  d1.loss_mask: 0.2384  d1.loss_dice: 2.1957  d2.loss_cls: 0.7156  d2.loss_mask: 0.2208  d2.loss_dice: 2.1219  d3.loss_cls: 0.6552  d3.loss_mask: 0.2110  d3.loss_dice: 2.0747  d4.loss_cls: 0.6296  d4.loss_mask: 0.2079  d4.loss_dice: 2.0396  d5.loss_cls: 0.6177  d5.loss_mask: 0.2066  d5.loss_dice: 2.0369  d6.loss_cls: 0.6177  d6.loss_mask: 0.2063  d6.loss_dice: 2.0339  d7.loss_cls: 0.6141  d7.loss_mask: 0.2047  d7.loss_dice: 2.0283  d8.loss_cls: 0.6142  d8.loss_mask: 0.2038  d8.loss_dice: 2.0048
05/11 08:37:45 - mmengine - INFO - Iter(train) [ 5800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:56:21  time: 1.5178  data_time: 0.0303  memory: 39295  grad_norm: 37.8242  loss: 31.0139  loss_cls: 0.6171  loss_mask: 0.2071  loss_dice: 2.0942  d0.loss_cls: 0.8762  d0.loss_mask: 0.2714  d0.loss_dice: 2.5638  d1.loss_cls: 0.8789  d1.loss_mask: 0.2416  d1.loss_dice: 2.3166  d2.loss_cls: 0.7151  d2.loss_mask: 0.2233  d2.loss_dice: 2.2368  d3.loss_cls: 0.6528  d3.loss_mask: 0.2164  d3.loss_dice: 2.1757  d4.loss_cls: 0.6219  d4.loss_mask: 0.2115  d4.loss_dice: 2.1347  d5.loss_cls: 0.6131  d5.loss_mask: 0.2104  d5.loss_dice: 2.1314  d6.loss_cls: 0.6137  d6.loss_mask: 0.2084  d6.loss_dice: 2.1256  d7.loss_cls: 0.6119  d7.loss_mask: 0.2076  d7.loss_dice: 2.1158  d8.loss_cls: 0.6184  d8.loss_mask: 0.2064  d8.loss_dice: 2.0961
05/11 08:39:01 - mmengine - INFO - Iter(train) [ 5850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:54:54  time: 1.5177  data_time: 0.0265  memory: 38829  grad_norm: 33.9299  loss: 29.4338  loss_cls: 0.6023  loss_mask: 0.2043  loss_dice: 1.9615  d0.loss_cls: 0.8700  d0.loss_mask: 0.2715  d0.loss_dice: 2.3771  d1.loss_cls: 0.8703  d1.loss_mask: 0.2440  d1.loss_dice: 2.1639  d2.loss_cls: 0.7035  d2.loss_mask: 0.2219  d2.loss_dice: 2.0872  d3.loss_cls: 0.6401  d3.loss_mask: 0.2155  d3.loss_dice: 2.0323  d4.loss_cls: 0.6091  d4.loss_mask: 0.2102  d4.loss_dice: 1.9951  d5.loss_cls: 0.6049  d5.loss_mask: 0.2092  d5.loss_dice: 1.9883  d6.loss_cls: 0.6007  d6.loss_mask: 0.2091  d6.loss_dice: 1.9877  d7.loss_cls: 0.6011  d7.loss_mask: 0.2069  d7.loss_dice: 1.9778  d8.loss_cls: 0.6026  d8.loss_mask: 0.2058  d8.loss_dice: 1.9599
05/11 08:40:18 - mmengine - INFO - Iter(train) [ 5900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:53:31  time: 1.5394  data_time: 0.0282  memory: 39058  grad_norm: 35.0200  loss: 29.0093  loss_cls: 0.5995  loss_mask: 0.1874  loss_dice: 1.9421  d0.loss_cls: 0.8895  d0.loss_mask: 0.2490  d0.loss_dice: 2.3444  d1.loss_cls: 0.8684  d1.loss_mask: 0.2234  d1.loss_dice: 2.1261  d2.loss_cls: 0.7001  d2.loss_mask: 0.2035  d2.loss_dice: 2.0659  d3.loss_cls: 0.6403  d3.loss_mask: 0.1943  d3.loss_dice: 2.0092  d4.loss_cls: 0.6097  d4.loss_mask: 0.1919  d4.loss_dice: 1.9697  d5.loss_cls: 0.6007  d5.loss_mask: 0.1908  d5.loss_dice: 1.9725  d6.loss_cls: 0.5973  d6.loss_mask: 0.1905  d6.loss_dice: 1.9717  d7.loss_cls: 0.5996  d7.loss_mask: 0.1879  d7.loss_dice: 1.9557  d8.loss_cls: 0.6013  d8.loss_mask: 0.1888  d8.loss_dice: 1.9379
05/11 08:41:34 - mmengine - INFO - Iter(train) [ 5950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:52:07  time: 1.5335  data_time: 0.0307  memory: 39009  grad_norm: 32.1256  loss: 28.9124  loss_cls: 0.5983  loss_mask: 0.1873  loss_dice: 1.9302  d0.loss_cls: 0.8743  d0.loss_mask: 0.2448  d0.loss_dice: 2.3344  d1.loss_cls: 0.8691  d1.loss_mask: 0.2203  d1.loss_dice: 2.1232  d2.loss_cls: 0.6941  d2.loss_mask: 0.2064  d2.loss_dice: 2.0657  d3.loss_cls: 0.6459  d3.loss_mask: 0.1986  d3.loss_dice: 2.0033  d4.loss_cls: 0.6070  d4.loss_mask: 0.1931  d4.loss_dice: 1.9692  d5.loss_cls: 0.5961  d5.loss_mask: 0.1908  d5.loss_dice: 1.9659  d6.loss_cls: 0.5945  d6.loss_mask: 0.1905  d6.loss_dice: 1.9565  d7.loss_cls: 0.5947  d7.loss_mask: 0.1878  d7.loss_dice: 1.9553  d8.loss_cls: 0.5972  d8.loss_mask: 0.1861  d8.loss_dice: 1.9315
05/11 08:42:51 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 08:42:51 - mmengine - INFO - Iter(train) [ 6000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:50:43  time: 1.5336  data_time: 0.0347  memory: 38665  grad_norm: 31.8782  loss: 29.0268  loss_cls: 0.6046  loss_mask: 0.1910  loss_dice: 1.9346  d0.loss_cls: 0.8681  d0.loss_mask: 0.2518  d0.loss_dice: 2.3589  d1.loss_cls: 0.8757  d1.loss_mask: 0.2295  d1.loss_dice: 2.1414  d2.loss_cls: 0.7066  d2.loss_mask: 0.2075  d2.loss_dice: 2.0613  d3.loss_cls: 0.6423  d3.loss_mask: 0.1976  d3.loss_dice: 2.0031  d4.loss_cls: 0.6097  d4.loss_mask: 0.1955  d4.loss_dice: 1.9642  d5.loss_cls: 0.6014  d5.loss_mask: 0.1941  d5.loss_dice: 1.9629  d6.loss_cls: 0.6018  d6.loss_mask: 0.1930  d6.loss_dice: 1.9644  d7.loss_cls: 0.6032  d7.loss_mask: 0.1915  d7.loss_dice: 1.9453  d8.loss_cls: 0.6029  d8.loss_mask: 0.1915  d8.loss_dice: 1.9314
05/11 08:42:51 - mmengine - INFO - Saving checkpoint at 6000 iterations
05/11 08:43:34 - mmengine - INFO - Iter(val) [ 50/100]    eta: 0:00:36  time: 0.7279  data_time: 0.0227  memory: 5704  
05/11 08:44:10 - mmengine - INFO - Iter(val) [100/100]    eta: 0:00:00  time: 0.7122  data_time: 0.0139  memory: 5704  
05/11 08:44:32 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.17s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/11 08:44:41 - mmengine - INFO - start multi processing evaluation ...

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

  0%|          | 0/25552 [00:00<?, ?it/s]
 50%|█████     | 12777/25552 [00:00<00:00, 127481.38it/s]
100%|██████████| 25552/25552 [00:00<00:00, 138677.28it/s]
DONE (t=50.51s).
Accumulating evaluation results...
DONE (t=0.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.344
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.702
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.285
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.415
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.872
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.482
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.852
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.438
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.371
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.527
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.482
05/11 08:45:31 - mmengine - INFO - segm_mAP_copypaste: 0.344 0.702 0.285 0.211 0.415 0.872
05/11 08:45:31 - mmengine - INFO - segm_mAR_copypaste: 0.482 0.852 0.438 0.371 0.527 0.900
05/11 08:45:32 - mmengine - INFO - Iter(val) [100/100]    coco/segm_mAP: 0.3440  coco/segm_mAP_50: 0.7020  coco/segm_mAP_75: 0.2850  coco/segm_mAP_s: 0.2110  coco/segm_mAP_m: 0.4150  coco/segm_mAP_l: 0.8720  data_time: 0.0182  time: 0.7187
05/11 08:45:32 - mmengine - INFO - The previous best checkpoint /data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-L_8xb32-24k_coco/best_coco_segm_mAP_50_iter_4000.pth is removed
05/11 08:45:34 - mmengine - INFO - The best checkpoint with 0.7020 coco/segm_mAP_50 at 6000 iter is saved to best_coco_segm_mAP_50_iter_6000.pth.
05/11 08:46:57 - mmengine - INFO - Iter(train) [ 6050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:53:47  time: 3.3342  data_time: 1.8463  memory: 39594  grad_norm: 35.7149  loss: 29.9226  loss_cls: 0.5937  loss_mask: 0.1994  loss_dice: 2.0217  d0.loss_cls: 0.8684  d0.loss_mask: 0.2677  d0.loss_dice: 2.4492  d1.loss_cls: 0.8707  d1.loss_mask: 0.2395  d1.loss_dice: 2.2249  d2.loss_cls: 0.6958  d2.loss_mask: 0.2179  d2.loss_dice: 2.1415  d3.loss_cls: 0.6328  d3.loss_mask: 0.2075  d3.loss_dice: 2.0870  d4.loss_cls: 0.6071  d4.loss_mask: 0.2032  d4.loss_dice: 2.0512  d5.loss_cls: 0.6011  d5.loss_mask: 0.2029  d5.loss_dice: 2.0413  d6.loss_cls: 0.6000  d6.loss_mask: 0.2025  d6.loss_dice: 2.0471  d7.loss_cls: 0.5960  d7.loss_mask: 0.2004  d7.loss_dice: 2.0337  d8.loss_cls: 0.5960  d8.loss_mask: 0.2004  d8.loss_dice: 2.0219
05/11 08:48:13 - mmengine - INFO - Iter(train) [ 6100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:52:19  time: 1.5288  data_time: 0.0294  memory: 38415  grad_norm: 30.9563  loss: 27.1412  loss_cls: 0.5722  loss_mask: 0.1903  loss_dice: 1.7945  d0.loss_cls: 0.8734  d0.loss_mask: 0.2576  d0.loss_dice: 2.1383  d1.loss_cls: 0.8274  d1.loss_mask: 0.2245  d1.loss_dice: 1.9595  d2.loss_cls: 0.6576  d2.loss_mask: 0.2076  d2.loss_dice: 1.9017  d3.loss_cls: 0.6003  d3.loss_mask: 0.1992  d3.loss_dice: 1.8510  d4.loss_cls: 0.5812  d4.loss_mask: 0.1942  d4.loss_dice: 1.8209  d5.loss_cls: 0.5712  d5.loss_mask: 0.1939  d5.loss_dice: 1.8184  d6.loss_cls: 0.5728  d6.loss_mask: 0.1927  d6.loss_dice: 1.8151  d7.loss_cls: 0.5696  d7.loss_mask: 0.1914  d7.loss_dice: 1.8063  d8.loss_cls: 0.5730  d8.loss_mask: 0.1908  d8.loss_dice: 1.7946
05/11 08:49:29 - mmengine - INFO - Iter(train) [ 6150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:50:51  time: 1.5234  data_time: 0.0297  memory: 38708  grad_norm: 31.7210  loss: 29.2427  loss_cls: 0.6094  loss_mask: 0.1947  loss_dice: 1.9478  d0.loss_cls: 0.8589  d0.loss_mask: 0.2597  d0.loss_dice: 2.3861  d1.loss_cls: 0.8840  d1.loss_mask: 0.2313  d1.loss_dice: 2.1573  d2.loss_cls: 0.7032  d2.loss_mask: 0.2100  d2.loss_dice: 2.0779  d3.loss_cls: 0.6414  d3.loss_mask: 0.2035  d3.loss_dice: 2.0125  d4.loss_cls: 0.6134  d4.loss_mask: 0.1985  d4.loss_dice: 1.9798  d5.loss_cls: 0.6133  d5.loss_mask: 0.1976  d5.loss_dice: 1.9719  d6.loss_cls: 0.6059  d6.loss_mask: 0.1981  d6.loss_dice: 1.9724  d7.loss_cls: 0.6052  d7.loss_mask: 0.1960  d7.loss_dice: 1.9636  d8.loss_cls: 0.6071  d8.loss_mask: 0.1953  d8.loss_dice: 1.9467
05/11 08:50:45 - mmengine - INFO - Iter(train) [ 6200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:49:23  time: 1.5183  data_time: 0.0299  memory: 39248  grad_norm: 37.3365  loss: 29.4169  loss_cls: 0.5973  loss_mask: 0.2047  loss_dice: 1.9793  d0.loss_cls: 0.8560  d0.loss_mask: 0.2677  d0.loss_dice: 2.3681  d1.loss_cls: 0.8597  d1.loss_mask: 0.2402  d1.loss_dice: 2.1751  d2.loss_cls: 0.6784  d2.loss_mask: 0.2213  d2.loss_dice: 2.0977  d3.loss_cls: 0.6207  d3.loss_mask: 0.2130  d3.loss_dice: 2.0433  d4.loss_cls: 0.6009  d4.loss_mask: 0.2111  d4.loss_dice: 2.0079  d5.loss_cls: 0.5948  d5.loss_mask: 0.2096  d5.loss_dice: 2.0025  d6.loss_cls: 0.5932  d6.loss_mask: 0.2083  d6.loss_dice: 2.0019  d7.loss_cls: 0.5955  d7.loss_mask: 0.2059  d7.loss_dice: 1.9887  d8.loss_cls: 0.6009  d8.loss_mask: 0.2041  d8.loss_dice: 1.9691
05/11 08:52:02 - mmengine - INFO - Iter(train) [ 6250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:47:56  time: 1.5287  data_time: 0.0362  memory: 38502  grad_norm: 38.2855  loss: 29.3068  loss_cls: 0.5914  loss_mask: 0.1855  loss_dice: 1.9845  d0.loss_cls: 0.8671  d0.loss_mask: 0.2495  d0.loss_dice: 2.3861  d1.loss_cls: 0.8718  d1.loss_mask: 0.2213  d1.loss_dice: 2.1742  d2.loss_cls: 0.6984  d2.loss_mask: 0.2028  d2.loss_dice: 2.1048  d3.loss_cls: 0.6252  d3.loss_mask: 0.1952  d3.loss_dice: 2.0419  d4.loss_cls: 0.5983  d4.loss_mask: 0.1889  d4.loss_dice: 2.0134  d5.loss_cls: 0.5904  d5.loss_mask: 0.1883  d5.loss_dice: 2.0075  d6.loss_cls: 0.5873  d6.loss_mask: 0.1888  d6.loss_dice: 2.0091  d7.loss_cls: 0.5869  d7.loss_mask: 0.1860  d7.loss_dice: 2.0015  d8.loss_cls: 0.5894  d8.loss_mask: 0.1857  d8.loss_dice: 1.9857
05/11 08:53:18 - mmengine - INFO - Iter(train) [ 6300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:46:30  time: 1.5294  data_time: 0.0286  memory: 38765  grad_norm: 37.5471  loss: 28.5219  loss_cls: 0.6119  loss_mask: 0.2007  loss_dice: 1.8698  d0.loss_cls: 0.8738  d0.loss_mask: 0.2735  d0.loss_dice: 2.2763  d1.loss_cls: 0.8550  d1.loss_mask: 0.2372  d1.loss_dice: 2.0740  d2.loss_cls: 0.6934  d2.loss_mask: 0.2182  d2.loss_dice: 2.0054  d3.loss_cls: 0.6359  d3.loss_mask: 0.2128  d3.loss_dice: 1.9507  d4.loss_cls: 0.6083  d4.loss_mask: 0.2071  d4.loss_dice: 1.9154  d5.loss_cls: 0.6049  d5.loss_mask: 0.2059  d5.loss_dice: 1.9024  d6.loss_cls: 0.6102  d6.loss_mask: 0.2054  d6.loss_dice: 1.8955  d7.loss_cls: 0.6079  d7.loss_mask: 0.2031  d7.loss_dice: 1.8869  d8.loss_cls: 0.6115  d8.loss_mask: 0.2006  d8.loss_dice: 1.8686
05/11 08:54:34 - mmengine - INFO - Iter(train) [ 6350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:45:02  time: 1.5174  data_time: 0.0300  memory: 38735  grad_norm: 38.0349  loss: 28.8712  loss_cls: 0.5860  loss_mask: 0.1911  loss_dice: 1.9422  d0.loss_cls: 0.8580  d0.loss_mask: 0.2558  d0.loss_dice: 2.3323  d1.loss_cls: 0.8529  d1.loss_mask: 0.2269  d1.loss_dice: 2.1460  d2.loss_cls: 0.6865  d2.loss_mask: 0.2077  d2.loss_dice: 2.0709  d3.loss_cls: 0.6198  d3.loss_mask: 0.1988  d3.loss_dice: 2.0154  d4.loss_cls: 0.5898  d4.loss_mask: 0.1961  d4.loss_dice: 1.9683  d5.loss_cls: 0.5850  d5.loss_mask: 0.1948  d5.loss_dice: 1.9606  d6.loss_cls: 0.5811  d6.loss_mask: 0.1951  d6.loss_dice: 1.9632  d7.loss_cls: 0.5875  d7.loss_mask: 0.1926  d7.loss_dice: 1.9532  d8.loss_cls: 0.5885  d8.loss_mask: 0.1925  d8.loss_dice: 1.9325
05/11 08:55:51 - mmengine - INFO - Iter(train) [ 6400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:43:37  time: 1.5352  data_time: 0.0310  memory: 38962  grad_norm: 36.5437  loss: 27.8274  loss_cls: 0.6017  loss_mask: 0.1878  loss_dice: 1.8299  d0.loss_cls: 0.8593  d0.loss_mask: 0.2483  d0.loss_dice: 2.2229  d1.loss_cls: 0.8682  d1.loss_mask: 0.2245  d1.loss_dice: 2.0323  d2.loss_cls: 0.6921  d2.loss_mask: 0.2013  d2.loss_dice: 1.9530  d3.loss_cls: 0.6297  d3.loss_mask: 0.1950  d3.loss_dice: 1.8999  d4.loss_cls: 0.6046  d4.loss_mask: 0.1907  d4.loss_dice: 1.8585  d5.loss_cls: 0.5999  d5.loss_mask: 0.1895  d5.loss_dice: 1.8564  d6.loss_cls: 0.5969  d6.loss_mask: 0.1901  d6.loss_dice: 1.8537  d7.loss_cls: 0.5984  d7.loss_mask: 0.1879  d7.loss_dice: 1.8364  d8.loss_cls: 0.5995  d8.loss_mask: 0.1878  d8.loss_dice: 1.8310
05/11 08:57:07 - mmengine - INFO - Iter(train) [ 6450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:42:10  time: 1.5267  data_time: 0.0290  memory: 38791  grad_norm: 35.0680  loss: 28.4967  loss_cls: 0.5805  loss_mask: 0.1759  loss_dice: 1.9286  d0.loss_cls: 0.8491  d0.loss_mask: 0.2407  d0.loss_dice: 2.2849  d1.loss_cls: 0.8500  d1.loss_mask: 0.2130  d1.loss_dice: 2.1082  d2.loss_cls: 0.6821  d2.loss_mask: 0.1918  d2.loss_dice: 2.0373  d3.loss_cls: 0.6213  d3.loss_mask: 0.1845  d3.loss_dice: 1.9880  d4.loss_cls: 0.5975  d4.loss_mask: 0.1814  d4.loss_dice: 1.9591  d5.loss_cls: 0.5825  d5.loss_mask: 0.1799  d5.loss_dice: 1.9593  d6.loss_cls: 0.5783  d6.loss_mask: 0.1795  d6.loss_dice: 1.9585  d7.loss_cls: 0.5805  d7.loss_mask: 0.1777  d7.loss_dice: 1.9433  d8.loss_cls: 0.5814  d8.loss_mask: 0.1775  d8.loss_dice: 1.9243
05/11 08:58:24 - mmengine - INFO - Iter(train) [ 6500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:40:46  time: 1.5375  data_time: 0.0291  memory: 39025  grad_norm: 34.6749  loss: 30.8700  loss_cls: 0.6317  loss_mask: 0.2079  loss_dice: 2.0698  d0.loss_cls: 0.8929  d0.loss_mask: 0.2758  d0.loss_dice: 2.5116  d1.loss_cls: 0.8761  d1.loss_mask: 0.2446  d1.loss_dice: 2.2873  d2.loss_cls: 0.7222  d2.loss_mask: 0.2244  d2.loss_dice: 2.2076  d3.loss_cls: 0.6597  d3.loss_mask: 0.2172  d3.loss_dice: 2.1499  d4.loss_cls: 0.6351  d4.loss_mask: 0.2138  d4.loss_dice: 2.1093  d5.loss_cls: 0.6302  d5.loss_mask: 0.2125  d5.loss_dice: 2.1044  d6.loss_cls: 0.6309  d6.loss_mask: 0.2130  d6.loss_dice: 2.0999  d7.loss_cls: 0.6300  d7.loss_mask: 0.2111  d7.loss_dice: 2.0934  d8.loss_cls: 0.6309  d8.loss_mask: 0.2092  d8.loss_dice: 2.0677
05/11 08:59:40 - mmengine - INFO - Iter(train) [ 6550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:39:19  time: 1.5262  data_time: 0.0301  memory: 38745  grad_norm: 39.1085  loss: 28.7126  loss_cls: 0.5837  loss_mask: 0.1889  loss_dice: 1.9179  d0.loss_cls: 0.8599  d0.loss_mask: 0.2511  d0.loss_dice: 2.3346  d1.loss_cls: 0.8531  d1.loss_mask: 0.2270  d1.loss_dice: 2.1404  d2.loss_cls: 0.6722  d2.loss_mask: 0.2047  d2.loss_dice: 2.0711  d3.loss_cls: 0.6181  d3.loss_mask: 0.1969  d3.loss_dice: 2.0111  d4.loss_cls: 0.5969  d4.loss_mask: 0.1939  d4.loss_dice: 1.9568  d5.loss_cls: 0.5845  d5.loss_mask: 0.1923  d5.loss_dice: 1.9472  d6.loss_cls: 0.5805  d6.loss_mask: 0.1926  d6.loss_dice: 1.9431  d7.loss_cls: 0.5814  d7.loss_mask: 0.1905  d7.loss_dice: 1.9332  d8.loss_cls: 0.5845  d8.loss_mask: 0.1881  d8.loss_dice: 1.9163
05/11 09:00:58 - mmengine - INFO - Iter(train) [ 6600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:37:56  time: 1.5467  data_time: 0.0292  memory: 38618  grad_norm: 38.0428  loss: 27.6518  loss_cls: 0.5900  loss_mask: 0.1839  loss_dice: 1.8154  d0.loss_cls: 0.8778  d0.loss_mask: 0.2496  d0.loss_dice: 2.2134  d1.loss_cls: 0.8563  d1.loss_mask: 0.2232  d1.loss_dice: 2.0246  d2.loss_cls: 0.6832  d2.loss_mask: 0.2035  d2.loss_dice: 1.9483  d3.loss_cls: 0.6265  d3.loss_mask: 0.1937  d3.loss_dice: 1.8920  d4.loss_cls: 0.5980  d4.loss_mask: 0.1895  d4.loss_dice: 1.8490  d5.loss_cls: 0.5891  d5.loss_mask: 0.1881  d5.loss_dice: 1.8432  d6.loss_cls: 0.5932  d6.loss_mask: 0.1871  d6.loss_dice: 1.8379  d7.loss_cls: 0.5881  d7.loss_mask: 0.1850  d7.loss_dice: 1.8282  d8.loss_cls: 0.5938  d8.loss_mask: 0.1848  d8.loss_dice: 1.8154
05/11 09:02:14 - mmengine - INFO - Iter(train) [ 6650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:36:31  time: 1.5338  data_time: 0.0310  memory: 38729  grad_norm: 35.5852  loss: 27.7334  loss_cls: 0.5682  loss_mask: 0.1918  loss_dice: 1.8418  d0.loss_cls: 0.8651  d0.loss_mask: 0.2531  d0.loss_dice: 2.2308  d1.loss_cls: 0.8453  d1.loss_mask: 0.2265  d1.loss_dice: 2.0342  d2.loss_cls: 0.6604  d2.loss_mask: 0.2058  d2.loss_dice: 1.9659  d3.loss_cls: 0.6001  d3.loss_mask: 0.1997  d3.loss_dice: 1.9111  d4.loss_cls: 0.5736  d4.loss_mask: 0.1961  d4.loss_dice: 1.8728  d5.loss_cls: 0.5661  d5.loss_mask: 0.1948  d5.loss_dice: 1.8728  d6.loss_cls: 0.5678  d6.loss_mask: 0.1947  d6.loss_dice: 1.8719  d7.loss_cls: 0.5664  d7.loss_mask: 0.1933  d7.loss_dice: 1.8594  d8.loss_cls: 0.5757  d8.loss_mask: 0.1919  d8.loss_dice: 1.8362
05/11 09:03:32 - mmengine - INFO - Iter(train) [ 6700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:35:08  time: 1.5431  data_time: 0.0310  memory: 38790  grad_norm: 39.0816  loss: 30.4490  loss_cls: 0.5951  loss_mask: 0.1956  loss_dice: 2.0648  d0.loss_cls: 0.8730  d0.loss_mask: 0.2675  d0.loss_dice: 2.5079  d1.loss_cls: 0.8746  d1.loss_mask: 0.2354  d1.loss_dice: 2.2875  d2.loss_cls: 0.7018  d2.loss_mask: 0.2132  d2.loss_dice: 2.2052  d3.loss_cls: 0.6279  d3.loss_mask: 0.2072  d3.loss_dice: 2.1584  d4.loss_cls: 0.6055  d4.loss_mask: 0.2019  d4.loss_dice: 2.1035  d5.loss_cls: 0.5972  d5.loss_mask: 0.2008  d5.loss_dice: 2.0995  d6.loss_cls: 0.5967  d6.loss_mask: 0.1993  d6.loss_dice: 2.0956  d7.loss_cls: 0.5921  d7.loss_mask: 0.1976  d7.loss_dice: 2.0825  d8.loss_cls: 0.5956  d8.loss_mask: 0.1958  d8.loss_dice: 2.0706
05/11 09:04:47 - mmengine - INFO - Iter(train) [ 6750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:33:41  time: 1.5140  data_time: 0.0295  memory: 38896  grad_norm: 32.4917  loss: 30.0103  loss_cls: 0.5967  loss_mask: 0.2018  loss_dice: 2.0205  d0.loss_cls: 0.8833  d0.loss_mask: 0.2639  d0.loss_dice: 2.4546  d1.loss_cls: 0.8540  d1.loss_mask: 0.2348  d1.loss_dice: 2.2470  d2.loss_cls: 0.6872  d2.loss_mask: 0.2210  d2.loss_dice: 2.1668  d3.loss_cls: 0.6259  d3.loss_mask: 0.2119  d3.loss_dice: 2.1128  d4.loss_cls: 0.6047  d4.loss_mask: 0.2053  d4.loss_dice: 2.0634  d5.loss_cls: 0.5939  d5.loss_mask: 0.2048  d5.loss_dice: 2.0529  d6.loss_cls: 0.5972  d6.loss_mask: 0.2050  d6.loss_dice: 2.0462  d7.loss_cls: 0.5982  d7.loss_mask: 0.2036  d7.loss_dice: 2.0367  d8.loss_cls: 0.5961  d8.loss_mask: 0.2022  d8.loss_dice: 2.0178
05/11 09:06:04 - mmengine - INFO - Iter(train) [ 6800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:32:17  time: 1.5382  data_time: 0.0354  memory: 39164  grad_norm: 32.3608  loss: 28.3201  loss_cls: 0.5896  loss_mask: 0.1853  loss_dice: 1.8869  d0.loss_cls: 0.8745  d0.loss_mask: 0.2462  d0.loss_dice: 2.2670  d1.loss_cls: 0.8572  d1.loss_mask: 0.2187  d1.loss_dice: 2.0777  d2.loss_cls: 0.6953  d2.loss_mask: 0.2003  d2.loss_dice: 2.0034  d3.loss_cls: 0.6263  d3.loss_mask: 0.1941  d3.loss_dice: 1.9608  d4.loss_cls: 0.6024  d4.loss_mask: 0.1904  d4.loss_dice: 1.9169  d5.loss_cls: 0.5939  d5.loss_mask: 0.1882  d5.loss_dice: 1.9151  d6.loss_cls: 0.5893  d6.loss_mask: 0.1882  d6.loss_dice: 1.9078  d7.loss_cls: 0.5938  d7.loss_mask: 0.1858  d7.loss_dice: 1.9005  d8.loss_cls: 0.5904  d8.loss_mask: 0.1847  d8.loss_dice: 1.8896
05/11 09:07:20 - mmengine - INFO - Iter(train) [ 6850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:30:51  time: 1.5222  data_time: 0.0313  memory: 39295  grad_norm: 32.2400  loss: 28.0318  loss_cls: 0.5783  loss_mask: 0.1904  loss_dice: 1.8632  d0.loss_cls: 0.8763  d0.loss_mask: 0.2549  d0.loss_dice: 2.2588  d1.loss_cls: 0.8367  d1.loss_mask: 0.2280  d1.loss_dice: 2.0713  d2.loss_cls: 0.6660  d2.loss_mask: 0.2041  d2.loss_dice: 1.9902  d3.loss_cls: 0.6068  d3.loss_mask: 0.1974  d3.loss_dice: 1.9407  d4.loss_cls: 0.5837  d4.loss_mask: 0.1938  d4.loss_dice: 1.8944  d5.loss_cls: 0.5812  d5.loss_mask: 0.1927  d5.loss_dice: 1.8889  d6.loss_cls: 0.5820  d6.loss_mask: 0.1914  d6.loss_dice: 1.8847  d7.loss_cls: 0.5815  d7.loss_mask: 0.1910  d7.loss_dice: 1.8737  d8.loss_cls: 0.5801  d8.loss_mask: 0.1900  d8.loss_dice: 1.8596
05/11 09:08:37 - mmengine - INFO - Iter(train) [ 6900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:29:26  time: 1.5325  data_time: 0.0316  memory: 38259  grad_norm: 42.7809  loss: 27.9683  loss_cls: 0.5985  loss_mask: 0.2006  loss_dice: 1.8343  d0.loss_cls: 0.8579  d0.loss_mask: 0.2744  d0.loss_dice: 2.2184  d1.loss_cls: 0.8569  d1.loss_mask: 0.2401  d1.loss_dice: 2.0339  d2.loss_cls: 0.6882  d2.loss_mask: 0.2174  d2.loss_dice: 1.9545  d3.loss_cls: 0.6279  d3.loss_mask: 0.2077  d3.loss_dice: 1.9061  d4.loss_cls: 0.6027  d4.loss_mask: 0.2049  d4.loss_dice: 1.8651  d5.loss_cls: 0.5921  d5.loss_mask: 0.2035  d5.loss_dice: 1.8582  d6.loss_cls: 0.5957  d6.loss_mask: 0.2026  d6.loss_dice: 1.8566  d7.loss_cls: 0.5926  d7.loss_mask: 0.2020  d7.loss_dice: 1.8455  d8.loss_cls: 0.5936  d8.loss_mask: 0.2009  d8.loss_dice: 1.8357
05/11 09:09:53 - mmengine - INFO - Iter(train) [ 6950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:28:01  time: 1.5265  data_time: 0.0316  memory: 39258  grad_norm: 33.0258  loss: 29.1129  loss_cls: 0.5984  loss_mask: 0.1903  loss_dice: 1.9454  d0.loss_cls: 0.8854  d0.loss_mask: 0.2541  d0.loss_dice: 2.3607  d1.loss_cls: 0.8478  d1.loss_mask: 0.2272  d1.loss_dice: 2.1649  d2.loss_cls: 0.6827  d2.loss_mask: 0.2081  d2.loss_dice: 2.0857  d3.loss_cls: 0.6224  d3.loss_mask: 0.2024  d3.loss_dice: 2.0328  d4.loss_cls: 0.6034  d4.loss_mask: 0.1989  d4.loss_dice: 1.9824  d5.loss_cls: 0.5940  d5.loss_mask: 0.1973  d5.loss_dice: 1.9810  d6.loss_cls: 0.5891  d6.loss_mask: 0.1951  d6.loss_dice: 1.9759  d7.loss_cls: 0.5927  d7.loss_mask: 0.1932  d7.loss_dice: 1.9672  d8.loss_cls: 0.5999  d8.loss_mask: 0.1918  d8.loss_dice: 1.9426
05/11 09:11:10 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 09:11:10 - mmengine - INFO - Iter(train) [ 7000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:26:38  time: 1.5433  data_time: 0.0308  memory: 39282  grad_norm: 39.1620  loss: 29.4773  loss_cls: 0.6020  loss_mask: 0.1916  loss_dice: 1.9644  d0.loss_cls: 0.9039  d0.loss_mask: 0.2548  d0.loss_dice: 2.4210  d1.loss_cls: 0.8597  d1.loss_mask: 0.2308  d1.loss_dice: 2.2042  d2.loss_cls: 0.6908  d2.loss_mask: 0.2106  d2.loss_dice: 2.1205  d3.loss_cls: 0.6299  d3.loss_mask: 0.1999  d3.loss_dice: 2.0575  d4.loss_cls: 0.6078  d4.loss_mask: 0.1947  d4.loss_dice: 2.0032  d5.loss_cls: 0.5973  d5.loss_mask: 0.1947  d5.loss_dice: 2.0005  d6.loss_cls: 0.5982  d6.loss_mask: 0.1944  d6.loss_dice: 2.0023  d7.loss_cls: 0.5966  d7.loss_mask: 0.1937  d7.loss_dice: 1.9891  d8.loss_cls: 0.6020  d8.loss_mask: 0.1914  d8.loss_dice: 1.9700
05/11 09:12:28 - mmengine - INFO - Iter(train) [ 7050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:25:16  time: 1.5469  data_time: 0.0301  memory: 39286  grad_norm: 34.0895  loss: 30.5409  loss_cls: 0.6297  loss_mask: 0.2036  loss_dice: 2.0588  d0.loss_cls: 0.8969  d0.loss_mask: 0.2683  d0.loss_dice: 2.4635  d1.loss_cls: 0.8630  d1.loss_mask: 0.2416  d1.loss_dice: 2.2773  d2.loss_cls: 0.7027  d2.loss_mask: 0.2211  d2.loss_dice: 2.1886  d3.loss_cls: 0.6471  d3.loss_mask: 0.2126  d3.loss_dice: 2.1315  d4.loss_cls: 0.6236  d4.loss_mask: 0.2089  d4.loss_dice: 2.0899  d5.loss_cls: 0.6189  d5.loss_mask: 0.2071  d5.loss_dice: 2.0908  d6.loss_cls: 0.6229  d6.loss_mask: 0.2055  d6.loss_dice: 2.0818  d7.loss_cls: 0.6205  d7.loss_mask: 0.2047  d7.loss_dice: 2.0782  d8.loss_cls: 0.6258  d8.loss_mask: 0.2040  d8.loss_dice: 2.0522
05/11 09:13:45 - mmengine - INFO - Iter(train) [ 7100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:23:53  time: 1.5411  data_time: 0.0296  memory: 38258  grad_norm: 33.2775  loss: 27.7276  loss_cls: 0.6001  loss_mask: 0.1873  loss_dice: 1.8196  d0.loss_cls: 0.8704  d0.loss_mask: 0.2527  d0.loss_dice: 2.1808  d1.loss_cls: 0.8590  d1.loss_mask: 0.2231  d1.loss_dice: 2.0174  d2.loss_cls: 0.6967  d2.loss_mask: 0.2041  d2.loss_dice: 1.9434  d3.loss_cls: 0.6333  d3.loss_mask: 0.1953  d3.loss_dice: 1.8972  d4.loss_cls: 0.6085  d4.loss_mask: 0.1910  d4.loss_dice: 1.8544  d5.loss_cls: 0.6039  d5.loss_mask: 0.1901  d5.loss_dice: 1.8440  d6.loss_cls: 0.5997  d6.loss_mask: 0.1901  d6.loss_dice: 1.8416  d7.loss_cls: 0.6011  d7.loss_mask: 0.1882  d7.loss_dice: 1.8300  d8.loss_cls: 0.6014  d8.loss_mask: 0.1885  d8.loss_dice: 1.8149
05/11 09:15:02 - mmengine - INFO - Iter(train) [ 7150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:22:30  time: 1.5397  data_time: 0.0319  memory: 38776  grad_norm: 33.7305  loss: 29.5379  loss_cls: 0.6056  loss_mask: 0.1981  loss_dice: 1.9682  d0.loss_cls: 0.8811  d0.loss_mask: 0.2698  d0.loss_dice: 2.4051  d1.loss_cls: 0.8729  d1.loss_mask: 0.2375  d1.loss_dice: 2.2048  d2.loss_cls: 0.7051  d2.loss_mask: 0.2156  d2.loss_dice: 2.1022  d3.loss_cls: 0.6402  d3.loss_mask: 0.2081  d3.loss_dice: 2.0549  d4.loss_cls: 0.6099  d4.loss_mask: 0.2042  d4.loss_dice: 2.0004  d5.loss_cls: 0.6033  d5.loss_mask: 0.2017  d5.loss_dice: 1.9976  d6.loss_cls: 0.6016  d6.loss_mask: 0.2024  d6.loss_dice: 1.9974  d7.loss_cls: 0.6015  d7.loss_mask: 0.2007  d7.loss_dice: 1.9830  d8.loss_cls: 0.6032  d8.loss_mask: 0.1986  d8.loss_dice: 1.9630
05/11 09:16:19 - mmengine - INFO - Iter(train) [ 7200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:21:08  time: 1.5440  data_time: 0.0370  memory: 38743  grad_norm: 41.8839  loss: 27.8764  loss_cls: 0.5859  loss_mask: 0.1808  loss_dice: 1.8536  d0.loss_cls: 0.8981  d0.loss_mask: 0.2435  d0.loss_dice: 2.2265  d1.loss_cls: 0.8484  d1.loss_mask: 0.2140  d1.loss_dice: 2.0570  d2.loss_cls: 0.6787  d2.loss_mask: 0.1972  d2.loss_dice: 1.9853  d3.loss_cls: 0.6168  d3.loss_mask: 0.1885  d3.loss_dice: 1.9229  d4.loss_cls: 0.5920  d4.loss_mask: 0.1860  d4.loss_dice: 1.8760  d5.loss_cls: 0.5849  d5.loss_mask: 0.1833  d5.loss_dice: 1.8741  d6.loss_cls: 0.5873  d6.loss_mask: 0.1825  d6.loss_dice: 1.8746  d7.loss_cls: 0.5857  d7.loss_mask: 0.1824  d7.loss_dice: 1.8594  d8.loss_cls: 0.5839  d8.loss_mask: 0.1815  d8.loss_dice: 1.8456
05/11 09:17:36 - mmengine - INFO - Iter(train) [ 7250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:19:44  time: 1.5312  data_time: 0.0297  memory: 38581  grad_norm: 31.5004  loss: 27.4244  loss_cls: 0.5765  loss_mask: 0.1791  loss_dice: 1.8099  d0.loss_cls: 0.8844  d0.loss_mask: 0.2456  d0.loss_dice: 2.1896  d1.loss_cls: 0.8437  d1.loss_mask: 0.2159  d1.loss_dice: 2.0175  d2.loss_cls: 0.6698  d2.loss_mask: 0.1936  d2.loss_dice: 1.9515  d3.loss_cls: 0.6047  d3.loss_mask: 0.1879  d3.loss_dice: 1.8964  d4.loss_cls: 0.5799  d4.loss_mask: 0.1845  d4.loss_dice: 1.8536  d5.loss_cls: 0.5741  d5.loss_mask: 0.1840  d5.loss_dice: 1.8441  d6.loss_cls: 0.5707  d6.loss_mask: 0.1828  d6.loss_dice: 1.8401  d7.loss_cls: 0.5743  d7.loss_mask: 0.1799  d7.loss_dice: 1.8243  d8.loss_cls: 0.5735  d8.loss_mask: 0.1792  d8.loss_dice: 1.8135
05/11 09:18:53 - mmengine - INFO - Iter(train) [ 7300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:18:22  time: 1.5471  data_time: 0.0335  memory: 39164  grad_norm: 32.4632  loss: 30.4613  loss_cls: 0.6148  loss_mask: 0.2069  loss_dice: 2.0403  d0.loss_cls: 0.9193  d0.loss_mask: 0.2828  d0.loss_dice: 2.4724  d1.loss_cls: 0.8592  d1.loss_mask: 0.2489  d1.loss_dice: 2.2792  d2.loss_cls: 0.6963  d2.loss_mask: 0.2242  d2.loss_dice: 2.1920  d3.loss_cls: 0.6405  d3.loss_mask: 0.2177  d3.loss_dice: 2.1270  d4.loss_cls: 0.6177  d4.loss_mask: 0.2136  d4.loss_dice: 2.0778  d5.loss_cls: 0.6124  d5.loss_mask: 0.2116  d5.loss_dice: 2.0716  d6.loss_cls: 0.6148  d6.loss_mask: 0.2106  d6.loss_dice: 2.0674  d7.loss_cls: 0.6158  d7.loss_mask: 0.2089  d7.loss_dice: 2.0540  d8.loss_cls: 0.6154  d8.loss_mask: 0.2081  d8.loss_dice: 2.0401
05/11 09:20:09 - mmengine - INFO - Iter(train) [ 7350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:16:56  time: 1.5176  data_time: 0.0290  memory: 38731  grad_norm: 37.1477  loss: 29.1225  loss_cls: 0.5932  loss_mask: 0.1961  loss_dice: 1.9464  d0.loss_cls: 0.8891  d0.loss_mask: 0.2589  d0.loss_dice: 2.3625  d1.loss_cls: 0.8478  d1.loss_mask: 0.2336  d1.loss_dice: 2.1779  d2.loss_cls: 0.6688  d2.loss_mask: 0.2152  d2.loss_dice: 2.0892  d3.loss_cls: 0.6175  d3.loss_mask: 0.2058  d3.loss_dice: 2.0290  d4.loss_cls: 0.5912  d4.loss_mask: 0.2014  d4.loss_dice: 1.9883  d5.loss_cls: 0.5895  d5.loss_mask: 0.1991  d5.loss_dice: 1.9763  d6.loss_cls: 0.5866  d6.loss_mask: 0.1999  d6.loss_dice: 1.9737  d7.loss_cls: 0.5885  d7.loss_mask: 0.1977  d7.loss_dice: 1.9640  d8.loss_cls: 0.5917  d8.loss_mask: 0.1963  d8.loss_dice: 1.9477
05/11 09:21:27 - mmengine - INFO - Iter(train) [ 7400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:15:36  time: 1.5580  data_time: 0.0289  memory: 39723  grad_norm: 34.0007  loss: 32.2547  loss_cls: 0.6411  loss_mask: 0.2189  loss_dice: 2.1710  d0.loss_cls: 0.9432  d0.loss_mask: 0.2927  d0.loss_dice: 2.6361  d1.loss_cls: 0.8723  d1.loss_mask: 0.2568  d1.loss_dice: 2.4366  d2.loss_cls: 0.7228  d2.loss_mask: 0.2365  d2.loss_dice: 2.3392  d3.loss_cls: 0.6736  d3.loss_mask: 0.2300  d3.loss_dice: 2.2629  d4.loss_cls: 0.6474  d4.loss_mask: 0.2270  d4.loss_dice: 2.2091  d5.loss_cls: 0.6399  d5.loss_mask: 0.2233  d5.loss_dice: 2.2106  d6.loss_cls: 0.6409  d6.loss_mask: 0.2231  d6.loss_dice: 2.2097  d7.loss_cls: 0.6458  d7.loss_mask: 0.2216  d7.loss_dice: 2.1890  d8.loss_cls: 0.6459  d8.loss_mask: 0.2207  d8.loss_dice: 2.1668
05/11 09:22:44 - mmengine - INFO - Iter(train) [ 7450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:14:14  time: 1.5495  data_time: 0.0264  memory: 39790  grad_norm: 34.3296  loss: 29.2154  loss_cls: 0.6142  loss_mask: 0.1809  loss_dice: 1.9642  d0.loss_cls: 0.9406  d0.loss_mask: 0.2388  d0.loss_dice: 2.3190  d1.loss_cls: 0.8472  d1.loss_mask: 0.2152  d1.loss_dice: 2.1512  d2.loss_cls: 0.6802  d2.loss_mask: 0.1976  d2.loss_dice: 2.0913  d3.loss_cls: 0.6329  d3.loss_mask: 0.1888  d3.loss_dice: 2.0436  d4.loss_cls: 0.6146  d4.loss_mask: 0.1867  d4.loss_dice: 2.0030  d5.loss_cls: 0.6111  d5.loss_mask: 0.1856  d5.loss_dice: 1.9949  d6.loss_cls: 0.6090  d6.loss_mask: 0.1849  d6.loss_dice: 1.9910  d7.loss_cls: 0.6067  d7.loss_mask: 0.1834  d7.loss_dice: 1.9768  d8.loss_cls: 0.6156  d8.loss_mask: 0.1818  d8.loss_dice: 1.9646
05/11 09:24:01 - mmengine - INFO - Iter(train) [ 7500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:12:52  time: 1.5387  data_time: 0.0314  memory: 38532  grad_norm: 34.9279  loss: 28.4096  loss_cls: 0.5876  loss_mask: 0.1775  loss_dice: 1.9077  d0.loss_cls: 0.8903  d0.loss_mask: 0.2374  d0.loss_dice: 2.2838  d1.loss_cls: 0.8410  d1.loss_mask: 0.2118  d1.loss_dice: 2.1114  d2.loss_cls: 0.6749  d2.loss_mask: 0.1921  d2.loss_dice: 2.0413  d3.loss_cls: 0.6138  d3.loss_mask: 0.1849  d3.loss_dice: 1.9890  d4.loss_cls: 0.5941  d4.loss_mask: 0.1814  d4.loss_dice: 1.9384  d5.loss_cls: 0.5889  d5.loss_mask: 0.1801  d5.loss_dice: 1.9270  d6.loss_cls: 0.5845  d6.loss_mask: 0.1797  d6.loss_dice: 1.9322  d7.loss_cls: 0.5862  d7.loss_mask: 0.1784  d7.loss_dice: 1.9186  d8.loss_cls: 0.5895  d8.loss_mask: 0.1766  d8.loss_dice: 1.9095
05/11 09:25:18 - mmengine - INFO - Iter(train) [ 7550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:11:28  time: 1.5317  data_time: 0.0285  memory: 38965  grad_norm: 32.9599  loss: 29.1066  loss_cls: 0.5974  loss_mask: 0.1981  loss_dice: 1.9348  d0.loss_cls: 0.8986  d0.loss_mask: 0.2739  d0.loss_dice: 2.3504  d1.loss_cls: 0.8440  d1.loss_mask: 0.2377  d1.loss_dice: 2.1619  d2.loss_cls: 0.6797  d2.loss_mask: 0.2184  d2.loss_dice: 2.0738  d3.loss_cls: 0.6237  d3.loss_mask: 0.2086  d3.loss_dice: 2.0319  d4.loss_cls: 0.6009  d4.loss_mask: 0.2061  d4.loss_dice: 1.9726  d5.loss_cls: 0.5936  d5.loss_mask: 0.2035  d5.loss_dice: 1.9679  d6.loss_cls: 0.5954  d6.loss_mask: 0.2023  d6.loss_dice: 1.9574  d7.loss_cls: 0.5980  d7.loss_mask: 0.2004  d7.loss_dice: 1.9451  d8.loss_cls: 0.5971  d8.loss_mask: 0.1996  d8.loss_dice: 1.9340
05/11 09:26:34 - mmengine - INFO - Iter(train) [ 7600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:10:05  time: 1.5320  data_time: 0.0298  memory: 38794  grad_norm: 30.4356  loss: 29.6289  loss_cls: 0.5729  loss_mask: 0.1923  loss_dice: 2.0158  d0.loss_cls: 0.9058  d0.loss_mask: 0.2561  d0.loss_dice: 2.4216  d1.loss_cls: 0.8435  d1.loss_mask: 0.2289  d1.loss_dice: 2.2215  d2.loss_cls: 0.6625  d2.loss_mask: 0.2082  d2.loss_dice: 2.1555  d3.loss_cls: 0.6076  d3.loss_mask: 0.2014  d3.loss_dice: 2.0982  d4.loss_cls: 0.5806  d4.loss_mask: 0.1986  d4.loss_dice: 2.0506  d5.loss_cls: 0.5685  d5.loss_mask: 0.1968  d5.loss_dice: 2.0518  d6.loss_cls: 0.5675  d6.loss_mask: 0.1962  d6.loss_dice: 2.0492  d7.loss_cls: 0.5720  d7.loss_mask: 0.1937  d7.loss_dice: 2.0349  d8.loss_cls: 0.5717  d8.loss_mask: 0.1927  d8.loss_dice: 2.0125
05/11 09:27:51 - mmengine - INFO - Iter(train) [ 7650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:08:43  time: 1.5390  data_time: 0.0281  memory: 38556  grad_norm: 31.7466  loss: 28.2900  loss_cls: 0.5821  loss_mask: 0.1777  loss_dice: 1.9053  d0.loss_cls: 0.8890  d0.loss_mask: 0.2388  d0.loss_dice: 2.2615  d1.loss_cls: 0.8351  d1.loss_mask: 0.2113  d1.loss_dice: 2.0942  d2.loss_cls: 0.6715  d2.loss_mask: 0.1931  d2.loss_dice: 2.0328  d3.loss_cls: 0.6139  d3.loss_mask: 0.1856  d3.loss_dice: 1.9831  d4.loss_cls: 0.5884  d4.loss_mask: 0.1816  d4.loss_dice: 1.9338  d5.loss_cls: 0.5812  d5.loss_mask: 0.1805  d5.loss_dice: 1.9292  d6.loss_cls: 0.5828  d6.loss_mask: 0.1791  d6.loss_dice: 1.9181  d7.loss_cls: 0.5808  d7.loss_mask: 0.1786  d7.loss_dice: 1.9164  d8.loss_cls: 0.5854  d8.loss_mask: 0.1781  d8.loss_dice: 1.9010
05/11 09:29:08 - mmengine - INFO - Iter(train) [ 7700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:07:21  time: 1.5412  data_time: 0.0313  memory: 39270  grad_norm: 32.4685  loss: 28.0362  loss_cls: 0.5919  loss_mask: 0.1819  loss_dice: 1.8479  d0.loss_cls: 0.8989  d0.loss_mask: 0.2455  d0.loss_dice: 2.2652  d1.loss_cls: 0.8561  d1.loss_mask: 0.2182  d1.loss_dice: 2.0763  d2.loss_cls: 0.6815  d2.loss_mask: 0.1999  d2.loss_dice: 1.9888  d3.loss_cls: 0.6199  d3.loss_mask: 0.1936  d3.loss_dice: 1.9361  d4.loss_cls: 0.5986  d4.loss_mask: 0.1877  d4.loss_dice: 1.8888  d5.loss_cls: 0.5894  d5.loss_mask: 0.1874  d5.loss_dice: 1.8767  d6.loss_cls: 0.5911  d6.loss_mask: 0.1859  d6.loss_dice: 1.8744  d7.loss_cls: 0.5901  d7.loss_mask: 0.1835  d7.loss_dice: 1.8636  d8.loss_cls: 0.5913  d8.loss_mask: 0.1831  d8.loss_dice: 1.8429
05/11 09:30:25 - mmengine - INFO - Iter(train) [ 7750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:05:58  time: 1.5351  data_time: 0.0344  memory: 38475  grad_norm: 30.3817  loss: 26.9185  loss_cls: 0.5836  loss_mask: 0.1875  loss_dice: 1.7581  d0.loss_cls: 0.8915  d0.loss_mask: 0.2517  d0.loss_dice: 2.1090  d1.loss_cls: 0.8434  d1.loss_mask: 0.2233  d1.loss_dice: 1.9380  d2.loss_cls: 0.6731  d2.loss_mask: 0.2026  d2.loss_dice: 1.8722  d3.loss_cls: 0.6106  d3.loss_mask: 0.1954  d3.loss_dice: 1.8336  d4.loss_cls: 0.5884  d4.loss_mask: 0.1928  d4.loss_dice: 1.7877  d5.loss_cls: 0.5826  d5.loss_mask: 0.1905  d5.loss_dice: 1.7849  d6.loss_cls: 0.5839  d6.loss_mask: 0.1895  d6.loss_dice: 1.7794  d7.loss_cls: 0.5833  d7.loss_mask: 0.1883  d7.loss_dice: 1.7655  d8.loss_cls: 0.5802  d8.loss_mask: 0.1884  d8.loss_dice: 1.7597
05/11 09:31:41 - mmengine - INFO - Iter(train) [ 7800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:04:35  time: 1.5271  data_time: 0.0282  memory: 38283  grad_norm: 38.5298  loss: 28.3149  loss_cls: 0.6023  loss_mask: 0.1894  loss_dice: 1.8703  d0.loss_cls: 0.8921  d0.loss_mask: 0.2542  d0.loss_dice: 2.2428  d1.loss_cls: 0.8523  d1.loss_mask: 0.2229  d1.loss_dice: 2.0708  d2.loss_cls: 0.6926  d2.loss_mask: 0.2073  d2.loss_dice: 2.0019  d3.loss_cls: 0.6353  d3.loss_mask: 0.1986  d3.loss_dice: 1.9468  d4.loss_cls: 0.6160  d4.loss_mask: 0.1941  d4.loss_dice: 1.9004  d5.loss_cls: 0.5971  d5.loss_mask: 0.1945  d5.loss_dice: 1.9038  d6.loss_cls: 0.6002  d6.loss_mask: 0.1941  d6.loss_dice: 1.8965  d7.loss_cls: 0.5995  d7.loss_mask: 0.1921  d7.loss_dice: 1.8817  d8.loss_cls: 0.6068  d8.loss_mask: 0.1903  d8.loss_dice: 1.8680
05/11 09:32:58 - mmengine - INFO - Iter(train) [ 7850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:03:12  time: 1.5364  data_time: 0.0289  memory: 38995  grad_norm: 34.4159  loss: 30.2113  loss_cls: 0.6049  loss_mask: 0.2028  loss_dice: 2.0361  d0.loss_cls: 0.9092  d0.loss_mask: 0.2669  d0.loss_dice: 2.4409  d1.loss_cls: 0.8551  d1.loss_mask: 0.2345  d1.loss_dice: 2.2514  d2.loss_cls: 0.6900  d2.loss_mask: 0.2184  d2.loss_dice: 2.1738  d3.loss_cls: 0.6370  d3.loss_mask: 0.2105  d3.loss_dice: 2.1227  d4.loss_cls: 0.6067  d4.loss_mask: 0.2078  d4.loss_dice: 2.0748  d5.loss_cls: 0.6021  d5.loss_mask: 0.2063  d5.loss_dice: 2.0725  d6.loss_cls: 0.6035  d6.loss_mask: 0.2061  d6.loss_dice: 2.0659  d7.loss_cls: 0.5998  d7.loss_mask: 0.2052  d7.loss_dice: 2.0589  d8.loss_cls: 0.6052  d8.loss_mask: 0.2027  d8.loss_dice: 2.0398
05/11 09:34:15 - mmengine - INFO - Iter(train) [ 7900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:01:49  time: 1.5300  data_time: 0.0291  memory: 38370  grad_norm: 37.8388  loss: 27.0235  loss_cls: 0.5803  loss_mask: 0.1838  loss_dice: 1.7634  d0.loss_cls: 0.8916  d0.loss_mask: 0.2541  d0.loss_dice: 2.1489  d1.loss_cls: 0.8426  d1.loss_mask: 0.2253  d1.loss_dice: 1.9821  d2.loss_cls: 0.6600  d2.loss_mask: 0.2020  d2.loss_dice: 1.8899  d3.loss_cls: 0.6053  d3.loss_mask: 0.1937  d3.loss_dice: 1.8479  d4.loss_cls: 0.5804  d4.loss_mask: 0.1901  d4.loss_dice: 1.7981  d5.loss_cls: 0.5796  d5.loss_mask: 0.1885  d5.loss_dice: 1.7938  d6.loss_cls: 0.5761  d6.loss_mask: 0.1883  d6.loss_dice: 1.7870  d7.loss_cls: 0.5799  d7.loss_mask: 0.1846  d7.loss_dice: 1.7781  d8.loss_cls: 0.5845  d8.loss_mask: 0.1836  d8.loss_dice: 1.7597
05/11 09:35:32 - mmengine - INFO - Iter(train) [ 7950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:00:27  time: 1.5382  data_time: 0.0305  memory: 38455  grad_norm: 38.2311  loss: 28.0808  loss_cls: 0.6010  loss_mask: 0.1853  loss_dice: 1.8566  d0.loss_cls: 0.8990  d0.loss_mask: 0.2555  d0.loss_dice: 2.2205  d1.loss_cls: 0.8545  d1.loss_mask: 0.2245  d1.loss_dice: 2.0600  d2.loss_cls: 0.6847  d2.loss_mask: 0.2034  d2.loss_dice: 1.9788  d3.loss_cls: 0.6274  d3.loss_mask: 0.1936  d3.loss_dice: 1.9425  d4.loss_cls: 0.6015  d4.loss_mask: 0.1897  d4.loss_dice: 1.8906  d5.loss_cls: 0.5988  d5.loss_mask: 0.1887  d5.loss_dice: 1.8791  d6.loss_cls: 0.5940  d6.loss_mask: 0.1893  d6.loss_dice: 1.8763  d7.loss_cls: 0.5954  d7.loss_mask: 0.1880  d7.loss_dice: 1.8643  d8.loss_cls: 0.6012  d8.loss_mask: 0.1865  d8.loss_dice: 1.8501
05/11 09:36:48 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 09:36:48 - mmengine - INFO - Iter(train) [ 8000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:59:05  time: 1.5340  data_time: 0.0317  memory: 38568  grad_norm: 36.1837  loss: 28.5668  loss_cls: 0.6046  loss_mask: 0.1866  loss_dice: 1.8876  d0.loss_cls: 0.9111  d0.loss_mask: 0.2590  d0.loss_dice: 2.2643  d1.loss_cls: 0.8516  d1.loss_mask: 0.2266  d1.loss_dice: 2.1092  d2.loss_cls: 0.6877  d2.loss_mask: 0.2029  d2.loss_dice: 2.0320  d3.loss_cls: 0.6320  d3.loss_mask: 0.1953  d3.loss_dice: 1.9729  d4.loss_cls: 0.6083  d4.loss_mask: 0.1911  d4.loss_dice: 1.9305  d5.loss_cls: 0.6052  d5.loss_mask: 0.1893  d5.loss_dice: 1.9196  d6.loss_cls: 0.6070  d6.loss_mask: 0.1881  d6.loss_dice: 1.9182  d7.loss_cls: 0.6051  d7.loss_mask: 0.1869  d7.loss_dice: 1.9115  d8.loss_cls: 0.6076  d8.loss_mask: 0.1860  d8.loss_dice: 1.8893
05/11 09:36:48 - mmengine - INFO - Saving checkpoint at 8000 iterations
05/11 09:37:33 - mmengine - INFO - Iter(val) [ 50/100]    eta: 0:00:36  time: 0.7368  data_time: 0.0144  memory: 5704  
05/11 09:38:09 - mmengine - INFO - Iter(val) [100/100]    eta: 0:00:00  time: 0.7242  data_time: 0.0137  memory: 5704  
05/11 09:38:32 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.24s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/11 09:38:39 - mmengine - INFO - start multi processing evaluation ...

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

  0%|          | 0/25552 [00:00<?, ?it/s]
 50%|█████     | 12777/25552 [00:00<00:00, 126154.95it/s]
100%|██████████| 25552/25552 [00:00<00:00, 137167.56it/s]
DONE (t=52.61s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.378
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.711
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.361
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.453
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.904
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.501
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.867
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.484
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.353
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.569
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.501
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.501
05/11 09:39:32 - mmengine - INFO - segm_mAP_copypaste: 0.378 0.711 0.361 0.217 0.453 0.904
05/11 09:39:32 - mmengine - INFO - segm_mAR_copypaste: 0.501 0.867 0.484 0.353 0.569 0.950
05/11 09:39:33 - mmengine - INFO - Iter(val) [100/100]    coco/segm_mAP: 0.3780  coco/segm_mAP_50: 0.7110  coco/segm_mAP_75: 0.3610  coco/segm_mAP_s: 0.2170  coco/segm_mAP_m: 0.4530  coco/segm_mAP_l: 0.9040  data_time: 0.0140  time: 0.7291
05/11 09:39:33 - mmengine - INFO - The previous best checkpoint /data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-L_8xb32-24k_coco/best_coco_segm_mAP_50_iter_6000.pth is removed
05/11 09:39:36 - mmengine - INFO - The best checkpoint with 0.7110 coco/segm_mAP_50 at 8000 iter is saved to best_coco_segm_mAP_50_iter_8000.pth.
05/11 09:40:59 - mmengine - INFO - Iter(train) [ 8050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:00:48  time: 3.4022  data_time: 1.9057  memory: 38664  grad_norm: 39.8878  loss: 28.1920  loss_cls: 0.6011  loss_mask: 0.1793  loss_dice: 1.8636  d0.loss_cls: 0.9142  d0.loss_mask: 0.2431  d0.loss_dice: 2.2523  d1.loss_cls: 0.8608  d1.loss_mask: 0.2164  d1.loss_dice: 2.0778  d2.loss_cls: 0.6815  d2.loss_mask: 0.1971  d2.loss_dice: 1.9961  d3.loss_cls: 0.6294  d3.loss_mask: 0.1909  d3.loss_dice: 1.9536  d4.loss_cls: 0.5999  d4.loss_mask: 0.1856  d4.loss_dice: 1.9040  d5.loss_cls: 0.5938  d5.loss_mask: 0.1848  d5.loss_dice: 1.8969  d6.loss_cls: 0.5915  d6.loss_mask: 0.1832  d6.loss_dice: 1.8983  d7.loss_cls: 0.5946  d7.loss_mask: 0.1818  d7.loss_dice: 1.8833  d8.loss_cls: 0.5974  d8.loss_mask: 0.1805  d8.loss_dice: 1.8593
05/11 09:42:15 - mmengine - INFO - Iter(train) [ 8100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:59:23  time: 1.5269  data_time: 0.0285  memory: 39153  grad_norm: 33.3873  loss: 29.8856  loss_cls: 0.6100  loss_mask: 0.1908  loss_dice: 2.0084  d0.loss_cls: 0.9390  d0.loss_mask: 0.2566  d0.loss_dice: 2.3896  d1.loss_cls: 0.8636  d1.loss_mask: 0.2262  d1.loss_dice: 2.2236  d2.loss_cls: 0.6990  d2.loss_mask: 0.2079  d2.loss_dice: 2.1456  d3.loss_cls: 0.6439  d3.loss_mask: 0.1997  d3.loss_dice: 2.0930  d4.loss_cls: 0.6184  d4.loss_mask: 0.1974  d4.loss_dice: 2.0527  d5.loss_cls: 0.6096  d5.loss_mask: 0.1960  d5.loss_dice: 2.0342  d6.loss_cls: 0.6075  d6.loss_mask: 0.1944  d6.loss_dice: 2.0380  d7.loss_cls: 0.6056  d7.loss_mask: 0.1940  d7.loss_dice: 2.0274  d8.loss_cls: 0.6145  d8.loss_mask: 0.1910  d8.loss_dice: 2.0079
05/11 09:43:32 - mmengine - INFO - Iter(train) [ 8150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:57:59  time: 1.5262  data_time: 0.0286  memory: 39538  grad_norm: 38.6918  loss: 30.4350  loss_cls: 0.6073  loss_mask: 0.2080  loss_dice: 2.0469  d0.loss_cls: 0.9291  d0.loss_mask: 0.2778  d0.loss_dice: 2.4430  d1.loss_cls: 0.8658  d1.loss_mask: 0.2455  d1.loss_dice: 2.2553  d2.loss_cls: 0.7012  d2.loss_mask: 0.2258  d2.loss_dice: 2.1753  d3.loss_cls: 0.6458  d3.loss_mask: 0.2178  d3.loss_dice: 2.1250  d4.loss_cls: 0.6208  d4.loss_mask: 0.2123  d4.loss_dice: 2.0862  d5.loss_cls: 0.6131  d5.loss_mask: 0.2124  d5.loss_dice: 2.0740  d6.loss_cls: 0.6083  d6.loss_mask: 0.2121  d6.loss_dice: 2.0816  d7.loss_cls: 0.6070  d7.loss_mask: 0.2104  d7.loss_dice: 2.0628  d8.loss_cls: 0.6096  d8.loss_mask: 0.2082  d8.loss_dice: 2.0467
05/11 09:44:49 - mmengine - INFO - Iter(train) [ 8200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:56:35  time: 1.5379  data_time: 0.0291  memory: 38593  grad_norm: 32.9033  loss: 28.6345  loss_cls: 0.5897  loss_mask: 0.1942  loss_dice: 1.9015  d0.loss_cls: 0.9126  d0.loss_mask: 0.2593  d0.loss_dice: 2.2809  d1.loss_cls: 0.8558  d1.loss_mask: 0.2293  d1.loss_dice: 2.1072  d2.loss_cls: 0.6795  d2.loss_mask: 0.2108  d2.loss_dice: 2.0308  d3.loss_cls: 0.6218  d3.loss_mask: 0.2034  d3.loss_dice: 1.9894  d4.loss_cls: 0.5971  d4.loss_mask: 0.1979  d4.loss_dice: 1.9431  d5.loss_cls: 0.5865  d5.loss_mask: 0.1988  d5.loss_dice: 1.9333  d6.loss_cls: 0.5879  d6.loss_mask: 0.1964  d6.loss_dice: 1.9333  d7.loss_cls: 0.5904  d7.loss_mask: 0.1956  d7.loss_dice: 1.9200  d8.loss_cls: 0.5927  d8.loss_mask: 0.1940  d8.loss_dice: 1.9012
05/11 09:46:05 - mmengine - INFO - Iter(train) [ 8250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:55:12  time: 1.5370  data_time: 0.0273  memory: 38658  grad_norm: 32.2098  loss: 27.7214  loss_cls: 0.5919  loss_mask: 0.1864  loss_dice: 1.8225  d0.loss_cls: 0.9255  d0.loss_mask: 0.2516  d0.loss_dice: 2.1948  d1.loss_cls: 0.8526  d1.loss_mask: 0.2234  d1.loss_dice: 2.0217  d2.loss_cls: 0.6807  d2.loss_mask: 0.2020  d2.loss_dice: 1.9380  d3.loss_cls: 0.6255  d3.loss_mask: 0.1956  d3.loss_dice: 1.8896  d4.loss_cls: 0.5970  d4.loss_mask: 0.1915  d4.loss_dice: 1.8613  d5.loss_cls: 0.5917  d5.loss_mask: 0.1900  d5.loss_dice: 1.8464  d6.loss_cls: 0.5882  d6.loss_mask: 0.1897  d6.loss_dice: 1.8504  d7.loss_cls: 0.5887  d7.loss_mask: 0.1888  d7.loss_dice: 1.8370  d8.loss_cls: 0.5904  d8.loss_mask: 0.1870  d8.loss_dice: 1.8214
05/11 09:47:22 - mmengine - INFO - Iter(train) [ 8300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:53:48  time: 1.5368  data_time: 0.0276  memory: 38482  grad_norm: 33.1071  loss: 27.1317  loss_cls: 0.5980  loss_mask: 0.1880  loss_dice: 1.7562  d0.loss_cls: 0.9188  d0.loss_mask: 0.2568  d0.loss_dice: 2.1189  d1.loss_cls: 0.8484  d1.loss_mask: 0.2233  d1.loss_dice: 1.9653  d2.loss_cls: 0.6842  d2.loss_mask: 0.2047  d2.loss_dice: 1.8817  d3.loss_cls: 0.6280  d3.loss_mask: 0.1970  d3.loss_dice: 1.8332  d4.loss_cls: 0.5952  d4.loss_mask: 0.1923  d4.loss_dice: 1.8013  d5.loss_cls: 0.5933  d5.loss_mask: 0.1911  d5.loss_dice: 1.7876  d6.loss_cls: 0.5894  d6.loss_mask: 0.1911  d6.loss_dice: 1.7871  d7.loss_cls: 0.5916  d7.loss_mask: 0.1893  d7.loss_dice: 1.7760  d8.loss_cls: 0.5991  d8.loss_mask: 0.1893  d8.loss_dice: 1.7555
05/11 09:48:38 - mmengine - INFO - Iter(train) [ 8350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:52:24  time: 1.5211  data_time: 0.0313  memory: 38337  grad_norm: 33.5172  loss: 27.8755  loss_cls: 0.5842  loss_mask: 0.1817  loss_dice: 1.8548  d0.loss_cls: 0.8943  d0.loss_mask: 0.2449  d0.loss_dice: 2.2173  d1.loss_cls: 0.8360  d1.loss_mask: 0.2153  d1.loss_dice: 2.0580  d2.loss_cls: 0.6718  d2.loss_mask: 0.1975  d2.loss_dice: 1.9885  d3.loss_cls: 0.6149  d3.loss_mask: 0.1891  d3.loss_dice: 1.9295  d4.loss_cls: 0.5914  d4.loss_mask: 0.1848  d4.loss_dice: 1.8922  d5.loss_cls: 0.5815  d5.loss_mask: 0.1846  d5.loss_dice: 1.8766  d6.loss_cls: 0.5831  d6.loss_mask: 0.1841  d6.loss_dice: 1.8760  d7.loss_cls: 0.5855  d7.loss_mask: 0.1814  d7.loss_dice: 1.8632  d8.loss_cls: 0.5822  d8.loss_mask: 0.1820  d8.loss_dice: 1.8490
05/11 09:49:55 - mmengine - INFO - Iter(train) [ 8400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:51:00  time: 1.5356  data_time: 0.0275  memory: 38654  grad_norm: 34.9399  loss: 28.3070  loss_cls: 0.5909  loss_mask: 0.1849  loss_dice: 1.8780  d0.loss_cls: 0.9307  d0.loss_mask: 0.2478  d0.loss_dice: 2.2547  d1.loss_cls: 0.8515  d1.loss_mask: 0.2217  d1.loss_dice: 2.0877  d2.loss_cls: 0.6823  d2.loss_mask: 0.2009  d2.loss_dice: 2.0076  d3.loss_cls: 0.6259  d3.loss_mask: 0.1946  d3.loss_dice: 1.9601  d4.loss_cls: 0.5986  d4.loss_mask: 0.1893  d4.loss_dice: 1.9147  d5.loss_cls: 0.5879  d5.loss_mask: 0.1881  d5.loss_dice: 1.9090  d6.loss_cls: 0.5885  d6.loss_mask: 0.1872  d6.loss_dice: 1.9059  d7.loss_cls: 0.5884  d7.loss_mask: 0.1865  d7.loss_dice: 1.8947  d8.loss_cls: 0.5933  d8.loss_mask: 0.1851  d8.loss_dice: 1.8704
05/11 09:51:11 - mmengine - INFO - Iter(train) [ 8450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:49:36  time: 1.5269  data_time: 0.0269  memory: 38894  grad_norm: 39.0815  loss: 28.3507  loss_cls: 0.6043  loss_mask: 0.1856  loss_dice: 1.8747  d0.loss_cls: 0.9277  d0.loss_mask: 0.2493  d0.loss_dice: 2.2233  d1.loss_cls: 0.8619  d1.loss_mask: 0.2200  d1.loss_dice: 2.0701  d2.loss_cls: 0.6910  d2.loss_mask: 0.2020  d2.loss_dice: 2.0058  d3.loss_cls: 0.6337  d3.loss_mask: 0.1917  d3.loss_dice: 1.9622  d4.loss_cls: 0.6097  d4.loss_mask: 0.1889  d4.loss_dice: 1.9143  d5.loss_cls: 0.5998  d5.loss_mask: 0.1891  d5.loss_dice: 1.9005  d6.loss_cls: 0.6017  d6.loss_mask: 0.1882  d6.loss_dice: 1.9074  d7.loss_cls: 0.6048  d7.loss_mask: 0.1864  d7.loss_dice: 1.8884  d8.loss_cls: 0.6065  d8.loss_mask: 0.1851  d8.loss_dice: 1.8763
05/11 09:52:29 - mmengine - INFO - Iter(train) [ 8500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:48:14  time: 1.5448  data_time: 0.0339  memory: 38957  grad_norm: 31.0346  loss: 28.8107  loss_cls: 0.5931  loss_mask: 0.1857  loss_dice: 1.9205  d0.loss_cls: 0.9345  d0.loss_mask: 0.2502  d0.loss_dice: 2.3076  d1.loss_cls: 0.8410  d1.loss_mask: 0.2218  d1.loss_dice: 2.1355  d2.loss_cls: 0.6852  d2.loss_mask: 0.2026  d2.loss_dice: 2.0687  d3.loss_cls: 0.6269  d3.loss_mask: 0.1929  d3.loss_dice: 2.0085  d4.loss_cls: 0.6040  d4.loss_mask: 0.1902  d4.loss_dice: 1.9583  d5.loss_cls: 0.5890  d5.loss_mask: 0.1899  d5.loss_dice: 1.9531  d6.loss_cls: 0.5915  d6.loss_mask: 0.1887  d6.loss_dice: 1.9529  d7.loss_cls: 0.5971  d7.loss_mask: 0.1866  d7.loss_dice: 1.9331  d8.loss_cls: 0.5972  d8.loss_mask: 0.1863  d8.loss_dice: 1.9184
05/11 09:53:45 - mmengine - INFO - Iter(train) [ 8550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:46:50  time: 1.5262  data_time: 0.0283  memory: 39123  grad_norm: 35.6414  loss: 29.9991  loss_cls: 0.6052  loss_mask: 0.1910  loss_dice: 2.0279  d0.loss_cls: 0.9471  d0.loss_mask: 0.2483  d0.loss_dice: 2.3942  d1.loss_cls: 0.8607  d1.loss_mask: 0.2288  d1.loss_dice: 2.2339  d2.loss_cls: 0.6934  d2.loss_mask: 0.2096  d2.loss_dice: 2.1647  d3.loss_cls: 0.6387  d3.loss_mask: 0.2016  d3.loss_dice: 2.1077  d4.loss_cls: 0.6103  d4.loss_mask: 0.1953  d4.loss_dice: 2.0680  d5.loss_cls: 0.6041  d5.loss_mask: 0.1932  d5.loss_dice: 2.0604  d6.loss_cls: 0.6034  d6.loss_mask: 0.1936  d6.loss_dice: 2.0572  d7.loss_cls: 0.6086  d7.loss_mask: 0.1908  d7.loss_dice: 2.0401  d8.loss_cls: 0.6025  d8.loss_mask: 0.1903  d8.loss_dice: 2.0284
05/11 09:55:02 - mmengine - INFO - Iter(train) [ 8600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:45:27  time: 1.5355  data_time: 0.0280  memory: 38792  grad_norm: 33.1075  loss: 28.2557  loss_cls: 0.5925  loss_mask: 0.1916  loss_dice: 1.8694  d0.loss_cls: 0.9380  d0.loss_mask: 0.2590  d0.loss_dice: 2.2237  d1.loss_cls: 0.8341  d1.loss_mask: 0.2285  d1.loss_dice: 2.0777  d2.loss_cls: 0.6684  d2.loss_mask: 0.2083  d2.loss_dice: 1.9952  d3.loss_cls: 0.6175  d3.loss_mask: 0.2006  d3.loss_dice: 1.9513  d4.loss_cls: 0.5976  d4.loss_mask: 0.1966  d4.loss_dice: 1.9117  d5.loss_cls: 0.5848  d5.loss_mask: 0.1948  d5.loss_dice: 1.9082  d6.loss_cls: 0.5872  d6.loss_mask: 0.1957  d6.loss_dice: 1.8966  d7.loss_cls: 0.5888  d7.loss_mask: 0.1936  d7.loss_dice: 1.8886  d8.loss_cls: 0.5878  d8.loss_mask: 0.1931  d8.loss_dice: 1.8746
05/11 09:56:17 - mmengine - INFO - Iter(train) [ 8650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:44:02  time: 1.5121  data_time: 0.0284  memory: 38598  grad_norm: 34.1354  loss: 28.6038  loss_cls: 0.5998  loss_mask: 0.1904  loss_dice: 1.8976  d0.loss_cls: 0.9534  d0.loss_mask: 0.2560  d0.loss_dice: 2.2672  d1.loss_cls: 0.8494  d1.loss_mask: 0.2269  d1.loss_dice: 2.1101  d2.loss_cls: 0.6908  d2.loss_mask: 0.2060  d2.loss_dice: 2.0281  d3.loss_cls: 0.6230  d3.loss_mask: 0.1992  d3.loss_dice: 1.9733  d4.loss_cls: 0.6012  d4.loss_mask: 0.1938  d4.loss_dice: 1.9352  d5.loss_cls: 0.5873  d5.loss_mask: 0.1955  d5.loss_dice: 1.9304  d6.loss_cls: 0.5905  d6.loss_mask: 0.1944  d6.loss_dice: 1.9254  d7.loss_cls: 0.5939  d7.loss_mask: 0.1919  d7.loss_dice: 1.9081  d8.loss_cls: 0.5968  d8.loss_mask: 0.1913  d8.loss_dice: 1.8968
05/11 09:57:34 - mmengine - INFO - Iter(train) [ 8700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:42:39  time: 1.5302  data_time: 0.0277  memory: 39218  grad_norm: 33.6412  loss: 29.8701  loss_cls: 0.5971  loss_mask: 0.1897  loss_dice: 2.0242  d0.loss_cls: 0.9595  d0.loss_mask: 0.2532  d0.loss_dice: 2.4056  d1.loss_cls: 0.8456  d1.loss_mask: 0.2256  d1.loss_dice: 2.2474  d2.loss_cls: 0.6731  d2.loss_mask: 0.2069  d2.loss_dice: 2.1645  d3.loss_cls: 0.6271  d3.loss_mask: 0.1997  d3.loss_dice: 2.1018  d4.loss_cls: 0.6049  d4.loss_mask: 0.1931  d4.loss_dice: 2.0566  d5.loss_cls: 0.5953  d5.loss_mask: 0.1935  d5.loss_dice: 2.0475  d6.loss_cls: 0.5953  d6.loss_mask: 0.1923  d6.loss_dice: 2.0402  d7.loss_cls: 0.5998  d7.loss_mask: 0.1905  d7.loss_dice: 2.0283  d8.loss_cls: 0.5972  d8.loss_mask: 0.1907  d8.loss_dice: 2.0240
05/11 09:58:51 - mmengine - INFO - Iter(train) [ 8750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:41:17  time: 1.5395  data_time: 0.0282  memory: 38929  grad_norm: 34.4411  loss: 26.6899  loss_cls: 0.5843  loss_mask: 0.1816  loss_dice: 1.7321  d0.loss_cls: 0.9254  d0.loss_mask: 0.2415  d0.loss_dice: 2.0762  d1.loss_cls: 0.8496  d1.loss_mask: 0.2170  d1.loss_dice: 1.9387  d2.loss_cls: 0.6664  d2.loss_mask: 0.1985  d2.loss_dice: 1.8576  d3.loss_cls: 0.6149  d3.loss_mask: 0.1900  d3.loss_dice: 1.8078  d4.loss_cls: 0.5920  d4.loss_mask: 0.1861  d4.loss_dice: 1.7690  d5.loss_cls: 0.5808  d5.loss_mask: 0.1850  d5.loss_dice: 1.7635  d6.loss_cls: 0.5803  d6.loss_mask: 0.1843  d6.loss_dice: 1.7609  d7.loss_cls: 0.5807  d7.loss_mask: 0.1828  d7.loss_dice: 1.7464  d8.loss_cls: 0.5848  d8.loss_mask: 0.1810  d8.loss_dice: 1.7308
05/11 10:00:07 - mmengine - INFO - Iter(train) [ 8800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:39:52  time: 1.5167  data_time: 0.0293  memory: 38251  grad_norm: 31.8700  loss: 26.1273  loss_cls: 0.5535  loss_mask: 0.1808  loss_dice: 1.7073  d0.loss_cls: 0.9291  d0.loss_mask: 0.2441  d0.loss_dice: 2.0300  d1.loss_cls: 0.8131  d1.loss_mask: 0.2151  d1.loss_dice: 1.9095  d2.loss_cls: 0.6424  d2.loss_mask: 0.1963  d2.loss_dice: 1.8313  d3.loss_cls: 0.5839  d3.loss_mask: 0.1895  d3.loss_dice: 1.7832  d4.loss_cls: 0.5588  d4.loss_mask: 0.1865  d4.loss_dice: 1.7444  d5.loss_cls: 0.5532  d5.loss_mask: 0.1846  d5.loss_dice: 1.7328  d6.loss_cls: 0.5511  d6.loss_mask: 0.1833  d6.loss_dice: 1.7300  d7.loss_cls: 0.5503  d7.loss_mask: 0.1822  d7.loss_dice: 1.7213  d8.loss_cls: 0.5510  d8.loss_mask: 0.1806  d8.loss_dice: 1.7081
05/11 10:01:23 - mmengine - INFO - Iter(train) [ 8850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:38:29  time: 1.5325  data_time: 0.0313  memory: 38738  grad_norm: 39.2410  loss: 27.9649  loss_cls: 0.5763  loss_mask: 0.1735  loss_dice: 1.8583  d0.loss_cls: 0.9574  d0.loss_mask: 0.2406  d0.loss_dice: 2.2269  d1.loss_cls: 0.8380  d1.loss_mask: 0.2074  d1.loss_dice: 2.0926  d2.loss_cls: 0.6696  d2.loss_mask: 0.1899  d2.loss_dice: 2.0023  d3.loss_cls: 0.6108  d3.loss_mask: 0.1836  d3.loss_dice: 1.9569  d4.loss_cls: 0.5787  d4.loss_mask: 0.1789  d4.loss_dice: 1.9051  d5.loss_cls: 0.5736  d5.loss_mask: 0.1771  d5.loss_dice: 1.8895  d6.loss_cls: 0.5735  d6.loss_mask: 0.1766  d6.loss_dice: 1.8920  d7.loss_cls: 0.5771  d7.loss_mask: 0.1754  d7.loss_dice: 1.8759  d8.loss_cls: 0.5754  d8.loss_mask: 0.1731  d8.loss_dice: 1.8588
05/11 10:02:40 - mmengine - INFO - Iter(train) [ 8900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:37:07  time: 1.5365  data_time: 0.0312  memory: 39563  grad_norm: 37.9043  loss: 28.8626  loss_cls: 0.5898  loss_mask: 0.1895  loss_dice: 1.9114  d0.loss_cls: 0.9654  d0.loss_mask: 0.2535  d0.loss_dice: 2.3187  d1.loss_cls: 0.8402  d1.loss_mask: 0.2258  d1.loss_dice: 2.1582  d2.loss_cls: 0.6741  d2.loss_mask: 0.2071  d2.loss_dice: 2.0711  d3.loss_cls: 0.6174  d3.loss_mask: 0.1988  d3.loss_dice: 2.0157  d4.loss_cls: 0.5936  d4.loss_mask: 0.1935  d4.loss_dice: 1.9651  d5.loss_cls: 0.5858  d5.loss_mask: 0.1925  d5.loss_dice: 1.9527  d6.loss_cls: 0.5862  d6.loss_mask: 0.1926  d6.loss_dice: 1.9517  d7.loss_cls: 0.5886  d7.loss_mask: 0.1920  d7.loss_dice: 1.9411  d8.loss_cls: 0.5891  d8.loss_mask: 0.1898  d8.loss_dice: 1.9116
05/11 10:03:56 - mmengine - INFO - Iter(train) [ 8950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:35:43  time: 1.5233  data_time: 0.0297  memory: 38615  grad_norm: 41.4858  loss: 27.4891  loss_cls: 0.5908  loss_mask: 0.1840  loss_dice: 1.7941  d0.loss_cls: 0.9458  d0.loss_mask: 0.2509  d0.loss_dice: 2.1755  d1.loss_cls: 0.8385  d1.loss_mask: 0.2198  d1.loss_dice: 2.0306  d2.loss_cls: 0.6852  d2.loss_mask: 0.2012  d2.loss_dice: 1.9346  d3.loss_cls: 0.6262  d3.loss_mask: 0.1921  d3.loss_dice: 1.8790  d4.loss_cls: 0.5995  d4.loss_mask: 0.1890  d4.loss_dice: 1.8301  d5.loss_cls: 0.5903  d5.loss_mask: 0.1872  d5.loss_dice: 1.8158  d6.loss_cls: 0.5904  d6.loss_mask: 0.1866  d6.loss_dice: 1.8086  d7.loss_cls: 0.5915  d7.loss_mask: 0.1853  d7.loss_dice: 1.8039  d8.loss_cls: 0.5933  d8.loss_mask: 0.1839  d8.loss_dice: 1.7854
05/11 10:05:12 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 10:05:12 - mmengine - INFO - Iter(train) [ 9000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:34:20  time: 1.5205  data_time: 0.0274  memory: 39519  grad_norm: 33.5968  loss: 27.8303  loss_cls: 0.5715  loss_mask: 0.1763  loss_dice: 1.8561  d0.loss_cls: 0.9498  d0.loss_mask: 0.2457  d0.loss_dice: 2.2044  d1.loss_cls: 0.8203  d1.loss_mask: 0.2168  d1.loss_dice: 2.0835  d2.loss_cls: 0.6538  d2.loss_mask: 0.1954  d2.loss_dice: 1.9945  d3.loss_cls: 0.5947  d3.loss_mask: 0.1872  d3.loss_dice: 1.9405  d4.loss_cls: 0.5733  d4.loss_mask: 0.1819  d4.loss_dice: 1.8980  d5.loss_cls: 0.5675  d5.loss_mask: 0.1804  d5.loss_dice: 1.8857  d6.loss_cls: 0.5711  d6.loss_mask: 0.1802  d6.loss_dice: 1.8784  d7.loss_cls: 0.5708  d7.loss_mask: 0.1785  d7.loss_dice: 1.8710  d8.loss_cls: 0.5706  d8.loss_mask: 0.1773  d8.loss_dice: 1.8553
05/11 10:06:29 - mmengine - INFO - Iter(train) [ 9050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:32:57  time: 1.5319  data_time: 0.0293  memory: 39335  grad_norm: 34.6400  loss: 28.2217  loss_cls: 0.6001  loss_mask: 0.1911  loss_dice: 1.8524  d0.loss_cls: 0.9525  d0.loss_mask: 0.2590  d0.loss_dice: 2.2101  d1.loss_cls: 0.8382  d1.loss_mask: 0.2289  d1.loss_dice: 2.0870  d2.loss_cls: 0.6877  d2.loss_mask: 0.2070  d2.loss_dice: 1.9946  d3.loss_cls: 0.6298  d3.loss_mask: 0.2015  d3.loss_dice: 1.9444  d4.loss_cls: 0.6040  d4.loss_mask: 0.1966  d4.loss_dice: 1.8935  d5.loss_cls: 0.5948  d5.loss_mask: 0.1951  d5.loss_dice: 1.8885  d6.loss_cls: 0.5982  d6.loss_mask: 0.1933  d6.loss_dice: 1.8779  d7.loss_cls: 0.5963  d7.loss_mask: 0.1928  d7.loss_dice: 1.8644  d8.loss_cls: 0.5973  d8.loss_mask: 0.1910  d8.loss_dice: 1.8536
05/11 10:07:45 - mmengine - INFO - Iter(train) [ 9100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:31:34  time: 1.5275  data_time: 0.0363  memory: 38715  grad_norm: 35.6923  loss: 27.8446  loss_cls: 0.6004  loss_mask: 0.1768  loss_dice: 1.8385  d0.loss_cls: 0.9434  d0.loss_mask: 0.2378  d0.loss_dice: 2.1703  d1.loss_cls: 0.8353  d1.loss_mask: 0.2090  d1.loss_dice: 2.0541  d2.loss_cls: 0.6823  d2.loss_mask: 0.1929  d2.loss_dice: 1.9804  d3.loss_cls: 0.6285  d3.loss_mask: 0.1876  d3.loss_dice: 1.9272  d4.loss_cls: 0.6019  d4.loss_mask: 0.1826  d4.loss_dice: 1.8813  d5.loss_cls: 0.5913  d5.loss_mask: 0.1804  d5.loss_dice: 1.8653  d6.loss_cls: 0.5884  d6.loss_mask: 0.1810  d6.loss_dice: 1.8660  d7.loss_cls: 0.5972  d7.loss_mask: 0.1795  d7.loss_dice: 1.8543  d8.loss_cls: 0.5955  d8.loss_mask: 0.1769  d8.loss_dice: 1.8385
05/11 10:09:02 - mmengine - INFO - Iter(train) [ 9150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:30:12  time: 1.5347  data_time: 0.0278  memory: 38989  grad_norm: 40.2789  loss: 27.9897  loss_cls: 0.5916  loss_mask: 0.1727  loss_dice: 1.8494  d0.loss_cls: 0.9640  d0.loss_mask: 0.2334  d0.loss_dice: 2.2134  d1.loss_cls: 0.8481  d1.loss_mask: 0.2099  d1.loss_dice: 2.0837  d2.loss_cls: 0.6816  d2.loss_mask: 0.1881  d2.loss_dice: 1.9960  d3.loss_cls: 0.6252  d3.loss_mask: 0.1799  d3.loss_dice: 1.9421  d4.loss_cls: 0.5954  d4.loss_mask: 0.1771  d4.loss_dice: 1.8965  d5.loss_cls: 0.5895  d5.loss_mask: 0.1753  d5.loss_dice: 1.8870  d6.loss_cls: 0.5948  d6.loss_mask: 0.1752  d6.loss_dice: 1.8787  d7.loss_cls: 0.5937  d7.loss_mask: 0.1741  d7.loss_dice: 1.8586  d8.loss_cls: 0.5917  d8.loss_mask: 0.1726  d8.loss_dice: 1.8504
05/11 10:10:18 - mmengine - INFO - Iter(train) [ 9200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:28:49  time: 1.5271  data_time: 0.0296  memory: 39308  grad_norm: 34.2477  loss: 30.3391  loss_cls: 0.6222  loss_mask: 0.2068  loss_dice: 2.0129  d0.loss_cls: 0.9725  d0.loss_mask: 0.2791  d0.loss_dice: 2.4035  d1.loss_cls: 0.8610  d1.loss_mask: 0.2456  d1.loss_dice: 2.2625  d2.loss_cls: 0.7041  d2.loss_mask: 0.2257  d2.loss_dice: 2.1822  d3.loss_cls: 0.6516  d3.loss_mask: 0.2199  d3.loss_dice: 2.1246  d4.loss_cls: 0.6258  d4.loss_mask: 0.2153  d4.loss_dice: 2.0617  d5.loss_cls: 0.6194  d5.loss_mask: 0.2135  d5.loss_dice: 2.0472  d6.loss_cls: 0.6185  d6.loss_mask: 0.2123  d6.loss_dice: 2.0433  d7.loss_cls: 0.6211  d7.loss_mask: 0.2093  d7.loss_dice: 2.0351  d8.loss_cls: 0.6275  d8.loss_mask: 0.2078  d8.loss_dice: 2.0071
05/11 10:11:35 - mmengine - INFO - Iter(train) [ 9250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:27:27  time: 1.5302  data_time: 0.0292  memory: 38834  grad_norm: 36.1080  loss: 28.0730  loss_cls: 0.5870  loss_mask: 0.1812  loss_dice: 1.8650  d0.loss_cls: 0.9587  d0.loss_mask: 0.2377  d0.loss_dice: 2.1996  d1.loss_cls: 0.8252  d1.loss_mask: 0.2138  d1.loss_dice: 2.0760  d2.loss_cls: 0.6642  d2.loss_mask: 0.1964  d2.loss_dice: 2.0114  d3.loss_cls: 0.6097  d3.loss_mask: 0.1908  d3.loss_dice: 1.9603  d4.loss_cls: 0.5883  d4.loss_mask: 0.1853  d4.loss_dice: 1.9149  d5.loss_cls: 0.5815  d5.loss_mask: 0.1835  d5.loss_dice: 1.8992  d6.loss_cls: 0.5817  d6.loss_mask: 0.1832  d6.loss_dice: 1.8967  d7.loss_cls: 0.5794  d7.loss_mask: 0.1835  d7.loss_dice: 1.8841  d8.loss_cls: 0.5890  d8.loss_mask: 0.1817  d8.loss_dice: 1.8639
05/11 10:12:52 - mmengine - INFO - Iter(train) [ 9300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:26:05  time: 1.5411  data_time: 0.0278  memory: 39041  grad_norm: 32.1760  loss: 27.2347  loss_cls: 0.5828  loss_mask: 0.1767  loss_dice: 1.7823  d0.loss_cls: 0.9518  d0.loss_mask: 0.2380  d0.loss_dice: 2.1102  d1.loss_cls: 0.8336  d1.loss_mask: 0.2105  d1.loss_dice: 1.9886  d2.loss_cls: 0.6754  d2.loss_mask: 0.1914  d2.loss_dice: 1.9282  d3.loss_cls: 0.6228  d3.loss_mask: 0.1855  d3.loss_dice: 1.8708  d4.loss_cls: 0.5939  d4.loss_mask: 0.1831  d4.loss_dice: 1.8337  d5.loss_cls: 0.5887  d5.loss_mask: 0.1803  d5.loss_dice: 1.8158  d6.loss_cls: 0.5862  d6.loss_mask: 0.1806  d6.loss_dice: 1.8099  d7.loss_cls: 0.5840  d7.loss_mask: 0.1783  d7.loss_dice: 1.8023  d8.loss_cls: 0.5853  d8.loss_mask: 0.1769  d8.loss_dice: 1.7868
05/11 10:14:09 - mmengine - INFO - Iter(train) [ 9350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:24:43  time: 1.5313  data_time: 0.0274  memory: 38630  grad_norm: 34.2389  loss: 28.5292  loss_cls: 0.5931  loss_mask: 0.1928  loss_dice: 1.8901  d0.loss_cls: 0.9503  d0.loss_mask: 0.2567  d0.loss_dice: 2.2333  d1.loss_cls: 0.8396  d1.loss_mask: 0.2274  d1.loss_dice: 2.1005  d2.loss_cls: 0.6806  d2.loss_mask: 0.2139  d2.loss_dice: 2.0317  d3.loss_cls: 0.6215  d3.loss_mask: 0.2028  d3.loss_dice: 1.9773  d4.loss_cls: 0.5949  d4.loss_mask: 0.1984  d4.loss_dice: 1.9302  d5.loss_cls: 0.5944  d5.loss_mask: 0.1975  d5.loss_dice: 1.9204  d6.loss_cls: 0.5967  d6.loss_mask: 0.1977  d6.loss_dice: 1.9107  d7.loss_cls: 0.5998  d7.loss_mask: 0.1948  d7.loss_dice: 1.9029  d8.loss_cls: 0.5989  d8.loss_mask: 0.1939  d8.loss_dice: 1.8865
05/11 10:15:26 - mmengine - INFO - Iter(train) [ 9400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:23:22  time: 1.5457  data_time: 0.0297  memory: 39056  grad_norm: 38.1848  loss: 29.3196  loss_cls: 0.5913  loss_mask: 0.1810  loss_dice: 1.9692  d0.loss_cls: 0.9774  d0.loss_mask: 0.2439  d0.loss_dice: 2.3299  d1.loss_cls: 0.8460  d1.loss_mask: 0.2205  d1.loss_dice: 2.1952  d2.loss_cls: 0.6887  d2.loss_mask: 0.2005  d2.loss_dice: 2.1264  d3.loss_cls: 0.6237  d3.loss_mask: 0.1923  d3.loss_dice: 2.0750  d4.loss_cls: 0.6029  d4.loss_mask: 0.1864  d4.loss_dice: 2.0189  d5.loss_cls: 0.5880  d5.loss_mask: 0.1855  d5.loss_dice: 2.0075  d6.loss_cls: 0.5858  d6.loss_mask: 0.1833  d6.loss_dice: 2.0023  d7.loss_cls: 0.5854  d7.loss_mask: 0.1820  d7.loss_dice: 1.9910  d8.loss_cls: 0.5898  d8.loss_mask: 0.1814  d8.loss_dice: 1.9684
05/11 10:16:42 - mmengine - INFO - Iter(train) [ 9450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:21:59  time: 1.5267  data_time: 0.0289  memory: 38120  grad_norm: 36.7255  loss: 26.7035  loss_cls: 0.5665  loss_mask: 0.1819  loss_dice: 1.7484  d0.loss_cls: 0.9341  d0.loss_mask: 0.2535  d0.loss_dice: 2.0636  d1.loss_cls: 0.8202  d1.loss_mask: 0.2179  d1.loss_dice: 1.9446  d2.loss_cls: 0.6633  d2.loss_mask: 0.1999  d2.loss_dice: 1.8747  d3.loss_cls: 0.6034  d3.loss_mask: 0.1910  d3.loss_dice: 1.8310  d4.loss_cls: 0.5811  d4.loss_mask: 0.1872  d4.loss_dice: 1.7798  d5.loss_cls: 0.5701  d5.loss_mask: 0.1861  d5.loss_dice: 1.7741  d6.loss_cls: 0.5709  d6.loss_mask: 0.1847  d6.loss_dice: 1.7659  d7.loss_cls: 0.5755  d7.loss_mask: 0.1841  d7.loss_dice: 1.7533  d8.loss_cls: 0.5671  d8.loss_mask: 0.1830  d8.loss_dice: 1.7467
05/11 10:17:59 - mmengine - INFO - Iter(train) [ 9500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:20:38  time: 1.5402  data_time: 0.0292  memory: 38579  grad_norm: 33.7456  loss: 26.4364  loss_cls: 0.5861  loss_mask: 0.1782  loss_dice: 1.7190  d0.loss_cls: 0.9394  d0.loss_mask: 0.2371  d0.loss_dice: 2.0237  d1.loss_cls: 0.8407  d1.loss_mask: 0.2100  d1.loss_dice: 1.9063  d2.loss_cls: 0.6702  d2.loss_mask: 0.1918  d2.loss_dice: 1.8372  d3.loss_cls: 0.6154  d3.loss_mask: 0.1851  d3.loss_dice: 1.7959  d4.loss_cls: 0.5895  d4.loss_mask: 0.1808  d4.loss_dice: 1.7542  d5.loss_cls: 0.5815  d5.loss_mask: 0.1822  d5.loss_dice: 1.7437  d6.loss_cls: 0.5832  d6.loss_mask: 0.1816  d6.loss_dice: 1.7365  d7.loss_cls: 0.5848  d7.loss_mask: 0.1797  d7.loss_dice: 1.7261  d8.loss_cls: 0.5847  d8.loss_mask: 0.1790  d8.loss_dice: 1.7129
05/11 10:19:15 - mmengine - INFO - Iter(train) [ 9550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:19:15  time: 1.5259  data_time: 0.0277  memory: 38444  grad_norm: 34.1230  loss: 28.3733  loss_cls: 0.5968  loss_mask: 0.1869  loss_dice: 1.8861  d0.loss_cls: 0.9623  d0.loss_mask: 0.2522  d0.loss_dice: 2.2197  d1.loss_cls: 0.8240  d1.loss_mask: 0.2246  d1.loss_dice: 2.0873  d2.loss_cls: 0.6747  d2.loss_mask: 0.2042  d2.loss_dice: 2.0125  d3.loss_cls: 0.6259  d3.loss_mask: 0.1960  d3.loss_dice: 1.9624  d4.loss_cls: 0.6014  d4.loss_mask: 0.1916  d4.loss_dice: 1.9228  d5.loss_cls: 0.5952  d5.loss_mask: 0.1905  d5.loss_dice: 1.9136  d6.loss_cls: 0.5960  d6.loss_mask: 0.1896  d6.loss_dice: 1.9093  d7.loss_cls: 0.5945  d7.loss_mask: 0.1878  d7.loss_dice: 1.8979  d8.loss_cls: 0.5962  d8.loss_mask: 0.1872  d8.loss_dice: 1.8844
05/11 10:20:33 - mmengine - INFO - Iter(train) [ 9600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:17:55  time: 1.5476  data_time: 0.0295  memory: 39192  grad_norm: 35.2397  loss: 27.0163  loss_cls: 0.5998  loss_mask: 0.1742  loss_dice: 1.7670  d0.loss_cls: 0.9560  d0.loss_mask: 0.2369  d0.loss_dice: 2.0631  d1.loss_cls: 0.8210  d1.loss_mask: 0.2070  d1.loss_dice: 1.9584  d2.loss_cls: 0.6797  d2.loss_mask: 0.1907  d2.loss_dice: 1.8930  d3.loss_cls: 0.6192  d3.loss_mask: 0.1842  d3.loss_dice: 1.8466  d4.loss_cls: 0.6040  d4.loss_mask: 0.1810  d4.loss_dice: 1.8057  d5.loss_cls: 0.5924  d5.loss_mask: 0.1801  d5.loss_dice: 1.7990  d6.loss_cls: 0.5955  d6.loss_mask: 0.1789  d6.loss_dice: 1.7887  d7.loss_cls: 0.5956  d7.loss_mask: 0.1771  d7.loss_dice: 1.7811  d8.loss_cls: 0.5974  d8.loss_mask: 0.1751  d8.loss_dice: 1.7678
05/11 10:21:49 - mmengine - INFO - Iter(train) [ 9650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:16:32  time: 1.5238  data_time: 0.0290  memory: 38743  grad_norm: 35.7902  loss: 28.4441  loss_cls: 0.5768  loss_mask: 0.1920  loss_dice: 1.9044  d0.loss_cls: 0.9591  d0.loss_mask: 0.2538  d0.loss_dice: 2.2443  d1.loss_cls: 0.8277  d1.loss_mask: 0.2257  d1.loss_dice: 2.1026  d2.loss_cls: 0.6651  d2.loss_mask: 0.2106  d2.loss_dice: 2.0342  d3.loss_cls: 0.6040  d3.loss_mask: 0.2004  d3.loss_dice: 1.9859  d4.loss_cls: 0.5823  d4.loss_mask: 0.1968  d4.loss_dice: 1.9365  d5.loss_cls: 0.5682  d5.loss_mask: 0.1972  d5.loss_dice: 1.9347  d6.loss_cls: 0.5729  d6.loss_mask: 0.1952  d6.loss_dice: 1.9252  d7.loss_cls: 0.5762  d7.loss_mask: 0.1931  d7.loss_dice: 1.9141  d8.loss_cls: 0.5740  d8.loss_mask: 0.1920  d8.loss_dice: 1.8991
05/11 10:23:06 - mmengine - INFO - Iter(train) [ 9700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:15:11  time: 1.5413  data_time: 0.0300  memory: 38853  grad_norm: 34.5711  loss: 27.2025  loss_cls: 0.6018  loss_mask: 0.1855  loss_dice: 1.7710  d0.loss_cls: 0.9536  d0.loss_mask: 0.2530  d0.loss_dice: 2.0846  d1.loss_cls: 0.8298  d1.loss_mask: 0.2184  d1.loss_dice: 1.9762  d2.loss_cls: 0.6730  d2.loss_mask: 0.1995  d2.loss_dice: 1.8974  d3.loss_cls: 0.6247  d3.loss_mask: 0.1929  d3.loss_dice: 1.8456  d4.loss_cls: 0.6040  d4.loss_mask: 0.1902  d4.loss_dice: 1.8075  d5.loss_cls: 0.6005  d5.loss_mask: 0.1893  d5.loss_dice: 1.7946  d6.loss_cls: 0.5998  d6.loss_mask: 0.1885  d6.loss_dice: 1.7932  d7.loss_cls: 0.6025  d7.loss_mask: 0.1879  d7.loss_dice: 1.7805  d8.loss_cls: 0.6000  d8.loss_mask: 0.1870  d8.loss_dice: 1.7698
05/11 10:24:23 - mmengine - INFO - Iter(train) [ 9750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:13:49  time: 1.5303  data_time: 0.0314  memory: 39327  grad_norm: 40.2800  loss: 26.7564  loss_cls: 0.5837  loss_mask: 0.1802  loss_dice: 1.7364  d0.loss_cls: 0.9358  d0.loss_mask: 0.2471  d0.loss_dice: 2.0879  d1.loss_cls: 0.8213  d1.loss_mask: 0.2155  d1.loss_dice: 1.9700  d2.loss_cls: 0.6563  d2.loss_mask: 0.2005  d2.loss_dice: 1.8796  d3.loss_cls: 0.6048  d3.loss_mask: 0.1896  d3.loss_dice: 1.8288  d4.loss_cls: 0.5826  d4.loss_mask: 0.1858  d4.loss_dice: 1.7817  d5.loss_cls: 0.5800  d5.loss_mask: 0.1840  d5.loss_dice: 1.7638  d6.loss_cls: 0.5830  d6.loss_mask: 0.1829  d6.loss_dice: 1.7615  d7.loss_cls: 0.5855  d7.loss_mask: 0.1822  d7.loss_dice: 1.7480  d8.loss_cls: 0.5878  d8.loss_mask: 0.1803  d8.loss_dice: 1.7299
05/11 10:25:39 - mmengine - INFO - Iter(train) [ 9800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:12:26  time: 1.5221  data_time: 0.0309  memory: 38587  grad_norm: 35.4444  loss: 27.7212  loss_cls: 0.5938  loss_mask: 0.2009  loss_dice: 1.8100  d0.loss_cls: 0.9406  d0.loss_mask: 0.2717  d0.loss_dice: 2.1458  d1.loss_cls: 0.8120  d1.loss_mask: 0.2370  d1.loss_dice: 2.0312  d2.loss_cls: 0.6617  d2.loss_mask: 0.2191  d2.loss_dice: 1.9437  d3.loss_cls: 0.6081  d3.loss_mask: 0.2107  d3.loss_dice: 1.9011  d4.loss_cls: 0.5962  d4.loss_mask: 0.2077  d4.loss_dice: 1.8533  d5.loss_cls: 0.5853  d5.loss_mask: 0.2055  d5.loss_dice: 1.8374  d6.loss_cls: 0.5873  d6.loss_mask: 0.2043  d6.loss_dice: 1.8403  d7.loss_cls: 0.5911  d7.loss_mask: 0.2028  d7.loss_dice: 1.8201  d8.loss_cls: 0.5925  d8.loss_mask: 0.2009  d8.loss_dice: 1.8094
05/11 10:26:56 - mmengine - INFO - Iter(train) [ 9850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:11:06  time: 1.5476  data_time: 0.0288  memory: 38746  grad_norm: 35.3104  loss: 28.7782  loss_cls: 0.6155  loss_mask: 0.1862  loss_dice: 1.9044  d0.loss_cls: 0.9637  d0.loss_mask: 0.2430  d0.loss_dice: 2.2437  d1.loss_cls: 0.8465  d1.loss_mask: 0.2196  d1.loss_dice: 2.1216  d2.loss_cls: 0.6974  d2.loss_mask: 0.2043  d2.loss_dice: 2.0461  d3.loss_cls: 0.6445  d3.loss_mask: 0.1953  d3.loss_dice: 1.9851  d4.loss_cls: 0.6269  d4.loss_mask: 0.1909  d4.loss_dice: 1.9429  d5.loss_cls: 0.6165  d5.loss_mask: 0.1894  d5.loss_dice: 1.9290  d6.loss_cls: 0.6235  d6.loss_mask: 0.1890  d6.loss_dice: 1.9302  d7.loss_cls: 0.6188  d7.loss_mask: 0.1877  d7.loss_dice: 1.9119  d8.loss_cls: 0.6161  d8.loss_mask: 0.1863  d8.loss_dice: 1.9022
05/11 10:28:13 - mmengine - INFO - Iter(train) [ 9900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:09:44  time: 1.5337  data_time: 0.0279  memory: 39480  grad_norm: 35.9706  loss: 27.2899  loss_cls: 0.5746  loss_mask: 0.1762  loss_dice: 1.8014  d0.loss_cls: 0.9548  d0.loss_mask: 0.2401  d0.loss_dice: 2.1374  d1.loss_cls: 0.8225  d1.loss_mask: 0.2101  d1.loss_dice: 2.0197  d2.loss_cls: 0.6616  d2.loss_mask: 0.1903  d2.loss_dice: 1.9408  d3.loss_cls: 0.6020  d3.loss_mask: 0.1838  d3.loss_dice: 1.8887  d4.loss_cls: 0.5773  d4.loss_mask: 0.1806  d4.loss_dice: 1.8450  d5.loss_cls: 0.5740  d5.loss_mask: 0.1792  d5.loss_dice: 1.8325  d6.loss_cls: 0.5757  d6.loss_mask: 0.1790  d6.loss_dice: 1.8227  d7.loss_cls: 0.5766  d7.loss_mask: 0.1782  d7.loss_dice: 1.8089  d8.loss_cls: 0.5732  d8.loss_mask: 0.1762  d8.loss_dice: 1.8070
05/11 10:29:29 - mmengine - INFO - Iter(train) [ 9950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:08:23  time: 1.5340  data_time: 0.0273  memory: 38763  grad_norm: 34.5803  loss: 27.0588  loss_cls: 0.5727  loss_mask: 0.1744  loss_dice: 1.7819  d0.loss_cls: 0.9550  d0.loss_mask: 0.2390  d0.loss_dice: 2.1024  d1.loss_cls: 0.8160  d1.loss_mask: 0.2112  d1.loss_dice: 2.0021  d2.loss_cls: 0.6532  d2.loss_mask: 0.1919  d2.loss_dice: 1.9272  d3.loss_cls: 0.5983  d3.loss_mask: 0.1858  d3.loss_dice: 1.8728  d4.loss_cls: 0.5793  d4.loss_mask: 0.1807  d4.loss_dice: 1.8200  d5.loss_cls: 0.5774  d5.loss_mask: 0.1797  d5.loss_dice: 1.8045  d6.loss_cls: 0.5776  d6.loss_mask: 0.1787  d6.loss_dice: 1.8054  d7.loss_cls: 0.5778  d7.loss_mask: 0.1779  d7.loss_dice: 1.7858  d8.loss_cls: 0.5728  d8.loss_mask: 0.1751  d8.loss_dice: 1.7822
05/11 10:30:47 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 10:30:47 - mmengine - INFO - Iter(train) [10000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:07:02  time: 1.5441  data_time: 0.0275  memory: 39816  grad_norm: 37.0798  loss: 28.9327  loss_cls: 0.5800  loss_mask: 0.1756  loss_dice: 1.9464  d0.loss_cls: 0.9911  d0.loss_mask: 0.2346  d0.loss_dice: 2.3040  d1.loss_cls: 0.8325  d1.loss_mask: 0.2127  d1.loss_dice: 2.1855  d2.loss_cls: 0.6746  d2.loss_mask: 0.1938  d2.loss_dice: 2.1006  d3.loss_cls: 0.6118  d3.loss_mask: 0.1843  d3.loss_dice: 2.0468  d4.loss_cls: 0.5853  d4.loss_mask: 0.1807  d4.loss_dice: 1.9928  d5.loss_cls: 0.5902  d5.loss_mask: 0.1800  d5.loss_dice: 1.9712  d6.loss_cls: 0.5895  d6.loss_mask: 0.1800  d6.loss_dice: 1.9667  d7.loss_cls: 0.5880  d7.loss_mask: 0.1779  d7.loss_dice: 1.9490  d8.loss_cls: 0.5816  d8.loss_mask: 0.1760  d8.loss_dice: 1.9496
05/11 10:30:47 - mmengine - INFO - Saving checkpoint at 10000 iterations
05/11 10:31:30 - mmengine - INFO - Iter(val) [ 50/100]    eta: 0:00:36  time: 0.7366  data_time: 0.0211  memory: 5704  
05/11 10:32:06 - mmengine - INFO - Iter(val) [100/100]    eta: 0:00:00  time: 0.7292  data_time: 0.0139  memory: 5704  
05/11 10:32:30 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.22s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/11 10:32:37 - mmengine - INFO - start multi processing evaluation ...

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

  0%|          | 0/25552 [00:00<?, ?it/s]
 50%|█████     | 12777/25552 [00:00<00:00, 126669.62it/s]
100%|██████████| 25552/25552 [00:00<00:00, 137260.49it/s]
DONE (t=51.25s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.376
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.737
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.351
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.437
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.903
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.492
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.859
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.477
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.369
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.545
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.917
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.492
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.492
05/11 10:33:29 - mmengine - INFO - segm_mAP_copypaste: 0.376 0.737 0.351 0.226 0.437 0.903
05/11 10:33:29 - mmengine - INFO - segm_mAR_copypaste: 0.492 0.859 0.477 0.369 0.545 0.917
05/11 10:33:30 - mmengine - INFO - Iter(val) [100/100]    coco/segm_mAP: 0.3760  coco/segm_mAP_50: 0.7370  coco/segm_mAP_75: 0.3510  coco/segm_mAP_s: 0.2260  coco/segm_mAP_m: 0.4370  coco/segm_mAP_l: 0.9030  data_time: 0.0174  time: 0.7315
05/11 10:33:30 - mmengine - INFO - The previous best checkpoint /data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-L_8xb32-24k_coco/best_coco_segm_mAP_50_iter_8000.pth is removed
05/11 10:33:32 - mmengine - INFO - The best checkpoint with 0.7370 coco/segm_mAP_50 at 10000 iter is saved to best_coco_segm_mAP_50_iter_10000.pth.
05/11 10:34:55 - mmengine - INFO - Iter(train) [10050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:07:48  time: 3.3695  data_time: 1.8768  memory: 39141  grad_norm: 35.9118  loss: 27.0598  loss_cls: 0.5769  loss_mask: 0.1867  loss_dice: 1.7641  d0.loss_cls: 0.9705  d0.loss_mask: 0.2530  d0.loss_dice: 2.0967  d1.loss_cls: 0.8342  d1.loss_mask: 0.2242  d1.loss_dice: 1.9750  d2.loss_cls: 0.6642  d2.loss_mask: 0.2049  d2.loss_dice: 1.9033  d3.loss_cls: 0.6134  d3.loss_mask: 0.1991  d3.loss_dice: 1.8413  d4.loss_cls: 0.5900  d4.loss_mask: 0.1931  d4.loss_dice: 1.7934  d5.loss_cls: 0.5928  d5.loss_mask: 0.1911  d5.loss_dice: 1.7704  d6.loss_cls: 0.5870  d6.loss_mask: 0.1916  d6.loss_dice: 1.7801  d7.loss_cls: 0.5831  d7.loss_mask: 0.1898  d7.loss_dice: 1.7688  d8.loss_cls: 0.5766  d8.loss_mask: 0.1874  d8.loss_dice: 1.7572
05/11 10:36:11 - mmengine - INFO - Iter(train) [10100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:06:25  time: 1.5195  data_time: 0.0268  memory: 38431  grad_norm: 35.6911  loss: 27.3979  loss_cls: 0.5779  loss_mask: 0.1841  loss_dice: 1.7993  d0.loss_cls: 0.9638  d0.loss_mask: 0.2485  d0.loss_dice: 2.1114  d1.loss_cls: 0.8136  d1.loss_mask: 0.2174  d1.loss_dice: 2.0115  d2.loss_cls: 0.6677  d2.loss_mask: 0.2036  d2.loss_dice: 1.9387  d3.loss_cls: 0.6146  d3.loss_mask: 0.1939  d3.loss_dice: 1.8876  d4.loss_cls: 0.5936  d4.loss_mask: 0.1901  d4.loss_dice: 1.8432  d5.loss_cls: 0.5944  d5.loss_mask: 0.1883  d5.loss_dice: 1.8152  d6.loss_cls: 0.5928  d6.loss_mask: 0.1868  d6.loss_dice: 1.8203  d7.loss_cls: 0.5810  d7.loss_mask: 0.1858  d7.loss_dice: 1.8073  d8.loss_cls: 0.5775  d8.loss_mask: 0.1839  d8.loss_dice: 1.8042
05/11 10:37:27 - mmengine - INFO - Iter(train) [10150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:05:02  time: 1.5294  data_time: 0.0276  memory: 39308  grad_norm: 35.1497  loss: 26.9629  loss_cls: 0.5700  loss_mask: 0.1841  loss_dice: 1.7666  d0.loss_cls: 0.9759  d0.loss_mask: 0.2480  d0.loss_dice: 2.0747  d1.loss_cls: 0.8175  d1.loss_mask: 0.2193  d1.loss_dice: 1.9707  d2.loss_cls: 0.6600  d2.loss_mask: 0.2009  d2.loss_dice: 1.8891  d3.loss_cls: 0.6022  d3.loss_mask: 0.1927  d3.loss_dice: 1.8445  d4.loss_cls: 0.5883  d4.loss_mask: 0.1885  d4.loss_dice: 1.7925  d5.loss_cls: 0.5956  d5.loss_mask: 0.1887  d5.loss_dice: 1.7728  d6.loss_cls: 0.5847  d6.loss_mask: 0.1879  d6.loss_dice: 1.7845  d7.loss_cls: 0.5785  d7.loss_mask: 0.1853  d7.loss_dice: 1.7772  d8.loss_cls: 0.5719  d8.loss_mask: 0.1844  d8.loss_dice: 1.7655
05/11 10:38:44 - mmengine - INFO - Iter(train) [10200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:03:40  time: 1.5296  data_time: 0.0312  memory: 39026  grad_norm: 32.4614  loss: 28.5291  loss_cls: 0.5782  loss_mask: 0.1837  loss_dice: 1.9049  d0.loss_cls: 0.9820  d0.loss_mask: 0.2455  d0.loss_dice: 2.2340  d1.loss_cls: 0.8259  d1.loss_mask: 0.2186  d1.loss_dice: 2.1193  d2.loss_cls: 0.6726  d2.loss_mask: 0.1990  d2.loss_dice: 2.0441  d3.loss_cls: 0.6162  d3.loss_mask: 0.1921  d3.loss_dice: 1.9884  d4.loss_cls: 0.6040  d4.loss_mask: 0.1879  d4.loss_dice: 1.9424  d5.loss_cls: 0.6151  d5.loss_mask: 0.1881  d5.loss_dice: 1.9138  d6.loss_cls: 0.5958  d6.loss_mask: 0.1871  d6.loss_dice: 1.9339  d7.loss_cls: 0.5822  d7.loss_mask: 0.1867  d7.loss_dice: 1.9207  d8.loss_cls: 0.5791  d8.loss_mask: 0.1845  d8.loss_dice: 1.9034
05/11 10:40:00 - mmengine - INFO - Iter(train) [10250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:02:17  time: 1.5260  data_time: 0.0274  memory: 39090  grad_norm: 35.2432  loss: 26.7945  loss_cls: 0.5625  loss_mask: 0.1644  loss_dice: 1.7783  d0.loss_cls: 0.9718  d0.loss_mask: 0.2170  d0.loss_dice: 2.0721  d1.loss_cls: 0.8182  d1.loss_mask: 0.1983  d1.loss_dice: 1.9737  d2.loss_cls: 0.6680  d2.loss_mask: 0.1782  d2.loss_dice: 1.8930  d3.loss_cls: 0.6042  d3.loss_mask: 0.1714  d3.loss_dice: 1.8484  d4.loss_cls: 0.6162  d4.loss_mask: 0.1669  d4.loss_dice: 1.7776  d5.loss_cls: 0.5936  d5.loss_mask: 0.1678  d5.loss_dice: 1.7905  d6.loss_cls: 0.5824  d6.loss_mask: 0.1666  d6.loss_dice: 1.7993  d7.loss_cls: 0.5673  d7.loss_mask: 0.1661  d7.loss_dice: 1.7825  d8.loss_cls: 0.5634  d8.loss_mask: 0.1643  d8.loss_dice: 1.7704
05/11 10:41:17 - mmengine - INFO - Iter(train) [10300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:00:55  time: 1.5363  data_time: 0.0357  memory: 38433  grad_norm: 36.9687  loss: 26.5447  loss_cls: 0.5745  loss_mask: 0.1637  loss_dice: 1.7383  d0.loss_cls: 0.9633  d0.loss_mask: 0.2126  d0.loss_dice: 2.0475  d1.loss_cls: 0.8229  d1.loss_mask: 0.1957  d1.loss_dice: 1.9425  d2.loss_cls: 0.6644  d2.loss_mask: 0.1777  d2.loss_dice: 1.8671  d3.loss_cls: 0.6142  d3.loss_mask: 0.1716  d3.loss_dice: 1.8131  d4.loss_cls: 0.6621  d4.loss_mask: 0.1672  d4.loss_dice: 1.6967  d5.loss_cls: 0.6018  d5.loss_mask: 0.1706  d5.loss_dice: 1.7665  d6.loss_cls: 0.5931  d6.loss_mask: 0.1689  d6.loss_dice: 1.7727  d7.loss_cls: 0.5773  d7.loss_mask: 0.1653  d7.loss_dice: 1.7551  d8.loss_cls: 0.5761  d8.loss_mask: 0.1644  d8.loss_dice: 1.7377
05/11 10:42:34 - mmengine - INFO - Iter(train) [10350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:59:33  time: 1.5425  data_time: 0.0307  memory: 38734  grad_norm: 35.8459  loss: 28.4843  loss_cls: 0.6048  loss_mask: 0.1775  loss_dice: 1.8871  d0.loss_cls: 0.9899  d0.loss_mask: 0.2351  d0.loss_dice: 2.2084  d1.loss_cls: 0.8373  d1.loss_mask: 0.2136  d1.loss_dice: 2.1042  d2.loss_cls: 0.6965  d2.loss_mask: 0.1956  d2.loss_dice: 2.0182  d3.loss_cls: 0.6407  d3.loss_mask: 0.1857  d3.loss_dice: 1.9611  d4.loss_cls: 0.6933  d4.loss_mask: 0.1834  d4.loss_dice: 1.8368  d5.loss_cls: 0.6254  d5.loss_mask: 0.1825  d5.loss_dice: 1.9223  d6.loss_cls: 0.6158  d6.loss_mask: 0.1806  d6.loss_dice: 1.9217  d7.loss_cls: 0.6077  d7.loss_mask: 0.1795  d7.loss_dice: 1.9079  d8.loss_cls: 0.6058  d8.loss_mask: 0.1777  d8.loss_dice: 1.8881
05/11 10:43:50 - mmengine - INFO - Iter(train) [10400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:58:11  time: 1.5286  data_time: 0.0298  memory: 38973  grad_norm: 35.3706  loss: 27.4840  loss_cls: 0.5579  loss_mask: 0.1772  loss_dice: 1.8251  d0.loss_cls: 0.9790  d0.loss_mask: 0.2359  d0.loss_dice: 2.1475  d1.loss_cls: 0.7951  d1.loss_mask: 0.2103  d1.loss_dice: 2.0311  d2.loss_cls: 0.6561  d2.loss_mask: 0.1922  d2.loss_dice: 1.9546  d3.loss_cls: 0.5918  d3.loss_mask: 0.1855  d3.loss_dice: 1.8990  d4.loss_cls: 0.6375  d4.loss_mask: 0.1818  d4.loss_dice: 1.8377  d5.loss_cls: 0.5813  d5.loss_mask: 0.1821  d5.loss_dice: 1.8653  d6.loss_cls: 0.5755  d6.loss_mask: 0.1820  d6.loss_dice: 1.8592  d7.loss_cls: 0.5655  d7.loss_mask: 0.1800  d7.loss_dice: 1.8363  d8.loss_cls: 0.5584  d8.loss_mask: 0.1776  d8.loss_dice: 1.8256
05/11 10:45:07 - mmengine - INFO - Iter(train) [10450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:56:49  time: 1.5318  data_time: 0.0317  memory: 38999  grad_norm: 34.5273  loss: 27.7035  loss_cls: 0.5946  loss_mask: 0.1877  loss_dice: 1.7950  d0.loss_cls: 0.9911  d0.loss_mask: 0.2585  d0.loss_dice: 2.1516  d1.loss_cls: 0.8422  d1.loss_mask: 0.2261  d1.loss_dice: 2.0373  d2.loss_cls: 0.6762  d2.loss_mask: 0.2044  d2.loss_dice: 1.9438  d3.loss_cls: 0.6199  d3.loss_mask: 0.1968  d3.loss_dice: 1.8856  d4.loss_cls: 0.6435  d4.loss_mask: 0.1938  d4.loss_dice: 1.8276  d5.loss_cls: 0.6121  d5.loss_mask: 0.1934  d5.loss_dice: 1.8259  d6.loss_cls: 0.6064  d6.loss_mask: 0.1913  d6.loss_dice: 1.8225  d7.loss_cls: 0.5937  d7.loss_mask: 0.1905  d7.loss_dice: 1.8112  d8.loss_cls: 0.5934  d8.loss_mask: 0.1881  d8.loss_dice: 1.7994
05/11 10:46:23 - mmengine - INFO - Iter(train) [10500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:55:26  time: 1.5261  data_time: 0.0378  memory: 38803  grad_norm: 44.3617  loss: 27.2123  loss_cls: 0.5722  loss_mask: 0.1867  loss_dice: 1.7836  d0.loss_cls: 0.9863  d0.loss_mask: 0.2513  d0.loss_dice: 2.0814  d1.loss_cls: 0.8184  d1.loss_mask: 0.2232  d1.loss_dice: 1.9766  d2.loss_cls: 0.6601  d2.loss_mask: 0.2025  d2.loss_dice: 1.9154  d3.loss_cls: 0.6102  d3.loss_mask: 0.1949  d3.loss_dice: 1.8603  d4.loss_cls: 0.6271  d4.loss_mask: 0.1919  d4.loss_dice: 1.8055  d5.loss_cls: 0.5869  d5.loss_mask: 0.1908  d5.loss_dice: 1.8050  d6.loss_cls: 0.5823  d6.loss_mask: 0.1891  d6.loss_dice: 1.8071  d7.loss_cls: 0.5815  d7.loss_mask: 0.1870  d7.loss_dice: 1.7910  d8.loss_cls: 0.5775  d8.loss_mask: 0.1861  d8.loss_dice: 1.7807
05/11 10:47:40 - mmengine - INFO - Iter(train) [10550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:54:04  time: 1.5283  data_time: 0.0315  memory: 39202  grad_norm: 35.8798  loss: 27.9912  loss_cls: 0.5996  loss_mask: 0.1880  loss_dice: 1.8281  d0.loss_cls: 1.0033  d0.loss_mask: 0.2459  d0.loss_dice: 2.1519  d1.loss_cls: 0.8477  d1.loss_mask: 0.2262  d1.loss_dice: 2.0366  d2.loss_cls: 0.6804  d2.loss_mask: 0.2076  d2.loss_dice: 1.9675  d3.loss_cls: 0.6250  d3.loss_mask: 0.1966  d3.loss_dice: 1.9203  d4.loss_cls: 0.6463  d4.loss_mask: 0.1945  d4.loss_dice: 1.8656  d5.loss_cls: 0.6042  d5.loss_mask: 0.1935  d5.loss_dice: 1.8593  d6.loss_cls: 0.6012  d6.loss_mask: 0.1921  d6.loss_dice: 1.8615  d7.loss_cls: 0.5941  d7.loss_mask: 0.1896  d7.loss_dice: 1.8497  d8.loss_cls: 0.6005  d8.loss_mask: 0.1880  d8.loss_dice: 1.8264
05/11 10:48:56 - mmengine - INFO - Iter(train) [10600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:52:42  time: 1.5312  data_time: 0.0296  memory: 39033  grad_norm: 36.2619  loss: 28.8232  loss_cls: 0.5833  loss_mask: 0.1821  loss_dice: 1.9204  d0.loss_cls: 1.0170  d0.loss_mask: 0.2363  d0.loss_dice: 2.2705  d1.loss_cls: 0.8266  d1.loss_mask: 0.2175  d1.loss_dice: 2.1666  d2.loss_cls: 0.6649  d2.loss_mask: 0.2029  d2.loss_dice: 2.0718  d3.loss_cls: 0.6139  d3.loss_mask: 0.1934  d3.loss_dice: 2.0089  d4.loss_cls: 0.6227  d4.loss_mask: 0.1902  d4.loss_dice: 1.9693  d5.loss_cls: 0.5932  d5.loss_mask: 0.1874  d5.loss_dice: 1.9622  d6.loss_cls: 0.5882  d6.loss_mask: 0.1868  d6.loss_dice: 1.9581  d7.loss_cls: 0.5855  d7.loss_mask: 0.1842  d7.loss_dice: 1.9327  d8.loss_cls: 0.5843  d8.loss_mask: 0.1823  d8.loss_dice: 1.9203
05/11 10:50:12 - mmengine - INFO - Iter(train) [10650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:51:19  time: 1.5189  data_time: 0.0317  memory: 38370  grad_norm: 36.6595  loss: 27.1269  loss_cls: 0.5936  loss_mask: 0.1744  loss_dice: 1.7832  d0.loss_cls: 0.9645  d0.loss_mask: 0.2284  d0.loss_dice: 2.0676  d1.loss_cls: 0.8274  d1.loss_mask: 0.2083  d1.loss_dice: 1.9610  d2.loss_cls: 0.6643  d2.loss_mask: 0.1885  d2.loss_dice: 1.9045  d3.loss_cls: 0.6209  d3.loss_mask: 0.1808  d3.loss_dice: 1.8596  d4.loss_cls: 0.6266  d4.loss_mask: 0.1777  d4.loss_dice: 1.8259  d5.loss_cls: 0.5917  d5.loss_mask: 0.1784  d5.loss_dice: 1.8147  d6.loss_cls: 0.5893  d6.loss_mask: 0.1772  d6.loss_dice: 1.8153  d7.loss_cls: 0.5899  d7.loss_mask: 0.1758  d7.loss_dice: 1.7911  d8.loss_cls: 0.5892  d8.loss_mask: 0.1750  d8.loss_dice: 1.7817
05/11 10:51:29 - mmengine - INFO - Iter(train) [10700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:49:57  time: 1.5293  data_time: 0.0319  memory: 38223  grad_norm: 34.1309  loss: 27.7381  loss_cls: 0.5801  loss_mask: 0.1788  loss_dice: 1.8344  d0.loss_cls: 1.0013  d0.loss_mask: 0.2360  d0.loss_dice: 2.1200  d1.loss_cls: 0.8274  d1.loss_mask: 0.2119  d1.loss_dice: 2.0403  d2.loss_cls: 0.6664  d2.loss_mask: 0.1917  d2.loss_dice: 1.9666  d3.loss_cls: 0.6177  d3.loss_mask: 0.1878  d3.loss_dice: 1.9153  d4.loss_cls: 0.6222  d4.loss_mask: 0.1863  d4.loss_dice: 1.8665  d5.loss_cls: 0.5918  d5.loss_mask: 0.1845  d5.loss_dice: 1.8676  d6.loss_cls: 0.5852  d6.loss_mask: 0.1836  d6.loss_dice: 1.8667  d7.loss_cls: 0.5807  d7.loss_mask: 0.1799  d7.loss_dice: 1.8539  d8.loss_cls: 0.5816  d8.loss_mask: 0.1798  d8.loss_dice: 1.8321
05/11 10:52:45 - mmengine - INFO - Iter(train) [10750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:48:35  time: 1.5338  data_time: 0.0324  memory: 39889  grad_norm: 36.6745  loss: 29.6029  loss_cls: 0.5863  loss_mask: 0.1786  loss_dice: 2.0006  d0.loss_cls: 1.0416  d0.loss_mask: 0.2312  d0.loss_dice: 2.3147  d1.loss_cls: 0.8410  d1.loss_mask: 0.2151  d1.loss_dice: 2.2098  d2.loss_cls: 0.6877  d2.loss_mask: 0.1960  d2.loss_dice: 2.1416  d3.loss_cls: 0.6278  d3.loss_mask: 0.1875  d3.loss_dice: 2.0851  d4.loss_cls: 0.6280  d4.loss_mask: 0.1849  d4.loss_dice: 2.0460  d5.loss_cls: 0.6037  d5.loss_mask: 0.1834  d5.loss_dice: 2.0380  d6.loss_cls: 0.5969  d6.loss_mask: 0.1826  d6.loss_dice: 2.0321  d7.loss_cls: 0.5877  d7.loss_mask: 0.1805  d7.loss_dice: 2.0255  d8.loss_cls: 0.5906  d8.loss_mask: 0.1796  d8.loss_dice: 1.9990
05/11 10:54:01 - mmengine - INFO - Iter(train) [10800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:47:13  time: 1.5210  data_time: 0.0317  memory: 38618  grad_norm: 39.9177  loss: 27.4391  loss_cls: 0.5771  loss_mask: 0.1771  loss_dice: 1.8131  d0.loss_cls: 0.9831  d0.loss_mask: 0.2422  d0.loss_dice: 2.0925  d1.loss_cls: 0.8084  d1.loss_mask: 0.2141  d1.loss_dice: 2.0013  d2.loss_cls: 0.6557  d2.loss_mask: 0.1919  d2.loss_dice: 1.9488  d3.loss_cls: 0.6059  d3.loss_mask: 0.1855  d3.loss_dice: 1.8965  d4.loss_cls: 0.6145  d4.loss_mask: 0.1833  d4.loss_dice: 1.8542  d5.loss_cls: 0.5948  d5.loss_mask: 0.1842  d5.loss_dice: 1.8504  d6.loss_cls: 0.5847  d6.loss_mask: 0.1825  d6.loss_dice: 1.8441  d7.loss_cls: 0.5784  d7.loss_mask: 0.1799  d7.loss_dice: 1.8270  d8.loss_cls: 0.5760  d8.loss_mask: 0.1776  d8.loss_dice: 1.8143
05/11 10:55:18 - mmengine - INFO - Iter(train) [10850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:45:51  time: 1.5340  data_time: 0.0319  memory: 39779  grad_norm: 38.6327  loss: 27.5598  loss_cls: 0.5690  loss_mask: 0.1694  loss_dice: 1.8281  d0.loss_cls: 1.0021  d0.loss_mask: 0.2245  d0.loss_dice: 2.1354  d1.loss_cls: 0.8273  d1.loss_mask: 0.2071  d1.loss_dice: 2.0529  d2.loss_cls: 0.6611  d2.loss_mask: 0.1853  d2.loss_dice: 1.9633  d3.loss_cls: 0.6120  d3.loss_mask: 0.1793  d3.loss_dice: 1.9151  d4.loss_cls: 0.6044  d4.loss_mask: 0.1753  d4.loss_dice: 1.8734  d5.loss_cls: 0.5815  d5.loss_mask: 0.1743  d5.loss_dice: 1.8580  d6.loss_cls: 0.5788  d6.loss_mask: 0.1734  d6.loss_dice: 1.8606  d7.loss_cls: 0.5700  d7.loss_mask: 0.1712  d7.loss_dice: 1.8476  d8.loss_cls: 0.5686  d8.loss_mask: 0.1691  d8.loss_dice: 1.8217
05/11 10:56:35 - mmengine - INFO - Iter(train) [10900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:44:30  time: 1.5372  data_time: 0.0381  memory: 39066  grad_norm: 37.8785  loss: 27.9592  loss_cls: 0.6058  loss_mask: 0.1716  loss_dice: 1.8276  d0.loss_cls: 1.0121  d0.loss_mask: 0.2325  d0.loss_dice: 2.1483  d1.loss_cls: 0.8581  d1.loss_mask: 0.2109  d1.loss_dice: 2.0558  d2.loss_cls: 0.6947  d2.loss_mask: 0.1907  d2.loss_dice: 1.9665  d3.loss_cls: 0.6395  d3.loss_mask: 0.1834  d3.loss_dice: 1.9175  d4.loss_cls: 0.6447  d4.loss_mask: 0.1809  d4.loss_dice: 1.8751  d5.loss_cls: 0.6221  d5.loss_mask: 0.1777  d5.loss_dice: 1.8614  d6.loss_cls: 0.6119  d6.loss_mask: 0.1761  d6.loss_dice: 1.8600  d7.loss_cls: 0.6046  d7.loss_mask: 0.1759  d7.loss_dice: 1.8452  d8.loss_cls: 0.6039  d8.loss_mask: 0.1732  d8.loss_dice: 1.8316
05/11 10:57:51 - mmengine - INFO - Iter(train) [10950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:43:08  time: 1.5291  data_time: 0.0415  memory: 38866  grad_norm: 37.1751  loss: 28.9948  loss_cls: 0.5968  loss_mask: 0.1858  loss_dice: 1.9302  d0.loss_cls: 1.0043  d0.loss_mask: 0.2505  d0.loss_dice: 2.2409  d1.loss_cls: 0.8435  d1.loss_mask: 0.2255  d1.loss_dice: 2.1411  d2.loss_cls: 0.6837  d2.loss_mask: 0.2060  d2.loss_dice: 2.0671  d3.loss_cls: 0.6317  d3.loss_mask: 0.1983  d3.loss_dice: 2.0261  d4.loss_cls: 0.6284  d4.loss_mask: 0.1934  d4.loss_dice: 1.9826  d5.loss_cls: 0.6004  d5.loss_mask: 0.1922  d5.loss_dice: 1.9670  d6.loss_cls: 0.5981  d6.loss_mask: 0.1911  d6.loss_dice: 1.9698  d7.loss_cls: 0.5948  d7.loss_mask: 0.1884  d7.loss_dice: 1.9466  d8.loss_cls: 0.5972  d8.loss_mask: 0.1854  d8.loss_dice: 1.9280
05/11 10:59:08 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 10:59:08 - mmengine - INFO - Iter(train) [11000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:41:47  time: 1.5293  data_time: 0.0309  memory: 38731  grad_norm: 35.9350  loss: 24.4749  loss_cls: 0.5311  loss_mask: 0.1735  loss_dice: 1.5682  d0.loss_cls: 0.9869  d0.loss_mask: 0.2403  d0.loss_dice: 1.8336  d1.loss_cls: 0.7738  d1.loss_mask: 0.2066  d1.loss_dice: 1.7571  d2.loss_cls: 0.6274  d2.loss_mask: 0.1894  d2.loss_dice: 1.6865  d3.loss_cls: 0.5733  d3.loss_mask: 0.1816  d3.loss_dice: 1.6384  d4.loss_cls: 0.5593  d4.loss_mask: 0.1788  d4.loss_dice: 1.6034  d5.loss_cls: 0.5384  d5.loss_mask: 0.1778  d5.loss_dice: 1.5936  d6.loss_cls: 0.5361  d6.loss_mask: 0.1772  d6.loss_dice: 1.5935  d7.loss_cls: 0.5335  d7.loss_mask: 0.1756  d7.loss_dice: 1.5725  d8.loss_cls: 0.5315  d8.loss_mask: 0.1738  d8.loss_dice: 1.5622
05/11 11:00:24 - mmengine - INFO - Iter(train) [11050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:40:25  time: 1.5264  data_time: 0.0306  memory: 38575  grad_norm: 37.2230  loss: 26.4579  loss_cls: 0.5746  loss_mask: 0.1769  loss_dice: 1.7145  d0.loss_cls: 0.9949  d0.loss_mask: 0.2335  d0.loss_dice: 2.0024  d1.loss_cls: 0.8308  d1.loss_mask: 0.2111  d1.loss_dice: 1.9198  d2.loss_cls: 0.6573  d2.loss_mask: 0.1936  d2.loss_dice: 1.8504  d3.loss_cls: 0.6007  d3.loss_mask: 0.1869  d3.loss_dice: 1.8036  d4.loss_cls: 0.6029  d4.loss_mask: 0.1836  d4.loss_dice: 1.7579  d5.loss_cls: 0.5811  d5.loss_mask: 0.1823  d5.loss_dice: 1.7508  d6.loss_cls: 0.5785  d6.loss_mask: 0.1810  d6.loss_dice: 1.7456  d7.loss_cls: 0.5712  d7.loss_mask: 0.1790  d7.loss_dice: 1.7310  d8.loss_cls: 0.5701  d8.loss_mask: 0.1774  d8.loss_dice: 1.7147
05/11 11:01:41 - mmengine - INFO - Iter(train) [11100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:39:03  time: 1.5325  data_time: 0.0277  memory: 39098  grad_norm: 45.2799  loss: 30.6129  loss_cls: 0.5931  loss_mask: 0.1996  loss_dice: 2.0660  d0.loss_cls: 1.0638  d0.loss_mask: 0.2616  d0.loss_dice: 2.4130  d1.loss_cls: 0.8418  d1.loss_mask: 0.2441  d1.loss_dice: 2.3267  d2.loss_cls: 0.6855  d2.loss_mask: 0.2201  d2.loss_dice: 2.2325  d3.loss_cls: 0.6311  d3.loss_mask: 0.2098  d3.loss_dice: 2.1682  d4.loss_cls: 0.6170  d4.loss_mask: 0.2026  d4.loss_dice: 2.1175  d5.loss_cls: 0.6000  d5.loss_mask: 0.2050  d5.loss_dice: 2.0994  d6.loss_cls: 0.5962  d6.loss_mask: 0.2036  d6.loss_dice: 2.0969  d7.loss_cls: 0.5977  d7.loss_mask: 0.2015  d7.loss_dice: 2.0697  d8.loss_cls: 0.5951  d8.loss_mask: 0.1997  d8.loss_dice: 2.0541
05/11 11:02:57 - mmengine - INFO - Iter(train) [11150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:37:42  time: 1.5320  data_time: 0.0317  memory: 38100  grad_norm: 39.6650  loss: 26.3247  loss_cls: 0.5650  loss_mask: 0.1763  loss_dice: 1.7197  d0.loss_cls: 0.9950  d0.loss_mask: 0.2292  d0.loss_dice: 1.9730  d1.loss_cls: 0.8234  d1.loss_mask: 0.2088  d1.loss_dice: 1.8982  d2.loss_cls: 0.6559  d2.loss_mask: 0.1930  d2.loss_dice: 1.8317  d3.loss_cls: 0.5966  d3.loss_mask: 0.1861  d3.loss_dice: 1.7998  d4.loss_cls: 0.5893  d4.loss_mask: 0.1821  d4.loss_dice: 1.7581  d5.loss_cls: 0.5688  d5.loss_mask: 0.1813  d5.loss_dice: 1.7480  d6.loss_cls: 0.5711  d6.loss_mask: 0.1805  d6.loss_dice: 1.7449  d7.loss_cls: 0.5696  d7.loss_mask: 0.1778  d7.loss_dice: 1.7372  d8.loss_cls: 0.5686  d8.loss_mask: 0.1777  d8.loss_dice: 1.7180
05/11 11:04:14 - mmengine - INFO - Iter(train) [11200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:36:20  time: 1.5348  data_time: 0.0321  memory: 39235  grad_norm: 34.3571  loss: 27.2174  loss_cls: 0.5656  loss_mask: 0.1759  loss_dice: 1.7950  d0.loss_cls: 1.0239  d0.loss_mask: 0.2336  d0.loss_dice: 2.0895  d1.loss_cls: 0.8253  d1.loss_mask: 0.2116  d1.loss_dice: 2.0038  d2.loss_cls: 0.6687  d2.loss_mask: 0.1924  d2.loss_dice: 1.9284  d3.loss_cls: 0.6042  d3.loss_mask: 0.1835  d3.loss_dice: 1.8776  d4.loss_cls: 0.5938  d4.loss_mask: 0.1797  d4.loss_dice: 1.8393  d5.loss_cls: 0.5699  d5.loss_mask: 0.1789  d5.loss_dice: 1.8260  d6.loss_cls: 0.5666  d6.loss_mask: 0.1785  d6.loss_dice: 1.8211  d7.loss_cls: 0.5646  d7.loss_mask: 0.1768  d7.loss_dice: 1.8075  d8.loss_cls: 0.5684  d8.loss_mask: 0.1758  d8.loss_dice: 1.7916
05/11 11:05:30 - mmengine - INFO - Iter(train) [11250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:34:58  time: 1.5165  data_time: 0.0318  memory: 38462  grad_norm: 40.7816  loss: 27.8672  loss_cls: 0.5850  loss_mask: 0.1890  loss_dice: 1.8261  d0.loss_cls: 1.0121  d0.loss_mask: 0.2508  d0.loss_dice: 2.1467  d1.loss_cls: 0.8324  d1.loss_mask: 0.2284  d1.loss_dice: 2.0484  d2.loss_cls: 0.6715  d2.loss_mask: 0.2066  d2.loss_dice: 1.9811  d3.loss_cls: 0.6121  d3.loss_mask: 0.1985  d3.loss_dice: 1.9230  d4.loss_cls: 0.5993  d4.loss_mask: 0.1928  d4.loss_dice: 1.8781  d5.loss_cls: 0.5845  d5.loss_mask: 0.1920  d5.loss_dice: 1.8595  d6.loss_cls: 0.5852  d6.loss_mask: 0.1909  d6.loss_dice: 1.8542  d7.loss_cls: 0.5836  d7.loss_mask: 0.1906  d7.loss_dice: 1.8414  d8.loss_cls: 0.5861  d8.loss_mask: 0.1886  d8.loss_dice: 1.8284
05/11 11:06:47 - mmengine - INFO - Iter(train) [11300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:33:38  time: 1.5437  data_time: 0.0376  memory: 38643  grad_norm: 37.4868  loss: 26.0913  loss_cls: 0.5650  loss_mask: 0.1683  loss_dice: 1.6911  d0.loss_cls: 1.0161  d0.loss_mask: 0.2240  d0.loss_dice: 1.9900  d1.loss_cls: 0.8113  d1.loss_mask: 0.2026  d1.loss_dice: 1.9152  d2.loss_cls: 0.6749  d2.loss_mask: 0.1847  d2.loss_dice: 1.8374  d3.loss_cls: 0.6017  d3.loss_mask: 0.1763  d3.loss_dice: 1.7791  d4.loss_cls: 0.5764  d4.loss_mask: 0.1750  d4.loss_dice: 1.7325  d5.loss_cls: 0.5661  d5.loss_mask: 0.1726  d5.loss_dice: 1.7136  d6.loss_cls: 0.5663  d6.loss_mask: 0.1714  d6.loss_dice: 1.7173  d7.loss_cls: 0.5639  d7.loss_mask: 0.1704  d7.loss_dice: 1.7023  d8.loss_cls: 0.5680  d8.loss_mask: 0.1681  d8.loss_dice: 1.6894
05/11 11:08:04 - mmengine - INFO - Iter(train) [11350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:32:17  time: 1.5422  data_time: 0.0316  memory: 38683  grad_norm: 37.1851  loss: 26.6873  loss_cls: 0.5711  loss_mask: 0.1740  loss_dice: 1.7465  d0.loss_cls: 1.0191  d0.loss_mask: 0.2309  d0.loss_dice: 2.0256  d1.loss_cls: 0.8177  d1.loss_mask: 0.2077  d1.loss_dice: 1.9655  d2.loss_cls: 0.6700  d2.loss_mask: 0.1950  d2.loss_dice: 1.8812  d3.loss_cls: 0.6049  d3.loss_mask: 0.1833  d3.loss_dice: 1.8246  d4.loss_cls: 0.5872  d4.loss_mask: 0.1801  d4.loss_dice: 1.7796  d5.loss_cls: 0.5768  d5.loss_mask: 0.1785  d5.loss_dice: 1.7646  d6.loss_cls: 0.5727  d6.loss_mask: 0.1773  d6.loss_dice: 1.7639  d7.loss_cls: 0.5713  d7.loss_mask: 0.1755  d7.loss_dice: 1.7508  d8.loss_cls: 0.5749  d8.loss_mask: 0.1755  d8.loss_dice: 1.7413
05/11 11:09:22 - mmengine - INFO - Iter(train) [11400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:30:57  time: 1.5577  data_time: 0.0319  memory: 38586  grad_norm: 37.9735  loss: 28.1223  loss_cls: 0.6071  loss_mask: 0.1777  loss_dice: 1.8420  d0.loss_cls: 1.0385  d0.loss_mask: 0.2377  d0.loss_dice: 2.1416  d1.loss_cls: 0.8336  d1.loss_mask: 0.2137  d1.loss_dice: 2.0773  d2.loss_cls: 0.6811  d2.loss_mask: 0.1985  d2.loss_dice: 2.0053  d3.loss_cls: 0.6202  d3.loss_mask: 0.1902  d3.loss_dice: 1.9530  d4.loss_cls: 0.6180  d4.loss_mask: 0.1855  d4.loss_dice: 1.8879  d5.loss_cls: 0.6043  d5.loss_mask: 0.1834  d5.loss_dice: 1.8813  d6.loss_cls: 0.6010  d6.loss_mask: 0.1825  d6.loss_dice: 1.8781  d7.loss_cls: 0.6018  d7.loss_mask: 0.1804  d7.loss_dice: 1.8660  d8.loss_cls: 0.6053  d8.loss_mask: 0.1787  d8.loss_dice: 1.8506
05/11 11:10:39 - mmengine - INFO - Iter(train) [11450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:29:36  time: 1.5324  data_time: 0.0319  memory: 39530  grad_norm: 36.4913  loss: 29.7604  loss_cls: 0.6021  loss_mask: 0.1816  loss_dice: 2.0065  d0.loss_cls: 1.0587  d0.loss_mask: 0.2418  d0.loss_dice: 2.3093  d1.loss_cls: 0.8381  d1.loss_mask: 0.2216  d1.loss_dice: 2.2291  d2.loss_cls: 0.6987  d2.loss_mask: 0.2001  d2.loss_dice: 2.1589  d3.loss_cls: 0.6344  d3.loss_mask: 0.1917  d3.loss_dice: 2.1033  d4.loss_cls: 0.6230  d4.loss_mask: 0.1852  d4.loss_dice: 2.0469  d5.loss_cls: 0.6016  d5.loss_mask: 0.1864  d5.loss_dice: 2.0318  d6.loss_cls: 0.5998  d6.loss_mask: 0.1854  d6.loss_dice: 2.0324  d7.loss_cls: 0.6001  d7.loss_mask: 0.1845  d7.loss_dice: 2.0219  d8.loss_cls: 0.6049  d8.loss_mask: 0.1816  d8.loss_dice: 1.9988
05/11 11:11:56 - mmengine - INFO - Iter(train) [11500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:28:15  time: 1.5396  data_time: 0.0319  memory: 38618  grad_norm: 35.1448  loss: 27.4542  loss_cls: 0.6052  loss_mask: 0.1759  loss_dice: 1.7842  d0.loss_cls: 1.0180  d0.loss_mask: 0.2322  d0.loss_dice: 2.0605  d1.loss_cls: 0.8592  d1.loss_mask: 0.2120  d1.loss_dice: 1.9821  d2.loss_cls: 0.6931  d2.loss_mask: 0.1928  d2.loss_dice: 1.9237  d3.loss_cls: 0.6359  d3.loss_mask: 0.1841  d3.loss_dice: 1.8708  d4.loss_cls: 0.6303  d4.loss_mask: 0.1823  d4.loss_dice: 1.8224  d5.loss_cls: 0.6257  d5.loss_mask: 0.1814  d5.loss_dice: 1.8123  d6.loss_cls: 0.6184  d6.loss_mask: 0.1797  d6.loss_dice: 1.8125  d7.loss_cls: 0.6131  d7.loss_mask: 0.1784  d7.loss_dice: 1.8015  d8.loss_cls: 0.6065  d8.loss_mask: 0.1761  d8.loss_dice: 1.7838
05/11 11:13:13 - mmengine - INFO - Iter(train) [11550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:26:55  time: 1.5465  data_time: 0.0317  memory: 38700  grad_norm: 34.3796  loss: 26.4152  loss_cls: 0.5615  loss_mask: 0.1718  loss_dice: 1.7268  d0.loss_cls: 1.0136  d0.loss_mask: 0.2224  d0.loss_dice: 2.0052  d1.loss_cls: 0.8007  d1.loss_mask: 0.2060  d1.loss_dice: 1.9454  d2.loss_cls: 0.6469  d2.loss_mask: 0.1875  d2.loss_dice: 1.8715  d3.loss_cls: 0.5919  d3.loss_mask: 0.1778  d3.loss_dice: 1.8193  d4.loss_cls: 0.5774  d4.loss_mask: 0.1755  d4.loss_dice: 1.7732  d5.loss_cls: 0.5700  d5.loss_mask: 0.1771  d5.loss_dice: 1.7570  d6.loss_cls: 0.5662  d6.loss_mask: 0.1752  d6.loss_dice: 1.7555  d7.loss_cls: 0.5624  d7.loss_mask: 0.1742  d7.loss_dice: 1.7428  d8.loss_cls: 0.5624  d8.loss_mask: 0.1721  d8.loss_dice: 1.7257
05/11 11:14:30 - mmengine - INFO - Iter(train) [11600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:25:34  time: 1.5386  data_time: 0.0317  memory: 38928  grad_norm: 37.7350  loss: 26.7344  loss_cls: 0.5685  loss_mask: 0.1743  loss_dice: 1.7495  d0.loss_cls: 1.0169  d0.loss_mask: 0.2278  d0.loss_dice: 2.0328  d1.loss_cls: 0.8109  d1.loss_mask: 0.2072  d1.loss_dice: 1.9659  d2.loss_cls: 0.6589  d2.loss_mask: 0.1923  d2.loss_dice: 1.8968  d3.loss_cls: 0.5976  d3.loss_mask: 0.1834  d3.loss_dice: 1.8406  d4.loss_cls: 0.5791  d4.loss_mask: 0.1798  d4.loss_dice: 1.7971  d5.loss_cls: 0.5682  d5.loss_mask: 0.1766  d5.loss_dice: 1.7885  d6.loss_cls: 0.5685  d6.loss_mask: 0.1753  d6.loss_dice: 1.7793  d7.loss_cls: 0.5664  d7.loss_mask: 0.1754  d7.loss_dice: 1.7645  d8.loss_cls: 0.5680  d8.loss_mask: 0.1739  d8.loss_dice: 1.7504
05/11 11:15:46 - mmengine - INFO - Iter(train) [11650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:24:12  time: 1.5210  data_time: 0.0319  memory: 38239  grad_norm: 46.8137  loss: 27.0623  loss_cls: 0.5809  loss_mask: 0.1743  loss_dice: 1.7689  d0.loss_cls: 1.0192  d0.loss_mask: 0.2321  d0.loss_dice: 2.0484  d1.loss_cls: 0.8242  d1.loss_mask: 0.2110  d1.loss_dice: 1.9945  d2.loss_cls: 0.6766  d2.loss_mask: 0.1953  d2.loss_dice: 1.9129  d3.loss_cls: 0.6071  d3.loss_mask: 0.1842  d3.loss_dice: 1.8587  d4.loss_cls: 0.5992  d4.loss_mask: 0.1794  d4.loss_dice: 1.8119  d5.loss_cls: 0.5798  d5.loss_mask: 0.1797  d5.loss_dice: 1.8040  d6.loss_cls: 0.5807  d6.loss_mask: 0.1771  d6.loss_dice: 1.8004  d7.loss_cls: 0.5797  d7.loss_mask: 0.1750  d7.loss_dice: 1.7870  d8.loss_cls: 0.5789  d8.loss_mask: 0.1740  d8.loss_dice: 1.7675
05/11 11:17:03 - mmengine - INFO - Iter(train) [11700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:22:52  time: 1.5412  data_time: 0.0383  memory: 38827  grad_norm: 44.3716  loss: 27.4948  loss_cls: 0.5957  loss_mask: 0.1730  loss_dice: 1.7913  d0.loss_cls: 1.0337  d0.loss_mask: 0.2301  d0.loss_dice: 2.0887  d1.loss_cls: 0.8493  d1.loss_mask: 0.2109  d1.loss_dice: 2.0024  d2.loss_cls: 0.6945  d2.loss_mask: 0.1937  d2.loss_dice: 1.9381  d3.loss_cls: 0.6252  d3.loss_mask: 0.1824  d3.loss_dice: 1.8890  d4.loss_cls: 0.6188  d4.loss_mask: 0.1810  d4.loss_dice: 1.8427  d5.loss_cls: 0.6026  d5.loss_mask: 0.1784  d5.loss_dice: 1.8304  d6.loss_cls: 0.5959  d6.loss_mask: 0.1775  d6.loss_dice: 1.8272  d7.loss_cls: 0.5911  d7.loss_mask: 0.1756  d7.loss_dice: 1.8140  d8.loss_cls: 0.5987  d8.loss_mask: 0.1729  d8.loss_dice: 1.7898
05/11 11:18:19 - mmengine - INFO - Iter(train) [11750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:21:30  time: 1.5209  data_time: 0.0317  memory: 38313  grad_norm: 38.2011  loss: 26.9683  loss_cls: 0.5666  loss_mask: 0.1744  loss_dice: 1.7810  d0.loss_cls: 1.0263  d0.loss_mask: 0.2351  d0.loss_dice: 2.0348  d1.loss_cls: 0.8016  d1.loss_mask: 0.2111  d1.loss_dice: 1.9844  d2.loss_cls: 0.6530  d2.loss_mask: 0.1920  d2.loss_dice: 1.9198  d3.loss_cls: 0.5958  d3.loss_mask: 0.1842  d3.loss_dice: 1.8672  d4.loss_cls: 0.5879  d4.loss_mask: 0.1796  d4.loss_dice: 1.8184  d5.loss_cls: 0.5693  d5.loss_mask: 0.1782  d5.loss_dice: 1.8099  d6.loss_cls: 0.5655  d6.loss_mask: 0.1783  d6.loss_dice: 1.8017  d7.loss_cls: 0.5647  d7.loss_mask: 0.1765  d7.loss_dice: 1.7935  d8.loss_cls: 0.5639  d8.loss_mask: 0.1748  d8.loss_dice: 1.7790
05/11 11:19:37 - mmengine - INFO - Iter(train) [11800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:20:10  time: 1.5529  data_time: 0.0318  memory: 39190  grad_norm: 39.1540  loss: 26.5403  loss_cls: 0.5722  loss_mask: 0.1852  loss_dice: 1.7116  d0.loss_cls: 1.0475  d0.loss_mask: 0.2452  d0.loss_dice: 1.9786  d1.loss_cls: 0.8139  d1.loss_mask: 0.2214  d1.loss_dice: 1.9289  d2.loss_cls: 0.6598  d2.loss_mask: 0.2053  d2.loss_dice: 1.8641  d3.loss_cls: 0.6005  d3.loss_mask: 0.1935  d3.loss_dice: 1.8058  d4.loss_cls: 0.5855  d4.loss_mask: 0.1915  d4.loss_dice: 1.7549  d5.loss_cls: 0.5708  d5.loss_mask: 0.1898  d5.loss_dice: 1.7493  d6.loss_cls: 0.5712  d6.loss_mask: 0.1884  d6.loss_dice: 1.7444  d7.loss_cls: 0.5706  d7.loss_mask: 0.1871  d7.loss_dice: 1.7319  d8.loss_cls: 0.5732  d8.loss_mask: 0.1857  d8.loss_dice: 1.7127
05/11 11:20:53 - mmengine - INFO - Iter(train) [11850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:18:49  time: 1.5204  data_time: 0.0310  memory: 38695  grad_norm: 35.9356  loss: 25.1690  loss_cls: 0.5573  loss_mask: 0.1847  loss_dice: 1.6067  d0.loss_cls: 1.0043  d0.loss_mask: 0.2411  d0.loss_dice: 1.8519  d1.loss_cls: 0.7912  d1.loss_mask: 0.2140  d1.loss_dice: 1.7993  d2.loss_cls: 0.6409  d2.loss_mask: 0.1979  d2.loss_dice: 1.7425  d3.loss_cls: 0.5858  d3.loss_mask: 0.1915  d3.loss_dice: 1.6888  d4.loss_cls: 0.5727  d4.loss_mask: 0.1900  d4.loss_dice: 1.6423  d5.loss_cls: 0.5584  d5.loss_mask: 0.1888  d5.loss_dice: 1.6357  d6.loss_cls: 0.5518  d6.loss_mask: 0.1887  d6.loss_dice: 1.6331  d7.loss_cls: 0.5542  d7.loss_mask: 0.1865  d7.loss_dice: 1.6163  d8.loss_cls: 0.5560  d8.loss_mask: 0.1838  d8.loss_dice: 1.6128
05/11 11:22:09 - mmengine - INFO - Iter(train) [11900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:17:28  time: 1.5257  data_time: 0.0357  memory: 38312  grad_norm: 39.8993  loss: 26.4352  loss_cls: 0.5468  loss_mask: 0.1748  loss_dice: 1.7345  d0.loss_cls: 1.0207  d0.loss_mask: 0.2326  d0.loss_dice: 1.9987  d1.loss_cls: 0.7926  d1.loss_mask: 0.2070  d1.loss_dice: 1.9528  d2.loss_cls: 0.6358  d2.loss_mask: 0.1899  d2.loss_dice: 1.8898  d3.loss_cls: 0.5754  d3.loss_mask: 0.1826  d3.loss_dice: 1.8304  d4.loss_cls: 0.5745  d4.loss_mask: 0.1789  d4.loss_dice: 1.7730  d5.loss_cls: 0.5560  d5.loss_mask: 0.1782  d5.loss_dice: 1.7763  d6.loss_cls: 0.5498  d6.loss_mask: 0.1778  d6.loss_dice: 1.7703  d7.loss_cls: 0.5445  d7.loss_mask: 0.1758  d7.loss_dice: 1.7573  d8.loss_cls: 0.5516  d8.loss_mask: 0.1750  d8.loss_dice: 1.7317
05/11 11:23:26 - mmengine - INFO - Iter(train) [11950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:16:07  time: 1.5416  data_time: 0.0308  memory: 38610  grad_norm: 37.7145  loss: 26.5422  loss_cls: 0.5819  loss_mask: 0.1715  loss_dice: 1.7309  d0.loss_cls: 1.0210  d0.loss_mask: 0.2244  d0.loss_dice: 1.9728  d1.loss_cls: 0.8181  d1.loss_mask: 0.2087  d1.loss_dice: 1.9303  d2.loss_cls: 0.6713  d2.loss_mask: 0.1879  d2.loss_dice: 1.8653  d3.loss_cls: 0.6092  d3.loss_mask: 0.1788  d3.loss_dice: 1.8120  d4.loss_cls: 0.6074  d4.loss_mask: 0.1759  d4.loss_dice: 1.7616  d5.loss_cls: 0.5869  d5.loss_mask: 0.1755  d5.loss_dice: 1.7583  d6.loss_cls: 0.5834  d6.loss_mask: 0.1743  d6.loss_dice: 1.7574  d7.loss_cls: 0.5779  d7.loss_mask: 0.1719  d7.loss_dice: 1.7433  d8.loss_cls: 0.5782  d8.loss_mask: 0.1718  d8.loss_dice: 1.7345
05/11 11:24:43 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 11:24:43 - mmengine - INFO - Iter(train) [12000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:14:46  time: 1.5279  data_time: 0.0307  memory: 38664  grad_norm: 34.5740  loss: 27.6293  loss_cls: 0.5834  loss_mask: 0.1761  loss_dice: 1.8221  d0.loss_cls: 1.0400  d0.loss_mask: 0.2351  d0.loss_dice: 2.0907  d1.loss_cls: 0.8286  d1.loss_mask: 0.2124  d1.loss_dice: 2.0432  d2.loss_cls: 0.6737  d2.loss_mask: 0.1935  d2.loss_dice: 1.9673  d3.loss_cls: 0.6040  d3.loss_mask: 0.1869  d3.loss_dice: 1.9164  d4.loss_cls: 0.5908  d4.loss_mask: 0.1816  d4.loss_dice: 1.8712  d5.loss_cls: 0.5797  d5.loss_mask: 0.1801  d5.loss_dice: 1.8577  d6.loss_cls: 0.5777  d6.loss_mask: 0.1799  d6.loss_dice: 1.8556  d7.loss_cls: 0.5773  d7.loss_mask: 0.1783  d7.loss_dice: 1.8407  d8.loss_cls: 0.5809  d8.loss_mask: 0.1767  d8.loss_dice: 1.8277
05/11 11:24:43 - mmengine - INFO - Saving checkpoint at 12000 iterations
05/11 11:25:26 - mmengine - INFO - Iter(val) [ 50/100]    eta: 0:00:37  time: 0.7403  data_time: 0.0147  memory: 5704  
05/11 11:26:03 - mmengine - INFO - Iter(val) [100/100]    eta: 0:00:00  time: 0.7350  data_time: 0.0135  memory: 5704  
05/11 11:26:27 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.36s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/11 11:26:34 - mmengine - INFO - start multi processing evaluation ...

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

  0%|          | 0/25552 [00:00<?, ?it/s]
 46%|████▌     | 11628/25552 [00:00<00:00, 116274.84it/s]
100%|██████████| 25552/25552 [00:00<00:00, 130034.32it/s]
DONE (t=52.58s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.383
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.720
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.349
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.230
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.440
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.898
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.517
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.859
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.500
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.380
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.582
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.917
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.517
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.517
05/11 11:27:26 - mmengine - INFO - segm_mAP_copypaste: 0.383 0.720 0.349 0.230 0.440 0.898
05/11 11:27:26 - mmengine - INFO - segm_mAR_copypaste: 0.517 0.859 0.500 0.380 0.582 0.917
05/11 11:27:27 - mmengine - INFO - Iter(val) [100/100]    coco/segm_mAP: 0.3830  coco/segm_mAP_50: 0.7200  coco/segm_mAP_75: 0.3490  coco/segm_mAP_s: 0.2300  coco/segm_mAP_m: 0.4400  coco/segm_mAP_l: 0.8980  data_time: 0.0141  time: 0.7362
05/11 11:28:44 - mmengine - INFO - Iter(train) [12050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:14:49  time: 3.2221  data_time: 1.7243  memory: 39144  grad_norm: 35.1685  loss: 27.3783  loss_cls: 0.5717  loss_mask: 0.1824  loss_dice: 1.8034  d0.loss_cls: 1.0467  d0.loss_mask: 0.2445  d0.loss_dice: 2.0682  d1.loss_cls: 0.8112  d1.loss_mask: 0.2198  d1.loss_dice: 2.0143  d2.loss_cls: 0.6587  d2.loss_mask: 0.1992  d2.loss_dice: 1.9460  d3.loss_cls: 0.5953  d3.loss_mask: 0.1924  d3.loss_dice: 1.8982  d4.loss_cls: 0.5752  d4.loss_mask: 0.1887  d4.loss_dice: 1.8551  d5.loss_cls: 0.5664  d5.loss_mask: 0.1868  d5.loss_dice: 1.8406  d6.loss_cls: 0.5638  d6.loss_mask: 0.1861  d6.loss_dice: 1.8307  d7.loss_cls: 0.5668  d7.loss_mask: 0.1848  d7.loss_dice: 1.8201  d8.loss_cls: 0.5715  d8.loss_mask: 0.1831  d8.loss_dice: 1.8067
05/11 11:30:01 - mmengine - INFO - Iter(train) [12100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:13:28  time: 1.5413  data_time: 0.0319  memory: 39534  grad_norm: 42.3143  loss: 27.0285  loss_cls: 0.5789  loss_mask: 0.1723  loss_dice: 1.7683  d0.loss_cls: 1.0571  d0.loss_mask: 0.2298  d0.loss_dice: 2.0509  d1.loss_cls: 0.8187  d1.loss_mask: 0.2108  d1.loss_dice: 1.9897  d2.loss_cls: 0.6616  d2.loss_mask: 0.1906  d2.loss_dice: 1.9203  d3.loss_cls: 0.6027  d3.loss_mask: 0.1819  d3.loss_dice: 1.8649  d4.loss_cls: 0.5827  d4.loss_mask: 0.1773  d4.loss_dice: 1.8109  d5.loss_cls: 0.5744  d5.loss_mask: 0.1762  d5.loss_dice: 1.8044  d6.loss_cls: 0.5810  d6.loss_mask: 0.1741  d6.loss_dice: 1.7884  d7.loss_cls: 0.5796  d7.loss_mask: 0.1721  d7.loss_dice: 1.7855  d8.loss_cls: 0.5805  d8.loss_mask: 0.1716  d8.loss_dice: 1.7713
05/11 11:31:17 - mmengine - INFO - Iter(train) [12150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:12:07  time: 1.5305  data_time: 0.0313  memory: 38859  grad_norm: 39.6600  loss: 27.1354  loss_cls: 0.5783  loss_mask: 0.1695  loss_dice: 1.7874  d0.loss_cls: 1.0546  d0.loss_mask: 0.2226  d0.loss_dice: 2.0627  d1.loss_cls: 0.8280  d1.loss_mask: 0.2013  d1.loss_dice: 2.0042  d2.loss_cls: 0.6635  d2.loss_mask: 0.1860  d2.loss_dice: 1.9296  d3.loss_cls: 0.6021  d3.loss_mask: 0.1758  d3.loss_dice: 1.8723  d4.loss_cls: 0.5855  d4.loss_mask: 0.1742  d4.loss_dice: 1.8276  d5.loss_cls: 0.5784  d5.loss_mask: 0.1730  d5.loss_dice: 1.8168  d6.loss_cls: 0.5788  d6.loss_mask: 0.1713  d6.loss_dice: 1.8125  d7.loss_cls: 0.5785  d7.loss_mask: 0.1702  d7.loss_dice: 1.7988  d8.loss_cls: 0.5831  d8.loss_mask: 0.1677  d8.loss_dice: 1.7812
05/11 11:32:35 - mmengine - INFO - Iter(train) [12200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:10:46  time: 1.5470  data_time: 0.0323  memory: 39545  grad_norm: 33.6156  loss: 29.2915  loss_cls: 0.5892  loss_mask: 0.1757  loss_dice: 1.9784  d0.loss_cls: 1.0679  d0.loss_mask: 0.2296  d0.loss_dice: 2.2606  d1.loss_cls: 0.8416  d1.loss_mask: 0.2140  d1.loss_dice: 2.2172  d2.loss_cls: 0.6773  d2.loss_mask: 0.1971  d2.loss_dice: 2.1409  d3.loss_cls: 0.6086  d3.loss_mask: 0.1869  d3.loss_dice: 2.0748  d4.loss_cls: 0.5935  d4.loss_mask: 0.1808  d4.loss_dice: 2.0220  d5.loss_cls: 0.5872  d5.loss_mask: 0.1793  d5.loss_dice: 2.0075  d6.loss_cls: 0.5882  d6.loss_mask: 0.1781  d6.loss_dice: 2.0068  d7.loss_cls: 0.5891  d7.loss_mask: 0.1771  d7.loss_dice: 1.9855  d8.loss_cls: 0.5919  d8.loss_mask: 0.1752  d8.loss_dice: 1.9695
05/11 11:33:51 - mmengine - INFO - Iter(train) [12250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:09:25  time: 1.5284  data_time: 0.0321  memory: 38739  grad_norm: 33.7732  loss: 27.7999  loss_cls: 0.5825  loss_mask: 0.1620  loss_dice: 1.8572  d0.loss_cls: 1.0457  d0.loss_mask: 0.2091  d0.loss_dice: 2.1153  d1.loss_cls: 0.8255  d1.loss_mask: 0.1960  d1.loss_dice: 2.0760  d2.loss_cls: 0.6633  d2.loss_mask: 0.1809  d2.loss_dice: 2.0032  d3.loss_cls: 0.6008  d3.loss_mask: 0.1727  d3.loss_dice: 1.9448  d4.loss_cls: 0.5890  d4.loss_mask: 0.1685  d4.loss_dice: 1.9015  d5.loss_cls: 0.5788  d5.loss_mask: 0.1683  d5.loss_dice: 1.8931  d6.loss_cls: 0.5843  d6.loss_mask: 0.1673  d6.loss_dice: 1.8857  d7.loss_cls: 0.5803  d7.loss_mask: 0.1648  d7.loss_dice: 1.8755  d8.loss_cls: 0.5869  d8.loss_mask: 0.1634  d8.loss_dice: 1.8572
05/11 11:35:08 - mmengine - INFO - Iter(train) [12300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:08:04  time: 1.5351  data_time: 0.0319  memory: 38262  grad_norm: 37.9243  loss: 26.2601  loss_cls: 0.5580  loss_mask: 0.1744  loss_dice: 1.7129  d0.loss_cls: 1.0316  d0.loss_mask: 0.2338  d0.loss_dice: 1.9871  d1.loss_cls: 0.8260  d1.loss_mask: 0.2128  d1.loss_dice: 1.9317  d2.loss_cls: 0.6521  d2.loss_mask: 0.1916  d2.loss_dice: 1.8499  d3.loss_cls: 0.5821  d3.loss_mask: 0.1818  d3.loss_dice: 1.7964  d4.loss_cls: 0.5670  d4.loss_mask: 0.1775  d4.loss_dice: 1.7476  d5.loss_cls: 0.5555  d5.loss_mask: 0.1781  d5.loss_dice: 1.7402  d6.loss_cls: 0.5563  d6.loss_mask: 0.1764  d6.loss_dice: 1.7385  d7.loss_cls: 0.5546  d7.loss_mask: 0.1756  d7.loss_dice: 1.7280  d8.loss_cls: 0.5594  d8.loss_mask: 0.1745  d8.loss_dice: 1.7087
05/11 11:36:24 - mmengine - INFO - Iter(train) [12350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:06:42  time: 1.5297  data_time: 0.0323  memory: 38177  grad_norm: 40.2710  loss: 27.5932  loss_cls: 0.5833  loss_mask: 0.1631  loss_dice: 1.8515  d0.loss_cls: 1.0428  d0.loss_mask: 0.2114  d0.loss_dice: 2.0738  d1.loss_cls: 0.8258  d1.loss_mask: 0.1960  d1.loss_dice: 2.0335  d2.loss_cls: 0.6706  d2.loss_mask: 0.1795  d2.loss_dice: 1.9811  d3.loss_cls: 0.6101  d3.loss_mask: 0.1712  d3.loss_dice: 1.9241  d4.loss_cls: 0.5932  d4.loss_mask: 0.1657  d4.loss_dice: 1.8881  d5.loss_cls: 0.5819  d5.loss_mask: 0.1659  d5.loss_dice: 1.8761  d6.loss_cls: 0.5739  d6.loss_mask: 0.1664  d6.loss_dice: 1.8713  d7.loss_cls: 0.5803  d7.loss_mask: 0.1637  d7.loss_dice: 1.8569  d8.loss_cls: 0.5847  d8.loss_mask: 0.1638  d8.loss_dice: 1.8435
05/11 11:37:42 - mmengine - INFO - Iter(train) [12400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:05:22  time: 1.5457  data_time: 0.0324  memory: 39564  grad_norm: 34.4128  loss: 29.2616  loss_cls: 0.5898  loss_mask: 0.1727  loss_dice: 1.9704  d0.loss_cls: 1.0950  d0.loss_mask: 0.2277  d0.loss_dice: 2.2626  d1.loss_cls: 0.8391  d1.loss_mask: 0.2157  d1.loss_dice: 2.2180  d2.loss_cls: 0.6737  d2.loss_mask: 0.1921  d2.loss_dice: 2.1368  d3.loss_cls: 0.6127  d3.loss_mask: 0.1836  d3.loss_dice: 2.0763  d4.loss_cls: 0.5919  d4.loss_mask: 0.1781  d4.loss_dice: 2.0192  d5.loss_cls: 0.5804  d5.loss_mask: 0.1779  d5.loss_dice: 2.0071  d6.loss_cls: 0.5805  d6.loss_mask: 0.1757  d6.loss_dice: 2.0002  d7.loss_cls: 0.5854  d7.loss_mask: 0.1748  d7.loss_dice: 1.9924  d8.loss_cls: 0.5887  d8.loss_mask: 0.1734  d8.loss_dice: 1.9697
05/11 11:38:59 - mmengine - INFO - Iter(train) [12450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:04:01  time: 1.5463  data_time: 0.0366  memory: 38879  grad_norm: 35.8428  loss: 26.7894  loss_cls: 0.5717  loss_mask: 0.1657  loss_dice: 1.7620  d0.loss_cls: 1.0501  d0.loss_mask: 0.2175  d0.loss_dice: 2.0410  d1.loss_cls: 0.8140  d1.loss_mask: 0.2023  d1.loss_dice: 2.0004  d2.loss_cls: 0.6555  d2.loss_mask: 0.1821  d2.loss_dice: 1.9147  d3.loss_cls: 0.5899  d3.loss_mask: 0.1740  d3.loss_dice: 1.8589  d4.loss_cls: 0.5697  d4.loss_mask: 0.1718  d4.loss_dice: 1.8021  d5.loss_cls: 0.5598  d5.loss_mask: 0.1692  d5.loss_dice: 1.7959  d6.loss_cls: 0.5621  d6.loss_mask: 0.1694  d6.loss_dice: 1.7918  d7.loss_cls: 0.5628  d7.loss_mask: 0.1671  d7.loss_dice: 1.7750  d8.loss_cls: 0.5710  d8.loss_mask: 0.1664  d8.loss_dice: 1.7556
05/11 11:40:16 - mmengine - INFO - Iter(train) [12500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:02:40  time: 1.5403  data_time: 0.0323  memory: 38792  grad_norm: 40.4704  loss: 28.3492  loss_cls: 0.5851  loss_mask: 0.1687  loss_dice: 1.8947  d0.loss_cls: 1.0748  d0.loss_mask: 0.2179  d0.loss_dice: 2.1545  d1.loss_cls: 0.8461  d1.loss_mask: 0.2021  d1.loss_dice: 2.1339  d2.loss_cls: 0.6745  d2.loss_mask: 0.1846  d2.loss_dice: 2.0545  d3.loss_cls: 0.6174  d3.loss_mask: 0.1768  d3.loss_dice: 1.9866  d4.loss_cls: 0.5899  d4.loss_mask: 0.1713  d4.loss_dice: 1.9444  d5.loss_cls: 0.5809  d5.loss_mask: 0.1717  d5.loss_dice: 1.9268  d6.loss_cls: 0.5832  d6.loss_mask: 0.1710  d6.loss_dice: 1.9273  d7.loss_cls: 0.5819  d7.loss_mask: 0.1691  d7.loss_dice: 1.9135  d8.loss_cls: 0.5837  d8.loss_mask: 0.1685  d8.loss_dice: 1.8934
05/11 11:41:33 - mmengine - INFO - Iter(train) [12550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:01:19  time: 1.5343  data_time: 0.0319  memory: 39023  grad_norm: 36.2232  loss: 27.6274  loss_cls: 0.5919  loss_mask: 0.1778  loss_dice: 1.8149  d0.loss_cls: 1.0637  d0.loss_mask: 0.2264  d0.loss_dice: 2.0866  d1.loss_cls: 0.8374  d1.loss_mask: 0.2141  d1.loss_dice: 2.0374  d2.loss_cls: 0.6730  d2.loss_mask: 0.1951  d2.loss_dice: 1.9708  d3.loss_cls: 0.6086  d3.loss_mask: 0.1894  d3.loss_dice: 1.9095  d4.loss_cls: 0.5930  d4.loss_mask: 0.1835  d4.loss_dice: 1.8553  d5.loss_cls: 0.5865  d5.loss_mask: 0.1822  d5.loss_dice: 1.8494  d6.loss_cls: 0.5850  d6.loss_mask: 0.1814  d6.loss_dice: 1.8454  d7.loss_cls: 0.5838  d7.loss_mask: 0.1797  d7.loss_dice: 1.8240  d8.loss_cls: 0.5910  d8.loss_mask: 0.1787  d8.loss_dice: 1.8120
05/11 11:42:50 - mmengine - INFO - Iter(train) [12600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:59:59  time: 1.5466  data_time: 0.0381  memory: 38540  grad_norm: 37.6729  loss: 27.4860  loss_cls: 0.6035  loss_mask: 0.1758  loss_dice: 1.7958  d0.loss_cls: 1.0615  d0.loss_mask: 0.2262  d0.loss_dice: 2.0342  d1.loss_cls: 0.8485  d1.loss_mask: 0.2135  d1.loss_dice: 2.0051  d2.loss_cls: 0.6789  d2.loss_mask: 0.1926  d2.loss_dice: 1.9412  d3.loss_cls: 0.6264  d3.loss_mask: 0.1838  d3.loss_dice: 1.8896  d4.loss_cls: 0.6059  d4.loss_mask: 0.1801  d4.loss_dice: 1.8444  d5.loss_cls: 0.6012  d5.loss_mask: 0.1800  d5.loss_dice: 1.8323  d6.loss_cls: 0.5993  d6.loss_mask: 0.1792  d6.loss_dice: 1.8243  d7.loss_cls: 0.5973  d7.loss_mask: 0.1781  d7.loss_dice: 1.8131  d8.loss_cls: 0.6040  d8.loss_mask: 0.1760  d8.loss_dice: 1.7938
05/11 11:44:07 - mmengine - INFO - Iter(train) [12650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:58:38  time: 1.5298  data_time: 0.0318  memory: 38581  grad_norm: 41.5481  loss: 27.2701  loss_cls: 0.5731  loss_mask: 0.1693  loss_dice: 1.7987  d0.loss_cls: 1.0732  d0.loss_mask: 0.2232  d0.loss_dice: 2.0655  d1.loss_cls: 0.8240  d1.loss_mask: 0.2061  d1.loss_dice: 2.0324  d2.loss_cls: 0.6583  d2.loss_mask: 0.1861  d2.loss_dice: 1.9504  d3.loss_cls: 0.5968  d3.loss_mask: 0.1804  d3.loss_dice: 1.8898  d4.loss_cls: 0.5771  d4.loss_mask: 0.1744  d4.loss_dice: 1.8391  d5.loss_cls: 0.5708  d5.loss_mask: 0.1739  d5.loss_dice: 1.8356  d6.loss_cls: 0.5738  d6.loss_mask: 0.1728  d6.loss_dice: 1.8298  d7.loss_cls: 0.5717  d7.loss_mask: 0.1717  d7.loss_dice: 1.8130  d8.loss_cls: 0.5754  d8.loss_mask: 0.1701  d8.loss_dice: 1.7937
05/11 11:45:23 - mmengine - INFO - Iter(train) [12700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:57:16  time: 1.5223  data_time: 0.0314  memory: 38324  grad_norm: 36.8231  loss: 25.8752  loss_cls: 0.5894  loss_mask: 0.1734  loss_dice: 1.6594  d0.loss_cls: 1.0297  d0.loss_mask: 0.2279  d0.loss_dice: 1.8786  d1.loss_cls: 0.8154  d1.loss_mask: 0.2113  d1.loss_dice: 1.8600  d2.loss_cls: 0.6583  d2.loss_mask: 0.1884  d2.loss_dice: 1.8017  d3.loss_cls: 0.6137  d3.loss_mask: 0.1819  d3.loss_dice: 1.7358  d4.loss_cls: 0.5967  d4.loss_mask: 0.1775  d4.loss_dice: 1.6958  d5.loss_cls: 0.5863  d5.loss_mask: 0.1766  d5.loss_dice: 1.6946  d6.loss_cls: 0.5888  d6.loss_mask: 0.1755  d6.loss_dice: 1.6922  d7.loss_cls: 0.5834  d7.loss_mask: 0.1735  d7.loss_dice: 1.6790  d8.loss_cls: 0.5931  d8.loss_mask: 0.1729  d8.loss_dice: 1.6643
05/11 11:46:39 - mmengine - INFO - Iter(train) [12750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:55:56  time: 1.5346  data_time: 0.0315  memory: 38362  grad_norm: 36.3896  loss: 26.1639  loss_cls: 0.5711  loss_mask: 0.1654  loss_dice: 1.6984  d0.loss_cls: 1.0565  d0.loss_mask: 0.2143  d0.loss_dice: 1.9584  d1.loss_cls: 0.8238  d1.loss_mask: 0.2010  d1.loss_dice: 1.9177  d2.loss_cls: 0.6572  d2.loss_mask: 0.1820  d2.loss_dice: 1.8571  d3.loss_cls: 0.5937  d3.loss_mask: 0.1751  d3.loss_dice: 1.7912  d4.loss_cls: 0.5720  d4.loss_mask: 0.1711  d4.loss_dice: 1.7387  d5.loss_cls: 0.5643  d5.loss_mask: 0.1696  d5.loss_dice: 1.7325  d6.loss_cls: 0.5650  d6.loss_mask: 0.1692  d6.loss_dice: 1.7249  d7.loss_cls: 0.5702  d7.loss_mask: 0.1674  d7.loss_dice: 1.7188  d8.loss_cls: 0.5671  d8.loss_mask: 0.1653  d8.loss_dice: 1.7047
05/11 11:47:56 - mmengine - INFO - Iter(train) [12800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:54:35  time: 1.5366  data_time: 0.0317  memory: 38700  grad_norm: 35.0699  loss: 26.2684  loss_cls: 0.5645  loss_mask: 0.1815  loss_dice: 1.7039  d0.loss_cls: 1.0668  d0.loss_mask: 0.2346  d0.loss_dice: 1.9388  d1.loss_cls: 0.8202  d1.loss_mask: 0.2166  d1.loss_dice: 1.9105  d2.loss_cls: 0.6416  d2.loss_mask: 0.1973  d2.loss_dice: 1.8456  d3.loss_cls: 0.5865  d3.loss_mask: 0.1911  d3.loss_dice: 1.7904  d4.loss_cls: 0.5666  d4.loss_mask: 0.1871  d4.loss_dice: 1.7475  d5.loss_cls: 0.5564  d5.loss_mask: 0.1858  d5.loss_dice: 1.7378  d6.loss_cls: 0.5593  d6.loss_mask: 0.1854  d6.loss_dice: 1.7366  d7.loss_cls: 0.5603  d7.loss_mask: 0.1842  d7.loss_dice: 1.7189  d8.loss_cls: 0.5612  d8.loss_mask: 0.1828  d8.loss_dice: 1.7084
05/11 11:49:13 - mmengine - INFO - Iter(train) [12850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:53:14  time: 1.5414  data_time: 0.0317  memory: 38755  grad_norm: 40.7464  loss: 26.1748  loss_cls: 0.5660  loss_mask: 0.1655  loss_dice: 1.7039  d0.loss_cls: 1.0589  d0.loss_mask: 0.2158  d0.loss_dice: 1.9517  d1.loss_cls: 0.8167  d1.loss_mask: 0.2045  d1.loss_dice: 1.9352  d2.loss_cls: 0.6491  d2.loss_mask: 0.1865  d2.loss_dice: 1.8562  d3.loss_cls: 0.5882  d3.loss_mask: 0.1733  d3.loss_dice: 1.7975  d4.loss_cls: 0.5685  d4.loss_mask: 0.1699  d4.loss_dice: 1.7515  d5.loss_cls: 0.5621  d5.loss_mask: 0.1685  d5.loss_dice: 1.7388  d6.loss_cls: 0.5608  d6.loss_mask: 0.1682  d6.loss_dice: 1.7304  d7.loss_cls: 0.5646  d7.loss_mask: 0.1668  d7.loss_dice: 1.7199  d8.loss_cls: 0.5659  d8.loss_mask: 0.1667  d8.loss_dice: 1.7030
05/11 11:50:29 - mmengine - INFO - Iter(train) [12900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:51:53  time: 1.5219  data_time: 0.0315  memory: 38191  grad_norm: 38.4499  loss: 25.2806  loss_cls: 0.5382  loss_mask: 0.1636  loss_dice: 1.6609  d0.loss_cls: 1.0330  d0.loss_mask: 0.2115  d0.loss_dice: 1.8721  d1.loss_cls: 0.7936  d1.loss_mask: 0.1951  d1.loss_dice: 1.8500  d2.loss_cls: 0.6202  d2.loss_mask: 0.1768  d2.loss_dice: 1.7876  d3.loss_cls: 0.5668  d3.loss_mask: 0.1721  d3.loss_dice: 1.7259  d4.loss_cls: 0.5482  d4.loss_mask: 0.1672  d4.loss_dice: 1.6916  d5.loss_cls: 0.5392  d5.loss_mask: 0.1662  d5.loss_dice: 1.6869  d6.loss_cls: 0.5378  d6.loss_mask: 0.1661  d6.loss_dice: 1.6773  d7.loss_cls: 0.5394  d7.loss_mask: 0.1650  d7.loss_dice: 1.6699  d8.loss_cls: 0.5398  d8.loss_mask: 0.1639  d8.loss_dice: 1.6548
05/11 11:51:46 - mmengine - INFO - Iter(train) [12950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:50:32  time: 1.5281  data_time: 0.0314  memory: 38841  grad_norm: 43.6062  loss: 27.7837  loss_cls: 0.5950  loss_mask: 0.1829  loss_dice: 1.8300  d0.loss_cls: 1.0588  d0.loss_mask: 0.2387  d0.loss_dice: 2.0704  d1.loss_cls: 0.8396  d1.loss_mask: 0.2194  d1.loss_dice: 2.0337  d2.loss_cls: 0.6706  d2.loss_mask: 0.2009  d2.loss_dice: 1.9734  d3.loss_cls: 0.6134  d3.loss_mask: 0.1909  d3.loss_dice: 1.9160  d4.loss_cls: 0.5948  d4.loss_mask: 0.1878  d4.loss_dice: 1.8748  d5.loss_cls: 0.5848  d5.loss_mask: 0.1876  d5.loss_dice: 1.8609  d6.loss_cls: 0.5874  d6.loss_mask: 0.1864  d6.loss_dice: 1.8533  d7.loss_cls: 0.5916  d7.loss_mask: 0.1852  d7.loss_dice: 1.8475  d8.loss_cls: 0.5954  d8.loss_mask: 0.1832  d8.loss_dice: 1.8292
05/11 11:53:02 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 11:53:02 - mmengine - INFO - Iter(train) [13000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:49:11  time: 1.5204  data_time: 0.0378  memory: 38253  grad_norm: 42.7330  loss: 27.2792  loss_cls: 0.5675  loss_mask: 0.1771  loss_dice: 1.8026  d0.loss_cls: 1.0571  d0.loss_mask: 0.2286  d0.loss_dice: 2.0480  d1.loss_cls: 0.8235  d1.loss_mask: 0.2102  d1.loss_dice: 2.0193  d2.loss_cls: 0.6557  d2.loss_mask: 0.1959  d2.loss_dice: 1.9571  d3.loss_cls: 0.6010  d3.loss_mask: 0.1883  d3.loss_dice: 1.8907  d4.loss_cls: 0.5783  d4.loss_mask: 0.1813  d4.loss_dice: 1.8407  d5.loss_cls: 0.5674  d5.loss_mask: 0.1809  d5.loss_dice: 1.8280  d6.loss_cls: 0.5643  d6.loss_mask: 0.1806  d6.loss_dice: 1.8327  d7.loss_cls: 0.5638  d7.loss_mask: 0.1783  d7.loss_dice: 1.8140  d8.loss_cls: 0.5680  d8.loss_mask: 0.1776  d8.loss_dice: 1.8005
05/11 11:54:19 - mmengine - INFO - Iter(train) [13050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:47:50  time: 1.5323  data_time: 0.0318  memory: 38872  grad_norm: 53.6078  loss: 28.5878  loss_cls: 0.5885  loss_mask: 0.1879  loss_dice: 1.9019  d0.loss_cls: 1.0865  d0.loss_mask: 0.2384  d0.loss_dice: 2.1455  d1.loss_cls: 0.8223  d1.loss_mask: 0.2229  d1.loss_dice: 2.1266  d2.loss_cls: 0.6741  d2.loss_mask: 0.2051  d2.loss_dice: 2.0596  d3.loss_cls: 0.6199  d3.loss_mask: 0.1967  d3.loss_dice: 1.9876  d4.loss_cls: 0.5935  d4.loss_mask: 0.1934  d4.loss_dice: 1.9467  d5.loss_cls: 0.5883  d5.loss_mask: 0.1922  d5.loss_dice: 1.9320  d6.loss_cls: 0.5873  d6.loss_mask: 0.1918  d6.loss_dice: 1.9321  d7.loss_cls: 0.5887  d7.loss_mask: 0.1893  d7.loss_dice: 1.9117  d8.loss_cls: 0.5885  d8.loss_mask: 0.1886  d8.loss_dice: 1.9001
05/11 11:55:36 - mmengine - INFO - Iter(train) [13100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:46:30  time: 1.5481  data_time: 0.0317  memory: 38900  grad_norm: 36.1252  loss: 27.4227  loss_cls: 0.5920  loss_mask: 0.1774  loss_dice: 1.7909  d0.loss_cls: 1.0613  d0.loss_mask: 0.2336  d0.loss_dice: 2.0490  d1.loss_cls: 0.8448  d1.loss_mask: 0.2153  d1.loss_dice: 2.0101  d2.loss_cls: 0.6849  d2.loss_mask: 0.1938  d2.loss_dice: 1.9403  d3.loss_cls: 0.6232  d3.loss_mask: 0.1869  d3.loss_dice: 1.8783  d4.loss_cls: 0.5978  d4.loss_mask: 0.1830  d4.loss_dice: 1.8355  d5.loss_cls: 0.5895  d5.loss_mask: 0.1820  d5.loss_dice: 1.8282  d6.loss_cls: 0.5872  d6.loss_mask: 0.1805  d6.loss_dice: 1.8209  d7.loss_cls: 0.5929  d7.loss_mask: 0.1779  d7.loss_dice: 1.8025  d8.loss_cls: 0.5952  d8.loss_mask: 0.1769  d8.loss_dice: 1.7906
05/11 11:56:53 - mmengine - INFO - Iter(train) [13150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:45:09  time: 1.5332  data_time: 0.0318  memory: 38810  grad_norm: 38.6509  loss: 26.7260  loss_cls: 0.5685  loss_mask: 0.1726  loss_dice: 1.7404  d0.loss_cls: 1.0721  d0.loss_mask: 0.2256  d0.loss_dice: 2.0060  d1.loss_cls: 0.8459  d1.loss_mask: 0.2112  d1.loss_dice: 1.9728  d2.loss_cls: 0.6713  d2.loss_mask: 0.1900  d2.loss_dice: 1.9001  d3.loss_cls: 0.5990  d3.loss_mask: 0.1820  d3.loss_dice: 1.8325  d4.loss_cls: 0.5795  d4.loss_mask: 0.1784  d4.loss_dice: 1.7858  d5.loss_cls: 0.5618  d5.loss_mask: 0.1776  d5.loss_dice: 1.7740  d6.loss_cls: 0.5650  d6.loss_mask: 0.1757  d6.loss_dice: 1.7678  d7.loss_cls: 0.5640  d7.loss_mask: 0.1740  d7.loss_dice: 1.7522  d8.loss_cls: 0.5671  d8.loss_mask: 0.1726  d8.loss_dice: 1.7405
05/11 11:58:09 - mmengine - INFO - Iter(train) [13200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:43:49  time: 1.5378  data_time: 0.0318  memory: 38697  grad_norm: 38.0490  loss: 26.6709  loss_cls: 0.5763  loss_mask: 0.1657  loss_dice: 1.7341  d0.loss_cls: 1.0694  d0.loss_mask: 0.2186  d0.loss_dice: 2.0095  d1.loss_cls: 0.8348  d1.loss_mask: 0.2053  d1.loss_dice: 1.9803  d2.loss_cls: 0.6684  d2.loss_mask: 0.1849  d2.loss_dice: 1.9034  d3.loss_cls: 0.6000  d3.loss_mask: 0.1761  d3.loss_dice: 1.8359  d4.loss_cls: 0.5797  d4.loss_mask: 0.1716  d4.loss_dice: 1.7764  d5.loss_cls: 0.5688  d5.loss_mask: 0.1711  d5.loss_dice: 1.7688  d6.loss_cls: 0.5731  d6.loss_mask: 0.1703  d6.loss_dice: 1.7617  d7.loss_cls: 0.5746  d7.loss_mask: 0.1673  d7.loss_dice: 1.7498  d8.loss_cls: 0.5730  d8.loss_mask: 0.1657  d8.loss_dice: 1.7360
05/11 11:59:26 - mmengine - INFO - Iter(train) [13250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:42:28  time: 1.5288  data_time: 0.0318  memory: 38603  grad_norm: 63.3964  loss: 27.6648  loss_cls: 0.5832  loss_mask: 0.1790  loss_dice: 1.8172  d0.loss_cls: 1.0591  d0.loss_mask: 0.2394  d0.loss_dice: 2.1017  d1.loss_cls: 0.8388  d1.loss_mask: 0.2177  d1.loss_dice: 2.0599  d2.loss_cls: 0.6733  d2.loss_mask: 0.1993  d2.loss_dice: 1.9781  d3.loss_cls: 0.6084  d3.loss_mask: 0.1895  d3.loss_dice: 1.9107  d4.loss_cls: 0.5843  d4.loss_mask: 0.1855  d4.loss_dice: 1.8575  d5.loss_cls: 0.5765  d5.loss_mask: 0.1837  d5.loss_dice: 1.8483  d6.loss_cls: 0.5802  d6.loss_mask: 0.1832  d6.loss_dice: 1.8419  d7.loss_cls: 0.5814  d7.loss_mask: 0.1803  d7.loss_dice: 1.8294  d8.loss_cls: 0.5821  d8.loss_mask: 0.1795  d8.loss_dice: 1.8159
05/11 12:00:42 - mmengine - INFO - Iter(train) [13300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:41:07  time: 1.5306  data_time: 0.0317  memory: 38288  grad_norm: 52.1964  loss: 26.0658  loss_cls: 0.5704  loss_mask: 0.1746  loss_dice: 1.6739  d0.loss_cls: 1.0529  d0.loss_mask: 0.2300  d0.loss_dice: 1.9309  d1.loss_cls: 0.8411  d1.loss_mask: 0.2124  d1.loss_dice: 1.9001  d2.loss_cls: 0.6712  d2.loss_mask: 0.1956  d2.loss_dice: 1.8216  d3.loss_cls: 0.6030  d3.loss_mask: 0.1855  d3.loss_dice: 1.7649  d4.loss_cls: 0.5815  d4.loss_mask: 0.1791  d4.loss_dice: 1.7110  d5.loss_cls: 0.5697  d5.loss_mask: 0.1781  d5.loss_dice: 1.7068  d6.loss_cls: 0.5687  d6.loss_mask: 0.1772  d6.loss_dice: 1.7009  d7.loss_cls: 0.5749  d7.loss_mask: 0.1758  d7.loss_dice: 1.6898  d8.loss_cls: 0.5742  d8.loss_mask: 0.1750  d8.loss_dice: 1.6749
05/11 12:02:00 - mmengine - INFO - Iter(train) [13350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:39:47  time: 1.5428  data_time: 0.0318  memory: 39434  grad_norm: 39.4142  loss: 27.2983  loss_cls: 0.5803  loss_mask: 0.1666  loss_dice: 1.7918  d0.loss_cls: 1.0786  d0.loss_mask: 0.2244  d0.loss_dice: 2.0471  d1.loss_cls: 0.8355  d1.loss_mask: 0.2077  d1.loss_dice: 2.0365  d2.loss_cls: 0.6790  d2.loss_mask: 0.1866  d2.loss_dice: 1.9622  d3.loss_cls: 0.6144  d3.loss_mask: 0.1784  d3.loss_dice: 1.8887  d4.loss_cls: 0.5873  d4.loss_mask: 0.1731  d4.loss_dice: 1.8357  d5.loss_cls: 0.5703  d5.loss_mask: 0.1724  d5.loss_dice: 1.8265  d6.loss_cls: 0.5766  d6.loss_mask: 0.1710  d6.loss_dice: 1.8222  d7.loss_cls: 0.5773  d7.loss_mask: 0.1686  d7.loss_dice: 1.8035  d8.loss_cls: 0.5814  d8.loss_mask: 0.1674  d8.loss_dice: 1.7871
05/11 12:03:16 - mmengine - INFO - Iter(train) [13400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:38:26  time: 1.5278  data_time: 0.0380  memory: 38487  grad_norm: 39.7581  loss: 27.0078  loss_cls: 0.5776  loss_mask: 0.1706  loss_dice: 1.7694  d0.loss_cls: 1.0641  d0.loss_mask: 0.2250  d0.loss_dice: 2.0079  d1.loss_cls: 0.8299  d1.loss_mask: 0.2063  d1.loss_dice: 2.0004  d2.loss_cls: 0.6697  d2.loss_mask: 0.1898  d2.loss_dice: 1.9283  d3.loss_cls: 0.5978  d3.loss_mask: 0.1819  d3.loss_dice: 1.8702  d4.loss_cls: 0.5836  d4.loss_mask: 0.1755  d4.loss_dice: 1.8127  d5.loss_cls: 0.5749  d5.loss_mask: 0.1751  d5.loss_dice: 1.7997  d6.loss_cls: 0.5760  d6.loss_mask: 0.1752  d6.loss_dice: 1.7943  d7.loss_cls: 0.5772  d7.loss_mask: 0.1738  d7.loss_dice: 1.7820  d8.loss_cls: 0.5817  d8.loss_mask: 0.1708  d8.loss_dice: 1.7664
05/11 12:04:33 - mmengine - INFO - Iter(train) [13450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:37:06  time: 1.5418  data_time: 0.0321  memory: 39465  grad_norm: 40.2446  loss: 29.1014  loss_cls: 0.6113  loss_mask: 0.1819  loss_dice: 1.9381  d0.loss_cls: 1.0826  d0.loss_mask: 0.2338  d0.loss_dice: 2.1945  d1.loss_cls: 0.8288  d1.loss_mask: 0.2213  d1.loss_dice: 2.1837  d2.loss_cls: 0.6769  d2.loss_mask: 0.2006  d2.loss_dice: 2.1095  d3.loss_cls: 0.6280  d3.loss_mask: 0.1923  d3.loss_dice: 2.0372  d4.loss_cls: 0.6059  d4.loss_mask: 0.1868  d4.loss_dice: 1.9874  d5.loss_cls: 0.6032  d5.loss_mask: 0.1854  d5.loss_dice: 1.9750  d6.loss_cls: 0.6071  d6.loss_mask: 0.1855  d6.loss_dice: 1.9643  d7.loss_cls: 0.6084  d7.loss_mask: 0.1839  d7.loss_dice: 1.9560  d8.loss_cls: 0.6129  d8.loss_mask: 0.1823  d8.loss_dice: 1.9369
05/11 12:05:50 - mmengine - INFO - Iter(train) [13500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:35:46  time: 1.5350  data_time: 0.0313  memory: 38548  grad_norm: 40.2517  loss: 27.0068  loss_cls: 0.5779  loss_mask: 0.1728  loss_dice: 1.7728  d0.loss_cls: 1.0822  d0.loss_mask: 0.2282  d0.loss_dice: 2.0004  d1.loss_cls: 0.8331  d1.loss_mask: 0.2114  d1.loss_dice: 1.9927  d2.loss_cls: 0.6721  d2.loss_mask: 0.1938  d2.loss_dice: 1.9226  d3.loss_cls: 0.5980  d3.loss_mask: 0.1834  d3.loss_dice: 1.8616  d4.loss_cls: 0.5818  d4.loss_mask: 0.1775  d4.loss_dice: 1.8092  d5.loss_cls: 0.5651  d5.loss_mask: 0.1778  d5.loss_dice: 1.8045  d6.loss_cls: 0.5694  d6.loss_mask: 0.1770  d6.loss_dice: 1.7965  d7.loss_cls: 0.5727  d7.loss_mask: 0.1747  d7.loss_dice: 1.7812  d8.loss_cls: 0.5800  d8.loss_mask: 0.1723  d8.loss_dice: 1.7642
05/11 12:07:07 - mmengine - INFO - Iter(train) [13550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:34:26  time: 1.5366  data_time: 0.0314  memory: 39203  grad_norm: 38.7415  loss: 27.3368  loss_cls: 0.5871  loss_mask: 0.1820  loss_dice: 1.7822  d0.loss_cls: 1.0783  d0.loss_mask: 0.2373  d0.loss_dice: 2.0364  d1.loss_cls: 0.8357  d1.loss_mask: 0.2215  d1.loss_dice: 1.9958  d2.loss_cls: 0.6665  d2.loss_mask: 0.2022  d2.loss_dice: 1.9449  d3.loss_cls: 0.6037  d3.loss_mask: 0.1928  d3.loss_dice: 1.8773  d4.loss_cls: 0.5879  d4.loss_mask: 0.1868  d4.loss_dice: 1.8259  d5.loss_cls: 0.5809  d5.loss_mask: 0.1861  d5.loss_dice: 1.8196  d6.loss_cls: 0.5790  d6.loss_mask: 0.1853  d6.loss_dice: 1.8197  d7.loss_cls: 0.5802  d7.loss_mask: 0.1827  d7.loss_dice: 1.8046  d8.loss_cls: 0.5867  d8.loss_mask: 0.1825  d8.loss_dice: 1.7853
05/11 12:08:24 - mmengine - INFO - Iter(train) [13600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:33:05  time: 1.5391  data_time: 0.0319  memory: 39497  grad_norm: 45.6957  loss: 29.1468  loss_cls: 0.5826  loss_mask: 0.1996  loss_dice: 1.9297  d0.loss_cls: 1.1048  d0.loss_mask: 0.2613  d0.loss_dice: 2.2309  d1.loss_cls: 0.8427  d1.loss_mask: 0.2406  d1.loss_dice: 2.1933  d2.loss_cls: 0.6881  d2.loss_mask: 0.2254  d2.loss_dice: 2.0936  d3.loss_cls: 0.6153  d3.loss_mask: 0.2099  d3.loss_dice: 2.0266  d4.loss_cls: 0.5907  d4.loss_mask: 0.2052  d4.loss_dice: 1.9739  d5.loss_cls: 0.5815  d5.loss_mask: 0.2035  d5.loss_dice: 1.9622  d6.loss_cls: 0.5832  d6.loss_mask: 0.2027  d6.loss_dice: 1.9585  d7.loss_cls: 0.5850  d7.loss_mask: 0.2002  d7.loss_dice: 1.9422  d8.loss_cls: 0.5862  d8.loss_mask: 0.1999  d8.loss_dice: 1.9272
05/11 12:09:40 - mmengine - INFO - Iter(train) [13650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:31:45  time: 1.5271  data_time: 0.0292  memory: 38346  grad_norm: 40.0450  loss: 26.4524  loss_cls: 0.5511  loss_mask: 0.1650  loss_dice: 1.7509  d0.loss_cls: 1.0557  d0.loss_mask: 0.2190  d0.loss_dice: 1.9875  d1.loss_cls: 0.7986  d1.loss_mask: 0.2051  d1.loss_dice: 1.9447  d2.loss_cls: 0.6472  d2.loss_mask: 0.1836  d2.loss_dice: 1.8894  d3.loss_cls: 0.5816  d3.loss_mask: 0.1771  d3.loss_dice: 1.8358  d4.loss_cls: 0.5644  d4.loss_mask: 0.1704  d4.loss_dice: 1.7890  d5.loss_cls: 0.5514  d5.loss_mask: 0.1695  d5.loss_dice: 1.7837  d6.loss_cls: 0.5550  d6.loss_mask: 0.1681  d6.loss_dice: 1.7705  d7.loss_cls: 0.5513  d7.loss_mask: 0.1663  d7.loss_dice: 1.7596  d8.loss_cls: 0.5504  d8.loss_mask: 0.1655  d8.loss_dice: 1.7448
05/11 12:10:56 - mmengine - INFO - Iter(train) [13700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:30:24  time: 1.5293  data_time: 0.0287  memory: 38867  grad_norm: 52.7602  loss: 25.6557  loss_cls: 0.5668  loss_mask: 0.1617  loss_dice: 1.6572  d0.loss_cls: 1.0614  d0.loss_mask: 0.2154  d0.loss_dice: 1.9048  d1.loss_cls: 0.8128  d1.loss_mask: 0.1952  d1.loss_dice: 1.8729  d2.loss_cls: 0.6591  d2.loss_mask: 0.1795  d2.loss_dice: 1.8102  d3.loss_cls: 0.5932  d3.loss_mask: 0.1722  d3.loss_dice: 1.7503  d4.loss_cls: 0.5678  d4.loss_mask: 0.1669  d4.loss_dice: 1.7006  d5.loss_cls: 0.5620  d5.loss_mask: 0.1659  d5.loss_dice: 1.6915  d6.loss_cls: 0.5611  d6.loss_mask: 0.1650  d6.loss_dice: 1.6806  d7.loss_cls: 0.5670  d7.loss_mask: 0.1630  d7.loss_dice: 1.6667  d8.loss_cls: 0.5677  d8.loss_mask: 0.1626  d8.loss_dice: 1.6545
05/11 12:12:13 - mmengine - INFO - Iter(train) [13750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:29:04  time: 1.5350  data_time: 0.0285  memory: 38527  grad_norm: 47.6306  loss: 27.4543  loss_cls: 0.5750  loss_mask: 0.1932  loss_dice: 1.7873  d0.loss_cls: 1.0796  d0.loss_mask: 0.2635  d0.loss_dice: 2.0583  d1.loss_cls: 0.8162  d1.loss_mask: 0.2334  d1.loss_dice: 2.0303  d2.loss_cls: 0.6585  d2.loss_mask: 0.2166  d2.loss_dice: 1.9480  d3.loss_cls: 0.5977  d3.loss_mask: 0.2056  d3.loss_dice: 1.8824  d4.loss_cls: 0.5811  d4.loss_mask: 0.1995  d4.loss_dice: 1.8278  d5.loss_cls: 0.5656  d5.loss_mask: 0.1983  d5.loss_dice: 1.8262  d6.loss_cls: 0.5688  d6.loss_mask: 0.1978  d6.loss_dice: 1.8181  d7.loss_cls: 0.5703  d7.loss_mask: 0.1954  d7.loss_dice: 1.8038  d8.loss_cls: 0.5765  d8.loss_mask: 0.1931  d8.loss_dice: 1.7866
05/11 12:13:30 - mmengine - INFO - Iter(train) [13800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:27:44  time: 1.5366  data_time: 0.0274  memory: 38250  grad_norm: 44.4619  loss: 28.0972  loss_cls: 0.5762  loss_mask: 0.1828  loss_dice: 1.8746  d0.loss_cls: 1.0648  d0.loss_mask: 0.2399  d0.loss_dice: 2.1062  d1.loss_cls: 0.8273  d1.loss_mask: 0.2203  d1.loss_dice: 2.0798  d2.loss_cls: 0.6778  d2.loss_mask: 0.2040  d2.loss_dice: 2.0212  d3.loss_cls: 0.6055  d3.loss_mask: 0.1945  d3.loss_dice: 1.9560  d4.loss_cls: 0.5817  d4.loss_mask: 0.1874  d4.loss_dice: 1.9162  d5.loss_cls: 0.5705  d5.loss_mask: 0.1879  d5.loss_dice: 1.9042  d6.loss_cls: 0.5728  d6.loss_mask: 0.1867  d6.loss_dice: 1.8979  d7.loss_cls: 0.5729  d7.loss_mask: 0.1831  d7.loss_dice: 1.8810  d8.loss_cls: 0.5734  d8.loss_mask: 0.1827  d8.loss_dice: 1.8680
05/11 12:14:47 - mmengine - INFO - Iter(train) [13850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:26:24  time: 1.5358  data_time: 0.0331  memory: 38555  grad_norm: 39.8154  loss: 27.0562  loss_cls: 0.5801  loss_mask: 0.1672  loss_dice: 1.7791  d0.loss_cls: 1.0702  d0.loss_mask: 0.2104  d0.loss_dice: 2.0226  d1.loss_cls: 0.8282  d1.loss_mask: 0.2000  d1.loss_dice: 2.0082  d2.loss_cls: 0.6843  d2.loss_mask: 0.1844  d2.loss_dice: 1.9287  d3.loss_cls: 0.6078  d3.loss_mask: 0.1759  d3.loss_dice: 1.8728  d4.loss_cls: 0.5852  d4.loss_mask: 0.1712  d4.loss_dice: 1.8148  d5.loss_cls: 0.5764  d5.loss_mask: 0.1712  d5.loss_dice: 1.8024  d6.loss_cls: 0.5723  d6.loss_mask: 0.1701  d6.loss_dice: 1.8096  d7.loss_cls: 0.5776  d7.loss_mask: 0.1694  d7.loss_dice: 1.7927  d8.loss_cls: 0.5836  d8.loss_mask: 0.1673  d8.loss_dice: 1.7727
05/11 12:16:03 - mmengine - INFO - Iter(train) [13900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:25:03  time: 1.5288  data_time: 0.0272  memory: 38608  grad_norm: 45.8807  loss: 26.5350  loss_cls: 0.5830  loss_mask: 0.1655  loss_dice: 1.7364  d0.loss_cls: 1.0558  d0.loss_mask: 0.2206  d0.loss_dice: 1.9605  d1.loss_cls: 0.8170  d1.loss_mask: 0.2012  d1.loss_dice: 1.9365  d2.loss_cls: 0.6697  d2.loss_mask: 0.1872  d2.loss_dice: 1.8700  d3.loss_cls: 0.6072  d3.loss_mask: 0.1771  d3.loss_dice: 1.8156  d4.loss_cls: 0.5838  d4.loss_mask: 0.1712  d4.loss_dice: 1.7795  d5.loss_cls: 0.5691  d5.loss_mask: 0.1702  d5.loss_dice: 1.7680  d6.loss_cls: 0.5701  d6.loss_mask: 0.1688  d6.loss_dice: 1.7648  d7.loss_cls: 0.5746  d7.loss_mask: 0.1675  d7.loss_dice: 1.7543  d8.loss_cls: 0.5854  d8.loss_mask: 0.1662  d8.loss_dice: 1.7379
05/11 12:17:20 - mmengine - INFO - Iter(train) [13950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:23:43  time: 1.5394  data_time: 0.0292  memory: 38720  grad_norm: 37.1287  loss: 26.3055  loss_cls: 0.5739  loss_mask: 0.1697  loss_dice: 1.7043  d0.loss_cls: 1.0867  d0.loss_mask: 0.2186  d0.loss_dice: 1.9440  d1.loss_cls: 0.8230  d1.loss_mask: 0.2068  d1.loss_dice: 1.9234  d2.loss_cls: 0.6569  d2.loss_mask: 0.1869  d2.loss_dice: 1.8616  d3.loss_cls: 0.5989  d3.loss_mask: 0.1798  d3.loss_dice: 1.8069  d4.loss_cls: 0.5787  d4.loss_mask: 0.1742  d4.loss_dice: 1.7447  d5.loss_cls: 0.5639  d5.loss_mask: 0.1735  d5.loss_dice: 1.7387  d6.loss_cls: 0.5664  d6.loss_mask: 0.1728  d6.loss_dice: 1.7365  d7.loss_cls: 0.5652  d7.loss_mask: 0.1720  d7.loss_dice: 1.7246  d8.loss_cls: 0.5727  d8.loss_mask: 0.1700  d8.loss_dice: 1.7103
05/11 12:18:37 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 12:18:37 - mmengine - INFO - Iter(train) [14000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:22:23  time: 1.5439  data_time: 0.0353  memory: 39062  grad_norm: 40.2555  loss: 26.9873  loss_cls: 0.5803  loss_mask: 0.1774  loss_dice: 1.7608  d0.loss_cls: 1.0823  d0.loss_mask: 0.2284  d0.loss_dice: 2.0135  d1.loss_cls: 0.8271  d1.loss_mask: 0.2153  d1.loss_dice: 1.9855  d2.loss_cls: 0.6791  d2.loss_mask: 0.1939  d2.loss_dice: 1.8959  d3.loss_cls: 0.6121  d3.loss_mask: 0.1865  d3.loss_dice: 1.8466  d4.loss_cls: 0.5881  d4.loss_mask: 0.1823  d4.loss_dice: 1.7977  d5.loss_cls: 0.5767  d5.loss_mask: 0.1812  d5.loss_dice: 1.7872  d6.loss_cls: 0.5785  d6.loss_mask: 0.1804  d6.loss_dice: 1.7859  d7.loss_cls: 0.5772  d7.loss_mask: 0.1793  d7.loss_dice: 1.7731  d8.loss_cls: 0.5838  d8.loss_mask: 0.1775  d8.loss_dice: 1.7537
05/11 12:18:37 - mmengine - INFO - Saving checkpoint at 14000 iterations
05/11 12:19:21 - mmengine - INFO - Iter(val) [ 50/100]    eta: 0:00:36  time: 0.7335  data_time: 0.0144  memory: 5704  
05/11 12:19:57 - mmengine - INFO - Iter(val) [100/100]    eta: 0:00:00  time: 0.7192  data_time: 0.0130  memory: 5704  
05/11 12:20:19 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.21s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/11 12:20:27 - mmengine - INFO - start multi processing evaluation ...

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

  0%|          | 0/25552 [00:00<?, ?it/s]
 50%|█████     | 12777/25552 [00:00<00:00, 127473.80it/s]
100%|██████████| 25552/25552 [00:00<00:00, 138524.39it/s]
DONE (t=50.89s).
Accumulating evaluation results...
DONE (t=0.06s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.408
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.750
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.396
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.256
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.470
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.909
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.511
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.859
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.516
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.402
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.554
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.933
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.511
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.511
05/11 12:21:18 - mmengine - INFO - segm_mAP_copypaste: 0.408 0.750 0.396 0.256 0.470 0.909
05/11 12:21:18 - mmengine - INFO - segm_mAR_copypaste: 0.511 0.859 0.516 0.402 0.554 0.933
05/11 12:21:19 - mmengine - INFO - Iter(val) [100/100]    coco/segm_mAP: 0.4080  coco/segm_mAP_50: 0.7500  coco/segm_mAP_75: 0.3960  coco/segm_mAP_s: 0.2560  coco/segm_mAP_m: 0.4700  coco/segm_mAP_l: 0.9090  data_time: 0.0137  time: 0.7250
05/11 12:21:19 - mmengine - INFO - The previous best checkpoint /data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-L_8xb32-24k_coco/best_coco_segm_mAP_50_iter_10000.pth is removed
05/11 12:21:22 - mmengine - INFO - The best checkpoint with 0.7500 coco/segm_mAP_50 at 14000 iter is saved to best_coco_segm_mAP_50_iter_14000.pth.
05/11 12:22:45 - mmengine - INFO - Iter(train) [14050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:22:08  time: 3.3713  data_time: 1.8693  memory: 39240  grad_norm: 42.8215  loss: 29.1361  loss_cls: 0.5911  loss_mask: 0.1916  loss_dice: 1.9273  d0.loss_cls: 1.1246  d0.loss_mask: 0.2434  d0.loss_dice: 2.2072  d1.loss_cls: 0.8451  d1.loss_mask: 0.2301  d1.loss_dice: 2.1829  d2.loss_cls: 0.6818  d2.loss_mask: 0.2127  d2.loss_dice: 2.1109  d3.loss_cls: 0.6160  d3.loss_mask: 0.2056  d3.loss_dice: 2.0399  d4.loss_cls: 0.6034  d4.loss_mask: 0.1998  d4.loss_dice: 1.9751  d5.loss_cls: 0.5866  d5.loss_mask: 0.1980  d5.loss_dice: 1.9659  d6.loss_cls: 0.5890  d6.loss_mask: 0.1966  d6.loss_dice: 1.9642  d7.loss_cls: 0.5917  d7.loss_mask: 0.1945  d7.loss_dice: 1.9476  d8.loss_cls: 0.5938  d8.loss_mask: 0.1930  d8.loss_dice: 1.9266
05/11 12:24:02 - mmengine - INFO - Iter(train) [14100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:20:47  time: 1.5288  data_time: 0.0291  memory: 38729  grad_norm: 43.3762  loss: 27.5231  loss_cls: 0.5781  loss_mask: 0.1709  loss_dice: 1.8069  d0.loss_cls: 1.0819  d0.loss_mask: 0.2209  d0.loss_dice: 2.0644  d1.loss_cls: 0.8415  d1.loss_mask: 0.2021  d1.loss_dice: 2.0500  d2.loss_cls: 0.6845  d2.loss_mask: 0.1904  d2.loss_dice: 1.9774  d3.loss_cls: 0.6114  d3.loss_mask: 0.1830  d3.loss_dice: 1.9036  d4.loss_cls: 0.6001  d4.loss_mask: 0.1769  d4.loss_dice: 1.8519  d5.loss_cls: 0.5769  d5.loss_mask: 0.1757  d5.loss_dice: 1.8485  d6.loss_cls: 0.5798  d6.loss_mask: 0.1738  d6.loss_dice: 1.8442  d7.loss_cls: 0.5718  d7.loss_mask: 0.1723  d7.loss_dice: 1.8268  d8.loss_cls: 0.5799  d8.loss_mask: 0.1714  d8.loss_dice: 1.8062
05/11 12:25:18 - mmengine - INFO - Iter(train) [14150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:19:26  time: 1.5264  data_time: 0.0296  memory: 39281  grad_norm: 40.2308  loss: 27.9568  loss_cls: 0.5760  loss_mask: 0.1658  loss_dice: 1.8657  d0.loss_cls: 1.0809  d0.loss_mask: 0.2134  d0.loss_dice: 2.1037  d1.loss_cls: 0.8258  d1.loss_mask: 0.2013  d1.loss_dice: 2.0958  d2.loss_cls: 0.6853  d2.loss_mask: 0.1853  d2.loss_dice: 2.0135  d3.loss_cls: 0.6143  d3.loss_mask: 0.1768  d3.loss_dice: 1.9453  d4.loss_cls: 0.6279  d4.loss_mask: 0.1704  d4.loss_dice: 1.8917  d5.loss_cls: 0.5856  d5.loss_mask: 0.1701  d5.loss_dice: 1.8996  d6.loss_cls: 0.5826  d6.loss_mask: 0.1707  d6.loss_dice: 1.8914  d7.loss_cls: 0.5794  d7.loss_mask: 0.1683  d7.loss_dice: 1.8723  d8.loss_cls: 0.5871  d8.loss_mask: 0.1658  d8.loss_dice: 1.8449
05/11 12:26:34 - mmengine - INFO - Iter(train) [14200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:18:05  time: 1.5161  data_time: 0.0298  memory: 38460  grad_norm: 39.5369  loss: 25.8388  loss_cls: 0.5632  loss_mask: 0.1647  loss_dice: 1.6855  d0.loss_cls: 1.0488  d0.loss_mask: 0.2197  d0.loss_dice: 1.9030  d1.loss_cls: 0.8054  d1.loss_mask: 0.1989  d1.loss_dice: 1.8758  d2.loss_cls: 0.6414  d2.loss_mask: 0.1807  d2.loss_dice: 1.8286  d3.loss_cls: 0.5809  d3.loss_mask: 0.1763  d3.loss_dice: 1.7734  d4.loss_cls: 0.5807  d4.loss_mask: 0.1708  d4.loss_dice: 1.7321  d5.loss_cls: 0.5525  d5.loss_mask: 0.1706  d5.loss_dice: 1.7215  d6.loss_cls: 0.5516  d6.loss_mask: 0.1684  d6.loss_dice: 1.7148  d7.loss_cls: 0.5516  d7.loss_mask: 0.1669  d7.loss_dice: 1.7032  d8.loss_cls: 0.5573  d8.loss_mask: 0.1660  d8.loss_dice: 1.6844
05/11 12:27:50 - mmengine - INFO - Iter(train) [14250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:16:44  time: 1.5209  data_time: 0.0293  memory: 38512  grad_norm: 48.0221  loss: 27.0766  loss_cls: 0.5621  loss_mask: 0.1779  loss_dice: 1.7698  d0.loss_cls: 1.0729  d0.loss_mask: 0.2306  d0.loss_dice: 2.0327  d1.loss_cls: 0.8331  d1.loss_mask: 0.2176  d1.loss_dice: 1.9969  d2.loss_cls: 0.6610  d2.loss_mask: 0.1951  d2.loss_dice: 1.9343  d3.loss_cls: 0.5979  d3.loss_mask: 0.1881  d3.loss_dice: 1.8798  d4.loss_cls: 0.5900  d4.loss_mask: 0.1863  d4.loss_dice: 1.8137  d5.loss_cls: 0.5666  d5.loss_mask: 0.1842  d5.loss_dice: 1.8048  d6.loss_cls: 0.5632  d6.loss_mask: 0.1825  d6.loss_dice: 1.8035  d7.loss_cls: 0.5600  d7.loss_mask: 0.1800  d7.loss_dice: 1.7835  d8.loss_cls: 0.5625  d8.loss_mask: 0.1779  d8.loss_dice: 1.7679
05/11 12:29:06 - mmengine - INFO - Iter(train) [14300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:15:23  time: 1.5291  data_time: 0.0296  memory: 39018  grad_norm: 41.6175  loss: 27.1696  loss_cls: 0.5612  loss_mask: 0.1753  loss_dice: 1.7817  d0.loss_cls: 1.0858  d0.loss_mask: 0.2254  d0.loss_dice: 2.0292  d1.loss_cls: 0.8229  d1.loss_mask: 0.2115  d1.loss_dice: 2.0140  d2.loss_cls: 0.6584  d2.loss_mask: 0.1897  d2.loss_dice: 1.9519  d3.loss_cls: 0.5895  d3.loss_mask: 0.1840  d3.loss_dice: 1.8870  d4.loss_cls: 0.5898  d4.loss_mask: 0.1804  d4.loss_dice: 1.8385  d5.loss_cls: 0.5646  d5.loss_mask: 0.1784  d5.loss_dice: 1.8229  d6.loss_cls: 0.5585  d6.loss_mask: 0.1779  d6.loss_dice: 1.8234  d7.loss_cls: 0.5620  d7.loss_mask: 0.1746  d7.loss_dice: 1.8048  d8.loss_cls: 0.5588  d8.loss_mask: 0.1748  d8.loss_dice: 1.7924
05/11 12:30:23 - mmengine - INFO - Iter(train) [14350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:14:03  time: 1.5366  data_time: 0.0297  memory: 38367  grad_norm: 43.0190  loss: 27.5663  loss_cls: 0.5925  loss_mask: 0.1754  loss_dice: 1.8113  d0.loss_cls: 1.0749  d0.loss_mask: 0.2202  d0.loss_dice: 2.0393  d1.loss_cls: 0.8458  d1.loss_mask: 0.2089  d1.loss_dice: 2.0158  d2.loss_cls: 0.6805  d2.loss_mask: 0.1922  d2.loss_dice: 1.9612  d3.loss_cls: 0.6274  d3.loss_mask: 0.1861  d3.loss_dice: 1.8913  d4.loss_cls: 0.6135  d4.loss_mask: 0.1821  d4.loss_dice: 1.8524  d5.loss_cls: 0.5966  d5.loss_mask: 0.1791  d5.loss_dice: 1.8391  d6.loss_cls: 0.5942  d6.loss_mask: 0.1793  d6.loss_dice: 1.8366  d7.loss_cls: 0.5924  d7.loss_mask: 0.1768  d7.loss_dice: 1.8247  d8.loss_cls: 0.5952  d8.loss_mask: 0.1761  d8.loss_dice: 1.8053
05/11 12:31:40 - mmengine - INFO - Iter(train) [14400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:12:42  time: 1.5306  data_time: 0.0351  memory: 38664  grad_norm: 38.9070  loss: 26.1293  loss_cls: 0.5740  loss_mask: 0.1772  loss_dice: 1.6855  d0.loss_cls: 1.0689  d0.loss_mask: 0.2277  d0.loss_dice: 1.9156  d1.loss_cls: 0.8119  d1.loss_mask: 0.2101  d1.loss_dice: 1.8904  d2.loss_cls: 0.6570  d2.loss_mask: 0.1949  d2.loss_dice: 1.8245  d3.loss_cls: 0.6071  d3.loss_mask: 0.1863  d3.loss_dice: 1.7740  d4.loss_cls: 0.5961  d4.loss_mask: 0.1824  d4.loss_dice: 1.7296  d5.loss_cls: 0.5764  d5.loss_mask: 0.1821  d5.loss_dice: 1.7142  d6.loss_cls: 0.5731  d6.loss_mask: 0.1810  d6.loss_dice: 1.7103  d7.loss_cls: 0.5730  d7.loss_mask: 0.1789  d7.loss_dice: 1.6965  d8.loss_cls: 0.5701  d8.loss_mask: 0.1775  d8.loss_dice: 1.6829
05/11 12:32:56 - mmengine - INFO - Iter(train) [14450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:11:21  time: 1.5197  data_time: 0.0292  memory: 39810  grad_norm: 35.5759  loss: 28.5339  loss_cls: 0.5925  loss_mask: 0.1809  loss_dice: 1.8907  d0.loss_cls: 1.1018  d0.loss_mask: 0.2271  d0.loss_dice: 2.1226  d1.loss_cls: 0.8502  d1.loss_mask: 0.2164  d1.loss_dice: 2.1062  d2.loss_cls: 0.6814  d2.loss_mask: 0.2013  d2.loss_dice: 2.0444  d3.loss_cls: 0.6293  d3.loss_mask: 0.1911  d3.loss_dice: 1.9885  d4.loss_cls: 0.6155  d4.loss_mask: 0.1874  d4.loss_dice: 1.9412  d5.loss_cls: 0.5953  d5.loss_mask: 0.1867  d5.loss_dice: 1.9350  d6.loss_cls: 0.5980  d6.loss_mask: 0.1840  d6.loss_dice: 1.9219  d7.loss_cls: 0.5958  d7.loss_mask: 0.1823  d7.loss_dice: 1.9048  d8.loss_cls: 0.5942  d8.loss_mask: 0.1808  d8.loss_dice: 1.8864
05/11 12:34:13 - mmengine - INFO - Iter(train) [14500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:10:01  time: 1.5440  data_time: 0.0309  memory: 38651  grad_norm: 43.4852  loss: 26.2132  loss_cls: 0.5718  loss_mask: 0.1622  loss_dice: 1.7081  d0.loss_cls: 1.0866  d0.loss_mask: 0.2117  d0.loss_dice: 1.9170  d1.loss_cls: 0.8316  d1.loss_mask: 0.1977  d1.loss_dice: 1.9077  d2.loss_cls: 0.6654  d2.loss_mask: 0.1796  d2.loss_dice: 1.8490  d3.loss_cls: 0.6025  d3.loss_mask: 0.1695  d3.loss_dice: 1.8001  d4.loss_cls: 0.5852  d4.loss_mask: 0.1676  d4.loss_dice: 1.7501  d5.loss_cls: 0.5717  d5.loss_mask: 0.1672  d5.loss_dice: 1.7310  d6.loss_cls: 0.5695  d6.loss_mask: 0.1670  d6.loss_dice: 1.7347  d7.loss_cls: 0.5761  d7.loss_mask: 0.1650  d7.loss_dice: 1.7243  d8.loss_cls: 0.5715  d8.loss_mask: 0.1626  d8.loss_dice: 1.7088
05/11 12:35:29 - mmengine - INFO - Iter(train) [14550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:08:40  time: 1.5199  data_time: 0.0319  memory: 39364  grad_norm: 38.6955  loss: 28.2581  loss_cls: 0.5821  loss_mask: 0.1747  loss_dice: 1.8871  d0.loss_cls: 1.0884  d0.loss_mask: 0.2228  d0.loss_dice: 2.1231  d1.loss_cls: 0.8314  d1.loss_mask: 0.2079  d1.loss_dice: 2.1092  d2.loss_cls: 0.6749  d2.loss_mask: 0.1901  d2.loss_dice: 2.0301  d3.loss_cls: 0.6083  d3.loss_mask: 0.1840  d3.loss_dice: 1.9797  d4.loss_cls: 0.6007  d4.loss_mask: 0.1803  d4.loss_dice: 1.9270  d5.loss_cls: 0.5821  d5.loss_mask: 0.1795  d5.loss_dice: 1.9165  d6.loss_cls: 0.5847  d6.loss_mask: 0.1784  d6.loss_dice: 1.9115  d7.loss_cls: 0.5821  d7.loss_mask: 0.1764  d7.loss_dice: 1.9007  d8.loss_cls: 0.5858  d8.loss_mask: 0.1748  d8.loss_dice: 1.8838
05/11 12:36:45 - mmengine - INFO - Iter(train) [14600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:07:19  time: 1.5145  data_time: 0.0319  memory: 38718  grad_norm: 50.5954  loss: 27.3409  loss_cls: 0.5712  loss_mask: 0.1691  loss_dice: 1.8096  d0.loss_cls: 1.0810  d0.loss_mask: 0.2237  d0.loss_dice: 2.0535  d1.loss_cls: 0.8125  d1.loss_mask: 0.2066  d1.loss_dice: 2.0295  d2.loss_cls: 0.6526  d2.loss_mask: 0.1901  d2.loss_dice: 1.9558  d3.loss_cls: 0.5921  d3.loss_mask: 0.1817  d3.loss_dice: 1.9026  d4.loss_cls: 0.5840  d4.loss_mask: 0.1773  d4.loss_dice: 1.8593  d5.loss_cls: 0.5683  d5.loss_mask: 0.1749  d5.loss_dice: 1.8515  d6.loss_cls: 0.5710  d6.loss_mask: 0.1736  d6.loss_dice: 1.8425  d7.loss_cls: 0.5701  d7.loss_mask: 0.1712  d7.loss_dice: 1.8256  d8.loss_cls: 0.5677  d8.loss_mask: 0.1693  d8.loss_dice: 1.8031
05/11 12:38:01 - mmengine - INFO - Iter(train) [14650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:05:59  time: 1.5317  data_time: 0.0314  memory: 38700  grad_norm: 52.3391  loss: 27.9825  loss_cls: 0.5816  loss_mask: 0.1764  loss_dice: 1.8542  d0.loss_cls: 1.0859  d0.loss_mask: 0.2212  d0.loss_dice: 2.0929  d1.loss_cls: 0.8365  d1.loss_mask: 0.2083  d1.loss_dice: 2.0763  d2.loss_cls: 0.6771  d2.loss_mask: 0.1962  d2.loss_dice: 1.9984  d3.loss_cls: 0.6126  d3.loss_mask: 0.1871  d3.loss_dice: 1.9318  d4.loss_cls: 0.6053  d4.loss_mask: 0.1819  d4.loss_dice: 1.8953  d5.loss_cls: 0.5921  d5.loss_mask: 0.1801  d5.loss_dice: 1.8836  d6.loss_cls: 0.5978  d6.loss_mask: 0.1804  d6.loss_dice: 1.8778  d7.loss_cls: 0.5891  d7.loss_mask: 0.1787  d7.loss_dice: 1.8692  d8.loss_cls: 0.5843  d8.loss_mask: 0.1766  d8.loss_dice: 1.8540
05/11 12:39:18 - mmengine - INFO - Iter(train) [14700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:04:39  time: 1.5338  data_time: 0.0311  memory: 39003  grad_norm: 45.4478  loss: 26.8954  loss_cls: 0.5824  loss_mask: 0.1650  loss_dice: 1.7684  d0.loss_cls: 1.0909  d0.loss_mask: 0.2107  d0.loss_dice: 1.9755  d1.loss_cls: 0.8376  d1.loss_mask: 0.2067  d1.loss_dice: 1.9612  d2.loss_cls: 0.6864  d2.loss_mask: 0.1829  d2.loss_dice: 1.9031  d3.loss_cls: 0.6150  d3.loss_mask: 0.1741  d3.loss_dice: 1.8376  d4.loss_cls: 0.5983  d4.loss_mask: 0.1698  d4.loss_dice: 1.8039  d5.loss_cls: 0.5856  d5.loss_mask: 0.1698  d5.loss_dice: 1.7932  d6.loss_cls: 0.5842  d6.loss_mask: 0.1686  d6.loss_dice: 1.7917  d7.loss_cls: 0.5812  d7.loss_mask: 0.1662  d7.loss_dice: 1.7765  d8.loss_cls: 0.5836  d8.loss_mask: 0.1651  d8.loss_dice: 1.7603
05/11 12:40:34 - mmengine - INFO - Iter(train) [14750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:03:18  time: 1.5282  data_time: 0.0354  memory: 38517  grad_norm: 52.0372  loss: 26.3317  loss_cls: 0.5623  loss_mask: 0.1703  loss_dice: 1.7112  d0.loss_cls: 1.0766  d0.loss_mask: 0.2222  d0.loss_dice: 1.9495  d1.loss_cls: 0.8239  d1.loss_mask: 0.2096  d1.loss_dice: 1.9206  d2.loss_cls: 0.6564  d2.loss_mask: 0.1898  d2.loss_dice: 1.8544  d3.loss_cls: 0.5900  d3.loss_mask: 0.1811  d3.loss_dice: 1.7949  d4.loss_cls: 0.5867  d4.loss_mask: 0.1777  d4.loss_dice: 1.7558  d5.loss_cls: 0.5659  d5.loss_mask: 0.1762  d5.loss_dice: 1.7445  d6.loss_cls: 0.5716  d6.loss_mask: 0.1751  d6.loss_dice: 1.7420  d7.loss_cls: 0.5690  d7.loss_mask: 0.1728  d7.loss_dice: 1.7348  d8.loss_cls: 0.5655  d8.loss_mask: 0.1705  d8.loss_dice: 1.7112
05/11 12:41:50 - mmengine - INFO - Iter(train) [14800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:01:57  time: 1.5160  data_time: 0.0320  memory: 38586  grad_norm: 39.4303  loss: 27.0867  loss_cls: 0.5605  loss_mask: 0.1680  loss_dice: 1.7900  d0.loss_cls: 1.0701  d0.loss_mask: 0.2181  d0.loss_dice: 2.0244  d1.loss_cls: 0.8153  d1.loss_mask: 0.2011  d1.loss_dice: 1.9943  d2.loss_cls: 0.6537  d2.loss_mask: 0.1839  d2.loss_dice: 1.9512  d3.loss_cls: 0.6018  d3.loss_mask: 0.1776  d3.loss_dice: 1.8853  d4.loss_cls: 0.5972  d4.loss_mask: 0.1730  d4.loss_dice: 1.8365  d5.loss_cls: 0.5671  d5.loss_mask: 0.1713  d5.loss_dice: 1.8297  d6.loss_cls: 0.5633  d6.loss_mask: 0.1704  d6.loss_dice: 1.8224  d7.loss_cls: 0.5637  d7.loss_mask: 0.1701  d7.loss_dice: 1.8049  d8.loss_cls: 0.5638  d8.loss_mask: 0.1678  d8.loss_dice: 1.7902
05/11 12:43:06 - mmengine - INFO - Iter(train) [14850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:00:37  time: 1.5196  data_time: 0.0320  memory: 38906  grad_norm: 42.1636  loss: 27.4860  loss_cls: 0.5745  loss_mask: 0.1791  loss_dice: 1.8044  d0.loss_cls: 1.0953  d0.loss_mask: 0.2310  d0.loss_dice: 2.0333  d1.loss_cls: 0.8345  d1.loss_mask: 0.2195  d1.loss_dice: 2.0129  d2.loss_cls: 0.6711  d2.loss_mask: 0.1987  d2.loss_dice: 1.9502  d3.loss_cls: 0.6121  d3.loss_mask: 0.1887  d3.loss_dice: 1.8951  d4.loss_cls: 0.6009  d4.loss_mask: 0.1845  d4.loss_dice: 1.8547  d5.loss_cls: 0.5761  d5.loss_mask: 0.1842  d5.loss_dice: 1.8457  d6.loss_cls: 0.5756  d6.loss_mask: 0.1840  d6.loss_dice: 1.8401  d7.loss_cls: 0.5756  d7.loss_mask: 0.1805  d7.loss_dice: 1.8215  d8.loss_cls: 0.5733  d8.loss_mask: 0.1798  d8.loss_dice: 1.8093
05/11 12:44:22 - mmengine - INFO - Iter(train) [14900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:59:16  time: 1.5254  data_time: 0.0315  memory: 38629  grad_norm: 46.2807  loss: 28.2755  loss_cls: 0.5856  loss_mask: 0.1777  loss_dice: 1.8686  d0.loss_cls: 1.1097  d0.loss_mask: 0.2278  d0.loss_dice: 2.1321  d1.loss_cls: 0.8578  d1.loss_mask: 0.2156  d1.loss_dice: 2.1078  d2.loss_cls: 0.6846  d2.loss_mask: 0.1949  d2.loss_dice: 2.0243  d3.loss_cls: 0.6176  d3.loss_mask: 0.1871  d3.loss_dice: 1.9655  d4.loss_cls: 0.6045  d4.loss_mask: 0.1820  d4.loss_dice: 1.9172  d5.loss_cls: 0.5861  d5.loss_mask: 0.1825  d5.loss_dice: 1.9012  d6.loss_cls: 0.5906  d6.loss_mask: 0.1813  d6.loss_dice: 1.8917  d7.loss_cls: 0.5967  d7.loss_mask: 0.1803  d7.loss_dice: 1.8793  d8.loss_cls: 0.5844  d8.loss_mask: 0.1772  d8.loss_dice: 1.8638
05/11 12:45:38 - mmengine - INFO - Iter(train) [14950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:57:56  time: 1.5214  data_time: 0.0305  memory: 39202  grad_norm: 57.0404  loss: 28.0813  loss_cls: 0.5877  loss_mask: 0.1855  loss_dice: 1.8319  d0.loss_cls: 1.1108  d0.loss_mask: 0.2448  d0.loss_dice: 2.1215  d1.loss_cls: 0.8408  d1.loss_mask: 0.2262  d1.loss_dice: 2.1035  d2.loss_cls: 0.6779  d2.loss_mask: 0.2069  d2.loss_dice: 2.0009  d3.loss_cls: 0.6201  d3.loss_mask: 0.1953  d3.loss_dice: 1.9367  d4.loss_cls: 0.5977  d4.loss_mask: 0.1931  d4.loss_dice: 1.8803  d5.loss_cls: 0.5870  d5.loss_mask: 0.1905  d5.loss_dice: 1.8703  d6.loss_cls: 0.5860  d6.loss_mask: 0.1892  d6.loss_dice: 1.8695  d7.loss_cls: 0.5912  d7.loss_mask: 0.1879  d7.loss_dice: 1.8443  d8.loss_cls: 0.5858  d8.loss_mask: 0.1865  d8.loss_dice: 1.8315
05/11 12:46:55 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 12:46:55 - mmengine - INFO - Iter(train) [15000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:56:35  time: 1.5248  data_time: 0.0365  memory: 38612  grad_norm: 39.2104  loss: 25.6225  loss_cls: 0.5641  loss_mask: 0.1619  loss_dice: 1.6630  d0.loss_cls: 1.0583  d0.loss_mask: 0.2117  d0.loss_dice: 1.8962  d1.loss_cls: 0.7865  d1.loss_mask: 0.1959  d1.loss_dice: 1.8786  d2.loss_cls: 0.6407  d2.loss_mask: 0.1782  d2.loss_dice: 1.8035  d3.loss_cls: 0.5873  d3.loss_mask: 0.1727  d3.loss_dice: 1.7465  d4.loss_cls: 0.5737  d4.loss_mask: 0.1683  d4.loss_dice: 1.7059  d5.loss_cls: 0.5613  d5.loss_mask: 0.1668  d5.loss_dice: 1.6927  d6.loss_cls: 0.5602  d6.loss_mask: 0.1653  d6.loss_dice: 1.6916  d7.loss_cls: 0.5644  d7.loss_mask: 0.1628  d7.loss_dice: 1.6756  d8.loss_cls: 0.5662  d8.loss_mask: 0.1621  d8.loss_dice: 1.6608
05/11 12:48:11 - mmengine - INFO - Iter(train) [15050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:55:14  time: 1.5163  data_time: 0.0321  memory: 38897  grad_norm: 46.8247  loss: 29.2986  loss_cls: 0.5955  loss_mask: 0.1838  loss_dice: 1.9600  d0.loss_cls: 1.1102  d0.loss_mask: 0.2364  d0.loss_dice: 2.2135  d1.loss_cls: 0.8510  d1.loss_mask: 0.2273  d1.loss_dice: 2.1930  d2.loss_cls: 0.6867  d2.loss_mask: 0.2036  d2.loss_dice: 2.1227  d3.loss_cls: 0.6341  d3.loss_mask: 0.1931  d3.loss_dice: 2.0532  d4.loss_cls: 0.6089  d4.loss_mask: 0.1917  d4.loss_dice: 2.0060  d5.loss_cls: 0.5922  d5.loss_mask: 0.1903  d5.loss_dice: 1.9912  d6.loss_cls: 0.5863  d6.loss_mask: 0.1888  d6.loss_dice: 1.9860  d7.loss_cls: 0.5959  d7.loss_mask: 0.1868  d7.loss_dice: 1.9733  d8.loss_cls: 0.5936  d8.loss_mask: 0.1846  d8.loss_dice: 1.9588
05/11 12:49:27 - mmengine - INFO - Iter(train) [15100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:53:54  time: 1.5299  data_time: 0.0314  memory: 39113  grad_norm: 40.0355  loss: 27.8129  loss_cls: 0.5835  loss_mask: 0.1765  loss_dice: 1.8292  d0.loss_cls: 1.0937  d0.loss_mask: 0.2245  d0.loss_dice: 2.0841  d1.loss_cls: 0.8362  d1.loss_mask: 0.2168  d1.loss_dice: 2.0745  d2.loss_cls: 0.6746  d2.loss_mask: 0.1949  d2.loss_dice: 1.9943  d3.loss_cls: 0.6058  d3.loss_mask: 0.1866  d3.loss_dice: 1.9338  d4.loss_cls: 0.5892  d4.loss_mask: 0.1819  d4.loss_dice: 1.8810  d5.loss_cls: 0.5810  d5.loss_mask: 0.1820  d5.loss_dice: 1.8706  d6.loss_cls: 0.5754  d6.loss_mask: 0.1807  d6.loss_dice: 1.8641  d7.loss_cls: 0.5787  d7.loss_mask: 0.1784  d7.loss_dice: 1.8463  d8.loss_cls: 0.5850  d8.loss_mask: 0.1775  d8.loss_dice: 1.8321
05/11 12:50:44 - mmengine - INFO - Iter(train) [15150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:52:34  time: 1.5301  data_time: 0.0315  memory: 38652  grad_norm: 59.2242  loss: 28.7283  loss_cls: 0.5954  loss_mask: 0.2000  loss_dice: 1.8875  d0.loss_cls: 1.1189  d0.loss_mask: 0.2546  d0.loss_dice: 2.1508  d1.loss_cls: 0.8326  d1.loss_mask: 0.2385  d1.loss_dice: 2.1266  d2.loss_cls: 0.6799  d2.loss_mask: 0.2218  d2.loss_dice: 2.0538  d3.loss_cls: 0.6259  d3.loss_mask: 0.2106  d3.loss_dice: 1.9746  d4.loss_cls: 0.6116  d4.loss_mask: 0.2062  d4.loss_dice: 1.9213  d5.loss_cls: 0.5978  d5.loss_mask: 0.2049  d5.loss_dice: 1.9172  d6.loss_cls: 0.6045  d6.loss_mask: 0.2026  d6.loss_dice: 1.9147  d7.loss_cls: 0.6049  d7.loss_mask: 0.2008  d7.loss_dice: 1.8886  d8.loss_cls: 0.6021  d8.loss_mask: 0.2002  d8.loss_dice: 1.8793
05/11 12:52:00 - mmengine - INFO - Iter(train) [15200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:51:14  time: 1.5265  data_time: 0.0316  memory: 38362  grad_norm: 49.2424  loss: 25.8810  loss_cls: 0.5740  loss_mask: 0.1673  loss_dice: 1.6830  d0.loss_cls: 1.0626  d0.loss_mask: 0.2159  d0.loss_dice: 1.8649  d1.loss_cls: 0.8207  d1.loss_mask: 0.2021  d1.loss_dice: 1.8495  d2.loss_cls: 0.6630  d2.loss_mask: 0.1854  d2.loss_dice: 1.8041  d3.loss_cls: 0.5912  d3.loss_mask: 0.1745  d3.loss_dice: 1.7681  d4.loss_cls: 0.5826  d4.loss_mask: 0.1719  d4.loss_dice: 1.7193  d5.loss_cls: 0.5713  d5.loss_mask: 0.1711  d5.loss_dice: 1.7165  d6.loss_cls: 0.5697  d6.loss_mask: 0.1712  d6.loss_dice: 1.7193  d7.loss_cls: 0.5729  d7.loss_mask: 0.1695  d7.loss_dice: 1.6950  d8.loss_cls: 0.5726  d8.loss_mask: 0.1676  d8.loss_dice: 1.6844
05/11 12:53:16 - mmengine - INFO - Iter(train) [15250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:49:54  time: 1.5325  data_time: 0.0296  memory: 38863  grad_norm: 53.0458  loss: 28.2240  loss_cls: 0.5710  loss_mask: 0.1759  loss_dice: 1.8786  d0.loss_cls: 1.1229  d0.loss_mask: 0.2295  d0.loss_dice: 2.1283  d1.loss_cls: 0.8417  d1.loss_mask: 0.2193  d1.loss_dice: 2.1180  d2.loss_cls: 0.6662  d2.loss_mask: 0.1942  d2.loss_dice: 2.0476  d3.loss_cls: 0.6025  d3.loss_mask: 0.1876  d3.loss_dice: 1.9756  d4.loss_cls: 0.5835  d4.loss_mask: 0.1820  d4.loss_dice: 1.9158  d5.loss_cls: 0.5725  d5.loss_mask: 0.1809  d5.loss_dice: 1.9097  d6.loss_cls: 0.5734  d6.loss_mask: 0.1793  d6.loss_dice: 1.9085  d7.loss_cls: 0.5785  d7.loss_mask: 0.1783  d7.loss_dice: 1.8796  d8.loss_cls: 0.5767  d8.loss_mask: 0.1774  d8.loss_dice: 1.8690
05/11 12:54:33 - mmengine - INFO - Iter(train) [15300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:48:33  time: 1.5242  data_time: 0.0318  memory: 38827  grad_norm: 49.1958  loss: 27.6343  loss_cls: 0.5703  loss_mask: 0.1720  loss_dice: 1.8244  d0.loss_cls: 1.0956  d0.loss_mask: 0.2251  d0.loss_dice: 2.0883  d1.loss_cls: 0.8211  d1.loss_mask: 0.2131  d1.loss_dice: 2.0747  d2.loss_cls: 0.6536  d2.loss_mask: 0.1937  d2.loss_dice: 1.9991  d3.loss_cls: 0.5926  d3.loss_mask: 0.1838  d3.loss_dice: 1.9264  d4.loss_cls: 0.5799  d4.loss_mask: 0.1789  d4.loss_dice: 1.8758  d5.loss_cls: 0.5680  d5.loss_mask: 0.1766  d5.loss_dice: 1.8647  d6.loss_cls: 0.5662  d6.loss_mask: 0.1763  d6.loss_dice: 1.8639  d7.loss_cls: 0.5670  d7.loss_mask: 0.1741  d7.loss_dice: 1.8440  d8.loss_cls: 0.5657  d8.loss_mask: 0.1718  d8.loss_dice: 1.8279
05/11 12:55:50 - mmengine - INFO - Iter(train) [15350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:47:14  time: 1.5466  data_time: 0.0291  memory: 39299  grad_norm: 51.0760  loss: 28.5389  loss_cls: 0.5940  loss_mask: 0.1832  loss_dice: 1.8954  d0.loss_cls: 1.0919  d0.loss_mask: 0.2378  d0.loss_dice: 2.1493  d1.loss_cls: 0.8184  d1.loss_mask: 0.2234  d1.loss_dice: 2.1339  d2.loss_cls: 0.6580  d2.loss_mask: 0.2048  d2.loss_dice: 2.0493  d3.loss_cls: 0.6204  d3.loss_mask: 0.1930  d3.loss_dice: 1.9807  d4.loss_cls: 0.6015  d4.loss_mask: 0.1872  d4.loss_dice: 1.9393  d5.loss_cls: 0.5940  d5.loss_mask: 0.1869  d5.loss_dice: 1.9311  d6.loss_cls: 0.5938  d6.loss_mask: 0.1867  d6.loss_dice: 1.9252  d7.loss_cls: 0.6033  d7.loss_mask: 0.1848  d7.loss_dice: 1.9000  d8.loss_cls: 0.5968  d8.loss_mask: 0.1830  d8.loss_dice: 1.8920
05/11 12:57:06 - mmengine - INFO - Iter(train) [15400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:45:53  time: 1.5260  data_time: 0.0276  memory: 38370  grad_norm: 41.6349  loss: 25.5588  loss_cls: 0.5558  loss_mask: 0.1585  loss_dice: 1.6652  d0.loss_cls: 1.0613  d0.loss_mask: 0.2161  d0.loss_dice: 1.8929  d1.loss_cls: 0.8101  d1.loss_mask: 0.1924  d1.loss_dice: 1.8734  d2.loss_cls: 0.6360  d2.loss_mask: 0.1751  d2.loss_dice: 1.8044  d3.loss_cls: 0.5799  d3.loss_mask: 0.1677  d3.loss_dice: 1.7544  d4.loss_cls: 0.5618  d4.loss_mask: 0.1645  d4.loss_dice: 1.7067  d5.loss_cls: 0.5511  d5.loss_mask: 0.1642  d5.loss_dice: 1.6929  d6.loss_cls: 0.5489  d6.loss_mask: 0.1626  d6.loss_dice: 1.6940  d7.loss_cls: 0.5530  d7.loss_mask: 0.1602  d7.loss_dice: 1.6769  d8.loss_cls: 0.5571  d8.loss_mask: 0.1592  d8.loss_dice: 1.6625
05/11 12:58:22 - mmengine - INFO - Iter(train) [15450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:44:33  time: 1.5198  data_time: 0.0282  memory: 38433  grad_norm: 50.1688  loss: 25.0850  loss_cls: 0.5639  loss_mask: 0.1624  loss_dice: 1.6139  d0.loss_cls: 1.0641  d0.loss_mask: 0.2123  d0.loss_dice: 1.8302  d1.loss_cls: 0.8091  d1.loss_mask: 0.1939  d1.loss_dice: 1.8140  d2.loss_cls: 0.6485  d2.loss_mask: 0.1774  d2.loss_dice: 1.7484  d3.loss_cls: 0.5812  d3.loss_mask: 0.1712  d3.loss_dice: 1.6991  d4.loss_cls: 0.5660  d4.loss_mask: 0.1666  d4.loss_dice: 1.6513  d5.loss_cls: 0.5569  d5.loss_mask: 0.1667  d5.loss_dice: 1.6443  d6.loss_cls: 0.5559  d6.loss_mask: 0.1655  d6.loss_dice: 1.6398  d7.loss_cls: 0.5585  d7.loss_mask: 0.1634  d7.loss_dice: 1.6287  d8.loss_cls: 0.5622  d8.loss_mask: 0.1626  d8.loss_dice: 1.6070
05/11 12:59:39 - mmengine - INFO - Iter(train) [15500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:43:13  time: 1.5280  data_time: 0.0282  memory: 38464  grad_norm: 39.5607  loss: 26.0081  loss_cls: 0.5691  loss_mask: 0.1748  loss_dice: 1.6803  d0.loss_cls: 1.0762  d0.loss_mask: 0.2293  d0.loss_dice: 1.9133  d1.loss_cls: 0.8169  d1.loss_mask: 0.2087  d1.loss_dice: 1.8946  d2.loss_cls: 0.6584  d2.loss_mask: 0.1938  d2.loss_dice: 1.8216  d3.loss_cls: 0.5904  d3.loss_mask: 0.1830  d3.loss_dice: 1.7651  d4.loss_cls: 0.5680  d4.loss_mask: 0.1807  d4.loss_dice: 1.7183  d5.loss_cls: 0.5604  d5.loss_mask: 0.1791  d5.loss_dice: 1.7158  d6.loss_cls: 0.5653  d6.loss_mask: 0.1781  d6.loss_dice: 1.7080  d7.loss_cls: 0.5734  d7.loss_mask: 0.1752  d7.loss_dice: 1.6905  d8.loss_cls: 0.5683  d8.loss_mask: 0.1754  d8.loss_dice: 1.6762
05/11 13:00:55 - mmengine - INFO - Iter(train) [15550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:41:53  time: 1.5303  data_time: 0.0276  memory: 38217  grad_norm: 41.3725  loss: 25.3881  loss_cls: 0.5748  loss_mask: 0.1605  loss_dice: 1.6403  d0.loss_cls: 1.0655  d0.loss_mask: 0.2084  d0.loss_dice: 1.8420  d1.loss_cls: 0.8097  d1.loss_mask: 0.1942  d1.loss_dice: 1.8373  d2.loss_cls: 0.6463  d2.loss_mask: 0.1770  d2.loss_dice: 1.7690  d3.loss_cls: 0.5886  d3.loss_mask: 0.1701  d3.loss_dice: 1.7170  d4.loss_cls: 0.5740  d4.loss_mask: 0.1674  d4.loss_dice: 1.6732  d5.loss_cls: 0.5647  d5.loss_mask: 0.1686  d5.loss_dice: 1.6703  d6.loss_cls: 0.5676  d6.loss_mask: 0.1657  d6.loss_dice: 1.6685  d7.loss_cls: 0.5741  d7.loss_mask: 0.1632  d7.loss_dice: 1.6492  d8.loss_cls: 0.5763  d8.loss_mask: 0.1626  d8.loss_dice: 1.6419
05/11 13:02:12 - mmengine - INFO - Iter(train) [15600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:40:33  time: 1.5432  data_time: 0.0284  memory: 38726  grad_norm: 43.9814  loss: 27.0686  loss_cls: 0.5671  loss_mask: 0.1623  loss_dice: 1.7941  d0.loss_cls: 1.0997  d0.loss_mask: 0.2103  d0.loss_dice: 2.0227  d1.loss_cls: 0.8186  d1.loss_mask: 0.1945  d1.loss_dice: 2.0207  d2.loss_cls: 0.6554  d2.loss_mask: 0.1786  d2.loss_dice: 1.9548  d3.loss_cls: 0.5958  d3.loss_mask: 0.1696  d3.loss_dice: 1.8806  d4.loss_cls: 0.5727  d4.loss_mask: 0.1657  d4.loss_dice: 1.8371  d5.loss_cls: 0.5638  d5.loss_mask: 0.1644  d5.loss_dice: 1.8303  d6.loss_cls: 0.5633  d6.loss_mask: 0.1638  d6.loss_dice: 1.8233  d7.loss_cls: 0.5660  d7.loss_mask: 0.1635  d7.loss_dice: 1.8070  d8.loss_cls: 0.5663  d8.loss_mask: 0.1619  d8.loss_dice: 1.7948
05/11 13:03:29 - mmengine - INFO - Iter(train) [15650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:39:13  time: 1.5325  data_time: 0.0291  memory: 39032  grad_norm: 41.2010  loss: 28.0637  loss_cls: 0.5843  loss_mask: 0.1772  loss_dice: 1.8530  d0.loss_cls: 1.1111  d0.loss_mask: 0.2261  d0.loss_dice: 2.1032  d1.loss_cls: 0.8175  d1.loss_mask: 0.2112  d1.loss_dice: 2.1120  d2.loss_cls: 0.6674  d2.loss_mask: 0.1978  d2.loss_dice: 2.0304  d3.loss_cls: 0.6058  d3.loss_mask: 0.1882  d3.loss_dice: 1.9571  d4.loss_cls: 0.5807  d4.loss_mask: 0.1824  d4.loss_dice: 1.9096  d5.loss_cls: 0.5721  d5.loss_mask: 0.1826  d5.loss_dice: 1.8944  d6.loss_cls: 0.5738  d6.loss_mask: 0.1804  d6.loss_dice: 1.8910  d7.loss_cls: 0.5819  d7.loss_mask: 0.1788  d7.loss_dice: 1.8725  d8.loss_cls: 0.5861  d8.loss_mask: 0.1779  d8.loss_dice: 1.8575
05/11 13:04:46 - mmengine - INFO - Iter(train) [15700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:37:54  time: 1.5395  data_time: 0.0306  memory: 39083  grad_norm: 46.1825  loss: 27.1171  loss_cls: 0.5592  loss_mask: 0.1678  loss_dice: 1.8038  d0.loss_cls: 1.1077  d0.loss_mask: 0.2200  d0.loss_dice: 2.0197  d1.loss_cls: 0.8153  d1.loss_mask: 0.1997  d1.loss_dice: 2.0208  d2.loss_cls: 0.6513  d2.loss_mask: 0.1823  d2.loss_dice: 1.9475  d3.loss_cls: 0.5840  d3.loss_mask: 0.1761  d3.loss_dice: 1.8916  d4.loss_cls: 0.5646  d4.loss_mask: 0.1736  d4.loss_dice: 1.8384  d5.loss_cls: 0.5612  d5.loss_mask: 0.1714  d5.loss_dice: 1.8250  d6.loss_cls: 0.5557  d6.loss_mask: 0.1705  d6.loss_dice: 1.8313  d7.loss_cls: 0.5639  d7.loss_mask: 0.1698  d7.loss_dice: 1.8148  d8.loss_cls: 0.5629  d8.loss_mask: 0.1674  d8.loss_dice: 1.7997
05/11 13:06:02 - mmengine - INFO - Iter(train) [15750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:36:34  time: 1.5291  data_time: 0.0300  memory: 39107  grad_norm: 37.1908  loss: 27.9457  loss_cls: 0.6036  loss_mask: 0.1758  loss_dice: 1.8300  d0.loss_cls: 1.0998  d0.loss_mask: 0.2324  d0.loss_dice: 2.0775  d1.loss_cls: 0.8405  d1.loss_mask: 0.2164  d1.loss_dice: 2.0643  d2.loss_cls: 0.6739  d2.loss_mask: 0.1969  d2.loss_dice: 1.9898  d3.loss_cls: 0.6158  d3.loss_mask: 0.1868  d3.loss_dice: 1.9245  d4.loss_cls: 0.6098  d4.loss_mask: 0.1838  d4.loss_dice: 1.8727  d5.loss_cls: 0.6080  d5.loss_mask: 0.1802  d5.loss_dice: 1.8591  d6.loss_cls: 0.6170  d6.loss_mask: 0.1795  d6.loss_dice: 1.8563  d7.loss_cls: 0.6154  d7.loss_mask: 0.1774  d7.loss_dice: 1.8451  d8.loss_cls: 0.6053  d8.loss_mask: 0.1760  d8.loss_dice: 1.8321
05/11 13:07:20 - mmengine - INFO - Iter(train) [15800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:35:14  time: 1.5455  data_time: 0.0290  memory: 39087  grad_norm: 44.8561  loss: 26.6952  loss_cls: 0.5605  loss_mask: 0.1704  loss_dice: 1.7526  d0.loss_cls: 1.0971  d0.loss_mask: 0.2224  d0.loss_dice: 1.9963  d1.loss_cls: 0.8135  d1.loss_mask: 0.2009  d1.loss_dice: 1.9820  d2.loss_cls: 0.6378  d2.loss_mask: 0.1868  d2.loss_dice: 1.9150  d3.loss_cls: 0.5789  d3.loss_mask: 0.1767  d3.loss_dice: 1.8506  d4.loss_cls: 0.5590  d4.loss_mask: 0.1734  d4.loss_dice: 1.8036  d5.loss_cls: 0.5518  d5.loss_mask: 0.1746  d5.loss_dice: 1.7892  d6.loss_cls: 0.5603  d6.loss_mask: 0.1748  d6.loss_dice: 1.7827  d7.loss_cls: 0.5650  d7.loss_mask: 0.1723  d7.loss_dice: 1.7637  d8.loss_cls: 0.5595  d8.loss_mask: 0.1702  d8.loss_dice: 1.7535
05/11 13:08:36 - mmengine - INFO - Iter(train) [15850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:33:54  time: 1.5310  data_time: 0.0281  memory: 39283  grad_norm: 45.7613  loss: 28.2336  loss_cls: 0.5809  loss_mask: 0.1745  loss_dice: 1.8848  d0.loss_cls: 1.1099  d0.loss_mask: 0.2233  d0.loss_dice: 2.1257  d1.loss_cls: 0.8087  d1.loss_mask: 0.2118  d1.loss_dice: 2.1296  d2.loss_cls: 0.6499  d2.loss_mask: 0.1936  d2.loss_dice: 2.0429  d3.loss_cls: 0.5953  d3.loss_mask: 0.1846  d3.loss_dice: 1.9756  d4.loss_cls: 0.5769  d4.loss_mask: 0.1802  d4.loss_dice: 1.9274  d5.loss_cls: 0.5770  d5.loss_mask: 0.1788  d5.loss_dice: 1.9167  d6.loss_cls: 0.5883  d6.loss_mask: 0.1783  d6.loss_dice: 1.9132  d7.loss_cls: 0.5888  d7.loss_mask: 0.1771  d7.loss_dice: 1.9021  d8.loss_cls: 0.5811  d8.loss_mask: 0.1752  d8.loss_dice: 1.8814
05/11 13:09:54 - mmengine - INFO - Iter(train) [15900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:32:35  time: 1.5446  data_time: 0.0349  memory: 38755  grad_norm: 38.7629  loss: 26.6930  loss_cls: 0.5632  loss_mask: 0.1844  loss_dice: 1.7361  d0.loss_cls: 1.0943  d0.loss_mask: 0.2403  d0.loss_dice: 1.9749  d1.loss_cls: 0.8221  d1.loss_mask: 0.2201  d1.loss_dice: 1.9645  d2.loss_cls: 0.6457  d2.loss_mask: 0.1987  d2.loss_dice: 1.8886  d3.loss_cls: 0.5839  d3.loss_mask: 0.1920  d3.loss_dice: 1.8220  d4.loss_cls: 0.5661  d4.loss_mask: 0.1895  d4.loss_dice: 1.7732  d5.loss_cls: 0.5669  d5.loss_mask: 0.1875  d5.loss_dice: 1.7614  d6.loss_cls: 0.5716  d6.loss_mask: 0.1873  d6.loss_dice: 1.7629  d7.loss_cls: 0.5815  d7.loss_mask: 0.1860  d7.loss_dice: 1.7409  d8.loss_cls: 0.5689  d8.loss_mask: 0.1839  d8.loss_dice: 1.7345
05/11 13:11:10 - mmengine - INFO - Iter(train) [15950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:31:15  time: 1.5318  data_time: 0.0286  memory: 38626  grad_norm: 42.3148  loss: 25.3020  loss_cls: 0.5618  loss_mask: 0.1632  loss_dice: 1.6336  d0.loss_cls: 1.0738  d0.loss_mask: 0.2142  d0.loss_dice: 1.8409  d1.loss_cls: 0.8027  d1.loss_mask: 0.1951  d1.loss_dice: 1.8279  d2.loss_cls: 0.6354  d2.loss_mask: 0.1794  d2.loss_dice: 1.7750  d3.loss_cls: 0.5813  d3.loss_mask: 0.1727  d3.loss_dice: 1.7138  d4.loss_cls: 0.5690  d4.loss_mask: 0.1682  d4.loss_dice: 1.6680  d5.loss_cls: 0.5675  d5.loss_mask: 0.1675  d5.loss_dice: 1.6586  d6.loss_cls: 0.5669  d6.loss_mask: 0.1670  d6.loss_dice: 1.6613  d7.loss_cls: 0.5663  d7.loss_mask: 0.1650  d7.loss_dice: 1.6443  d8.loss_cls: 0.5617  d8.loss_mask: 0.1641  d8.loss_dice: 1.6358
05/11 13:12:27 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 13:12:27 - mmengine - INFO - Iter(train) [16000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:29:55  time: 1.5424  data_time: 0.0316  memory: 38189  grad_norm: 49.6245  loss: 26.3697  loss_cls: 0.5697  loss_mask: 0.1704  loss_dice: 1.7172  d0.loss_cls: 1.0977  d0.loss_mask: 0.2275  d0.loss_dice: 1.9462  d1.loss_cls: 0.8109  d1.loss_mask: 0.2084  d1.loss_dice: 1.9437  d2.loss_cls: 0.6407  d2.loss_mask: 0.1869  d2.loss_dice: 1.8719  d3.loss_cls: 0.5819  d3.loss_mask: 0.1808  d3.loss_dice: 1.8035  d4.loss_cls: 0.5741  d4.loss_mask: 0.1743  d4.loss_dice: 1.7579  d5.loss_cls: 0.5662  d5.loss_mask: 0.1745  d5.loss_dice: 1.7481  d6.loss_cls: 0.5678  d6.loss_mask: 0.1737  d6.loss_dice: 1.7477  d7.loss_cls: 0.5705  d7.loss_mask: 0.1722  d7.loss_dice: 1.7260  d8.loss_cls: 0.5671  d8.loss_mask: 0.1702  d8.loss_dice: 1.7220
05/11 13:12:27 - mmengine - INFO - Saving checkpoint at 16000 iterations
05/11 13:13:10 - mmengine - INFO - Iter(val) [ 50/100]    eta: 0:00:36  time: 0.7336  data_time: 0.0150  memory: 5704  
05/11 13:13:47 - mmengine - INFO - Iter(val) [100/100]    eta: 0:00:00  time: 0.7253  data_time: 0.0138  memory: 5704  
05/11 13:14:10 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.17s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/11 13:14:17 - mmengine - INFO - start multi processing evaluation ...

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

  0%|          | 0/25552 [00:00<?, ?it/s]
 50%|█████     | 12777/25552 [00:00<00:00, 127044.08it/s]
100%|██████████| 25552/25552 [00:00<00:00, 137730.59it/s]
DONE (t=52.44s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.375
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.703
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.340
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.233
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.429
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.493
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.844
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.477
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.376
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.544
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.883
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.493
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.493
05/11 13:15:09 - mmengine - INFO - segm_mAP_copypaste: 0.375 0.703 0.340 0.233 0.429 0.861
05/11 13:15:09 - mmengine - INFO - segm_mAR_copypaste: 0.493 0.844 0.477 0.376 0.544 0.883
05/11 13:15:10 - mmengine - INFO - Iter(val) [100/100]    coco/segm_mAP: 0.3750  coco/segm_mAP_50: 0.7030  coco/segm_mAP_75: 0.3400  coco/segm_mAP_s: 0.2330  coco/segm_mAP_m: 0.4290  coco/segm_mAP_l: 0.8610  data_time: 0.0143  time: 0.7281
05/11 13:16:27 - mmengine - INFO - Iter(train) [16050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:29:17  time: 3.2061  data_time: 1.7012  memory: 38883  grad_norm: 38.9824  loss: 27.3606  loss_cls: 0.5830  loss_mask: 0.1731  loss_dice: 1.7885  d0.loss_cls: 1.1215  d0.loss_mask: 0.2289  d0.loss_dice: 2.0392  d1.loss_cls: 0.8263  d1.loss_mask: 0.2135  d1.loss_dice: 2.0267  d2.loss_cls: 0.6731  d2.loss_mask: 0.1930  d2.loss_dice: 1.9513  d3.loss_cls: 0.6027  d3.loss_mask: 0.1838  d3.loss_dice: 1.8866  d4.loss_cls: 0.5901  d4.loss_mask: 0.1790  d4.loss_dice: 1.8300  d5.loss_cls: 0.5834  d5.loss_mask: 0.1773  d5.loss_dice: 1.8196  d6.loss_cls: 0.5833  d6.loss_mask: 0.1776  d6.loss_dice: 1.8187  d7.loss_cls: 0.5858  d7.loss_mask: 0.1739  d7.loss_dice: 1.8055  d8.loss_cls: 0.5837  d8.loss_mask: 0.1733  d8.loss_dice: 1.7880
05/11 13:17:44 - mmengine - INFO - Iter(train) [16100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:27:57  time: 1.5357  data_time: 0.0295  memory: 38167  grad_norm: 40.0447  loss: 26.5879  loss_cls: 0.5668  loss_mask: 0.1684  loss_dice: 1.7415  d0.loss_cls: 1.0879  d0.loss_mask: 0.2226  d0.loss_dice: 1.9794  d1.loss_cls: 0.8092  d1.loss_mask: 0.2039  d1.loss_dice: 1.9714  d2.loss_cls: 0.6415  d2.loss_mask: 0.1884  d2.loss_dice: 1.8883  d3.loss_cls: 0.5863  d3.loss_mask: 0.1803  d3.loss_dice: 1.8219  d4.loss_cls: 0.5786  d4.loss_mask: 0.1744  d4.loss_dice: 1.7712  d5.loss_cls: 0.5722  d5.loss_mask: 0.1739  d5.loss_dice: 1.7670  d6.loss_cls: 0.5703  d6.loss_mask: 0.1736  d6.loss_dice: 1.7737  d7.loss_cls: 0.5793  d7.loss_mask: 0.1719  d7.loss_dice: 1.7473  d8.loss_cls: 0.5690  d8.loss_mask: 0.1693  d8.loss_dice: 1.7386
05/11 13:19:01 - mmengine - INFO - Iter(train) [16150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:26:37  time: 1.5377  data_time: 0.0301  memory: 38596  grad_norm: 48.5620  loss: 27.3362  loss_cls: 0.5667  loss_mask: 0.1626  loss_dice: 1.8184  d0.loss_cls: 1.1071  d0.loss_mask: 0.2116  d0.loss_dice: 2.0618  d1.loss_cls: 0.8421  d1.loss_mask: 0.1981  d1.loss_dice: 2.0278  d2.loss_cls: 0.6595  d2.loss_mask: 0.1799  d2.loss_dice: 1.9653  d3.loss_cls: 0.5951  d3.loss_mask: 0.1708  d3.loss_dice: 1.9066  d4.loss_cls: 0.5792  d4.loss_mask: 0.1661  d4.loss_dice: 1.8544  d5.loss_cls: 0.5641  d5.loss_mask: 0.1658  d5.loss_dice: 1.8416  d6.loss_cls: 0.5664  d6.loss_mask: 0.1651  d6.loss_dice: 1.8453  d7.loss_cls: 0.5751  d7.loss_mask: 0.1643  d7.loss_dice: 1.8281  d8.loss_cls: 0.5664  d8.loss_mask: 0.1632  d8.loss_dice: 1.8177
05/11 13:20:18 - mmengine - INFO - Iter(train) [16200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:25:17  time: 1.5372  data_time: 0.0317  memory: 39097  grad_norm: 42.7732  loss: 27.2734  loss_cls: 0.5619  loss_mask: 0.1779  loss_dice: 1.7870  d0.loss_cls: 1.1056  d0.loss_mask: 0.2324  d0.loss_dice: 2.0453  d1.loss_cls: 0.8364  d1.loss_mask: 0.2141  d1.loss_dice: 2.0258  d2.loss_cls: 0.6614  d2.loss_mask: 0.1984  d2.loss_dice: 1.9497  d3.loss_cls: 0.5891  d3.loss_mask: 0.1881  d3.loss_dice: 1.8804  d4.loss_cls: 0.5706  d4.loss_mask: 0.1830  d4.loss_dice: 1.8306  d5.loss_cls: 0.5710  d5.loss_mask: 0.1833  d5.loss_dice: 1.8247  d6.loss_cls: 0.5680  d6.loss_mask: 0.1826  d6.loss_dice: 1.8218  d7.loss_cls: 0.5707  d7.loss_mask: 0.1814  d7.loss_dice: 1.8005  d8.loss_cls: 0.5649  d8.loss_mask: 0.1783  d8.loss_dice: 1.7885
05/11 13:21:34 - mmengine - INFO - Iter(train) [16250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:23:57  time: 1.5222  data_time: 0.0318  memory: 38502  grad_norm: 53.5709  loss: 26.1260  loss_cls: 0.5584  loss_mask: 0.1711  loss_dice: 1.7024  d0.loss_cls: 1.0853  d0.loss_mask: 0.2216  d0.loss_dice: 1.9320  d1.loss_cls: 0.8209  d1.loss_mask: 0.2079  d1.loss_dice: 1.9053  d2.loss_cls: 0.6413  d2.loss_mask: 0.1858  d2.loss_dice: 1.8469  d3.loss_cls: 0.5822  d3.loss_mask: 0.1803  d3.loss_dice: 1.7885  d4.loss_cls: 0.5641  d4.loss_mask: 0.1744  d4.loss_dice: 1.7431  d5.loss_cls: 0.5541  d5.loss_mask: 0.1742  d5.loss_dice: 1.7364  d6.loss_cls: 0.5564  d6.loss_mask: 0.1738  d6.loss_dice: 1.7338  d7.loss_cls: 0.5610  d7.loss_mask: 0.1719  d7.loss_dice: 1.7193  d8.loss_cls: 0.5594  d8.loss_mask: 0.1706  d8.loss_dice: 1.7038
05/11 13:22:50 - mmengine - INFO - Iter(train) [16300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:22:36  time: 1.5227  data_time: 0.0299  memory: 38351  grad_norm: 44.6636  loss: 26.8796  loss_cls: 0.5726  loss_mask: 0.1722  loss_dice: 1.7637  d0.loss_cls: 1.0906  d0.loss_mask: 0.2257  d0.loss_dice: 1.9964  d1.loss_cls: 0.8326  d1.loss_mask: 0.2074  d1.loss_dice: 1.9796  d2.loss_cls: 0.6560  d2.loss_mask: 0.1907  d2.loss_dice: 1.9132  d3.loss_cls: 0.6009  d3.loss_mask: 0.1820  d3.loss_dice: 1.8420  d4.loss_cls: 0.5771  d4.loss_mask: 0.1783  d4.loss_dice: 1.7955  d5.loss_cls: 0.5658  d5.loss_mask: 0.1765  d5.loss_dice: 1.7891  d6.loss_cls: 0.5636  d6.loss_mask: 0.1758  d6.loss_dice: 1.7943  d7.loss_cls: 0.5701  d7.loss_mask: 0.1742  d7.loss_dice: 1.7799  d8.loss_cls: 0.5763  d8.loss_mask: 0.1728  d8.loss_dice: 1.7647
05/11 13:24:07 - mmengine - INFO - Iter(train) [16350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:21:17  time: 1.5352  data_time: 0.0292  memory: 38419  grad_norm: 37.8411  loss: 24.9386  loss_cls: 0.5583  loss_mask: 0.1641  loss_dice: 1.5833  d0.loss_cls: 1.0841  d0.loss_mask: 0.2165  d0.loss_dice: 1.8267  d1.loss_cls: 0.8421  d1.loss_mask: 0.2000  d1.loss_dice: 1.7936  d2.loss_cls: 0.6485  d2.loss_mask: 0.1800  d2.loss_dice: 1.7292  d3.loss_cls: 0.5799  d3.loss_mask: 0.1711  d3.loss_dice: 1.6749  d4.loss_cls: 0.5720  d4.loss_mask: 0.1694  d4.loss_dice: 1.6266  d5.loss_cls: 0.5585  d5.loss_mask: 0.1678  d5.loss_dice: 1.6131  d6.loss_cls: 0.5559  d6.loss_mask: 0.1668  d6.loss_dice: 1.6163  d7.loss_cls: 0.5601  d7.loss_mask: 0.1659  d7.loss_dice: 1.5975  d8.loss_cls: 0.5610  d8.loss_mask: 0.1643  d8.loss_dice: 1.5909
05/11 13:25:22 - mmengine - INFO - Iter(train) [16400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:19:56  time: 1.5182  data_time: 0.0291  memory: 38769  grad_norm: 37.9641  loss: 26.2372  loss_cls: 0.5707  loss_mask: 0.1703  loss_dice: 1.7110  d0.loss_cls: 1.0921  d0.loss_mask: 0.2164  d0.loss_dice: 1.9040  d1.loss_cls: 0.8185  d1.loss_mask: 0.2024  d1.loss_dice: 1.9097  d2.loss_cls: 0.6435  d2.loss_mask: 0.1882  d2.loss_dice: 1.8599  d3.loss_cls: 0.5839  d3.loss_mask: 0.1802  d3.loss_dice: 1.7929  d4.loss_cls: 0.5729  d4.loss_mask: 0.1775  d4.loss_dice: 1.7486  d5.loss_cls: 0.5672  d5.loss_mask: 0.1755  d5.loss_dice: 1.7412  d6.loss_cls: 0.5711  d6.loss_mask: 0.1757  d6.loss_dice: 1.7384  d7.loss_cls: 0.5766  d7.loss_mask: 0.1727  d7.loss_dice: 1.7234  d8.loss_cls: 0.5731  d8.loss_mask: 0.1699  d8.loss_dice: 1.7095
05/11 13:26:39 - mmengine - INFO - Iter(train) [16450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:18:36  time: 1.5309  data_time: 0.0284  memory: 38630  grad_norm: 41.4440  loss: 26.3261  loss_cls: 0.5873  loss_mask: 0.1608  loss_dice: 1.7110  d0.loss_cls: 1.1011  d0.loss_mask: 0.2061  d0.loss_dice: 1.9226  d1.loss_cls: 0.8480  d1.loss_mask: 0.1963  d1.loss_dice: 1.9024  d2.loss_cls: 0.6602  d2.loss_mask: 0.1764  d2.loss_dice: 1.8602  d3.loss_cls: 0.6019  d3.loss_mask: 0.1683  d3.loss_dice: 1.7939  d4.loss_cls: 0.5926  d4.loss_mask: 0.1658  d4.loss_dice: 1.7494  d5.loss_cls: 0.5905  d5.loss_mask: 0.1663  d5.loss_dice: 1.7413  d6.loss_cls: 0.5870  d6.loss_mask: 0.1641  d6.loss_dice: 1.7374  d7.loss_cls: 0.6007  d7.loss_mask: 0.1619  d7.loss_dice: 1.7192  d8.loss_cls: 0.5867  d8.loss_mask: 0.1605  d8.loss_dice: 1.7059
05/11 13:27:55 - mmengine - INFO - Iter(train) [16500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:17:16  time: 1.5272  data_time: 0.0294  memory: 38816  grad_norm: 44.9140  loss: 26.3421  loss_cls: 0.5648  loss_mask: 0.1714  loss_dice: 1.7190  d0.loss_cls: 1.0860  d0.loss_mask: 0.2256  d0.loss_dice: 1.9439  d1.loss_cls: 0.8202  d1.loss_mask: 0.2083  d1.loss_dice: 1.9174  d2.loss_cls: 0.6497  d2.loss_mask: 0.1885  d2.loss_dice: 1.8597  d3.loss_cls: 0.5799  d3.loss_mask: 0.1790  d3.loss_dice: 1.8111  d4.loss_cls: 0.5723  d4.loss_mask: 0.1773  d4.loss_dice: 1.7583  d5.loss_cls: 0.5685  d5.loss_mask: 0.1765  d5.loss_dice: 1.7462  d6.loss_cls: 0.5655  d6.loss_mask: 0.1766  d6.loss_dice: 1.7446  d7.loss_cls: 0.5723  d7.loss_mask: 0.1741  d7.loss_dice: 1.7309  d8.loss_cls: 0.5704  d8.loss_mask: 0.1719  d8.loss_dice: 1.7118
05/11 13:29:11 - mmengine - INFO - Iter(train) [16550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:15:56  time: 1.5228  data_time: 0.0293  memory: 38947  grad_norm: 44.7031  loss: 26.9034  loss_cls: 0.5778  loss_mask: 0.1684  loss_dice: 1.7579  d0.loss_cls: 1.1030  d0.loss_mask: 0.2230  d0.loss_dice: 1.9940  d1.loss_cls: 0.8326  d1.loss_mask: 0.2126  d1.loss_dice: 1.9798  d2.loss_cls: 0.6590  d2.loss_mask: 0.1905  d2.loss_dice: 1.9134  d3.loss_cls: 0.5984  d3.loss_mask: 0.1797  d3.loss_dice: 1.8536  d4.loss_cls: 0.5798  d4.loss_mask: 0.1742  d4.loss_dice: 1.8070  d5.loss_cls: 0.5695  d5.loss_mask: 0.1727  d5.loss_dice: 1.7929  d6.loss_cls: 0.5680  d6.loss_mask: 0.1729  d6.loss_dice: 1.7937  d7.loss_cls: 0.5813  d7.loss_mask: 0.1709  d7.loss_dice: 1.7742  d8.loss_cls: 0.5804  d8.loss_mask: 0.1688  d8.loss_dice: 1.7533
05/11 13:30:28 - mmengine - INFO - Iter(train) [16600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:14:36  time: 1.5363  data_time: 0.0288  memory: 38816  grad_norm: 40.1592  loss: 25.3460  loss_cls: 0.5623  loss_mask: 0.1559  loss_dice: 1.6438  d0.loss_cls: 1.0793  d0.loss_mask: 0.2034  d0.loss_dice: 1.8617  d1.loss_cls: 0.8105  d1.loss_mask: 0.1894  d1.loss_dice: 1.8351  d2.loss_cls: 0.6441  d2.loss_mask: 0.1718  d2.loss_dice: 1.7910  d3.loss_cls: 0.5846  d3.loss_mask: 0.1649  d3.loss_dice: 1.7294  d4.loss_cls: 0.5654  d4.loss_mask: 0.1604  d4.loss_dice: 1.6800  d5.loss_cls: 0.5557  d5.loss_mask: 0.1600  d5.loss_dice: 1.6689  d6.loss_cls: 0.5551  d6.loss_mask: 0.1599  d6.loss_dice: 1.6698  d7.loss_cls: 0.5636  d7.loss_mask: 0.1588  d7.loss_dice: 1.6576  d8.loss_cls: 0.5639  d8.loss_mask: 0.1554  d8.loss_dice: 1.6444
05/11 13:31:44 - mmengine - INFO - Iter(train) [16650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:13:16  time: 1.5232  data_time: 0.0295  memory: 38973  grad_norm: 37.4202  loss: 27.6030  loss_cls: 0.5893  loss_mask: 0.1823  loss_dice: 1.8080  d0.loss_cls: 1.1044  d0.loss_mask: 0.2290  d0.loss_dice: 2.0418  d1.loss_cls: 0.8203  d1.loss_mask: 0.2164  d1.loss_dice: 2.0325  d2.loss_cls: 0.6599  d2.loss_mask: 0.1990  d2.loss_dice: 1.9813  d3.loss_cls: 0.6032  d3.loss_mask: 0.1924  d3.loss_dice: 1.9083  d4.loss_cls: 0.5866  d4.loss_mask: 0.1883  d4.loss_dice: 1.8619  d5.loss_cls: 0.5862  d5.loss_mask: 0.1857  d5.loss_dice: 1.8407  d6.loss_cls: 0.5768  d6.loss_mask: 0.1862  d6.loss_dice: 1.8415  d7.loss_cls: 0.5936  d7.loss_mask: 0.1842  d7.loss_dice: 1.8225  d8.loss_cls: 0.5917  d8.loss_mask: 0.1826  d8.loss_dice: 1.8063
05/11 13:33:01 - mmengine - INFO - Iter(train) [16700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:11:56  time: 1.5377  data_time: 0.0312  memory: 39502  grad_norm: 43.3107  loss: 29.1444  loss_cls: 0.5901  loss_mask: 0.1743  loss_dice: 1.9560  d0.loss_cls: 1.1438  d0.loss_mask: 0.2190  d0.loss_dice: 2.2095  d1.loss_cls: 0.8469  d1.loss_mask: 0.2111  d1.loss_dice: 2.2047  d2.loss_cls: 0.6776  d2.loss_mask: 0.1911  d2.loss_dice: 2.1418  d3.loss_cls: 0.6204  d3.loss_mask: 0.1831  d3.loss_dice: 2.0593  d4.loss_cls: 0.6003  d4.loss_mask: 0.1780  d4.loss_dice: 1.9914  d5.loss_cls: 0.5885  d5.loss_mask: 0.1776  d5.loss_dice: 1.9805  d6.loss_cls: 0.5878  d6.loss_mask: 0.1784  d6.loss_dice: 1.9787  d7.loss_cls: 0.5970  d7.loss_mask: 0.1757  d7.loss_dice: 1.9652  d8.loss_cls: 0.5921  d8.loss_mask: 0.1741  d8.loss_dice: 1.9505
05/11 13:34:18 - mmengine - INFO - Iter(train) [16750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:10:36  time: 1.5319  data_time: 0.0303  memory: 39265  grad_norm: 36.5087  loss: 27.1904  loss_cls: 0.5786  loss_mask: 0.1755  loss_dice: 1.7836  d0.loss_cls: 1.1059  d0.loss_mask: 0.2261  d0.loss_dice: 2.0088  d1.loss_cls: 0.8251  d1.loss_mask: 0.2110  d1.loss_dice: 1.9998  d2.loss_cls: 0.6577  d2.loss_mask: 0.1940  d2.loss_dice: 1.9386  d3.loss_cls: 0.6052  d3.loss_mask: 0.1845  d3.loss_dice: 1.8766  d4.loss_cls: 0.5829  d4.loss_mask: 0.1817  d4.loss_dice: 1.8219  d5.loss_cls: 0.5741  d5.loss_mask: 0.1794  d5.loss_dice: 1.8143  d6.loss_cls: 0.5776  d6.loss_mask: 0.1784  d6.loss_dice: 1.8093  d7.loss_cls: 0.5875  d7.loss_mask: 0.1768  d7.loss_dice: 1.7962  d8.loss_cls: 0.5842  d8.loss_mask: 0.1761  d8.loss_dice: 1.7792
05/11 13:35:34 - mmengine - INFO - Iter(train) [16800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:09:16  time: 1.5137  data_time: 0.0301  memory: 38652  grad_norm: 45.0297  loss: 27.9701  loss_cls: 0.5950  loss_mask: 0.1922  loss_dice: 1.8326  d0.loss_cls: 1.0912  d0.loss_mask: 0.2414  d0.loss_dice: 2.0644  d1.loss_cls: 0.8316  d1.loss_mask: 0.2246  d1.loss_dice: 2.0558  d2.loss_cls: 0.6544  d2.loss_mask: 0.2091  d2.loss_dice: 2.0007  d3.loss_cls: 0.6079  d3.loss_mask: 0.2019  d3.loss_dice: 1.9347  d4.loss_cls: 0.5972  d4.loss_mask: 0.1980  d4.loss_dice: 1.8773  d5.loss_cls: 0.5853  d5.loss_mask: 0.1953  d5.loss_dice: 1.8716  d6.loss_cls: 0.5851  d6.loss_mask: 0.1963  d6.loss_dice: 1.8652  d7.loss_cls: 0.5932  d7.loss_mask: 0.1945  d7.loss_dice: 1.8527  d8.loss_cls: 0.5895  d8.loss_mask: 0.1928  d8.loss_dice: 1.8387
05/11 13:36:50 - mmengine - INFO - Iter(train) [16850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:07:56  time: 1.5369  data_time: 0.0361  memory: 38553  grad_norm: 39.8565  loss: 27.4683  loss_cls: 0.5857  loss_mask: 0.1653  loss_dice: 1.8146  d0.loss_cls: 1.1029  d0.loss_mask: 0.2086  d0.loss_dice: 2.0441  d1.loss_cls: 0.8344  d1.loss_mask: 0.2021  d1.loss_dice: 2.0350  d2.loss_cls: 0.6627  d2.loss_mask: 0.1849  d2.loss_dice: 1.9831  d3.loss_cls: 0.6090  d3.loss_mask: 0.1752  d3.loss_dice: 1.9019  d4.loss_cls: 0.5880  d4.loss_mask: 0.1702  d4.loss_dice: 1.8590  d5.loss_cls: 0.5770  d5.loss_mask: 0.1700  d5.loss_dice: 1.8504  d6.loss_cls: 0.5782  d6.loss_mask: 0.1686  d6.loss_dice: 1.8446  d7.loss_cls: 0.5840  d7.loss_mask: 0.1667  d7.loss_dice: 1.8358  d8.loss_cls: 0.5881  d8.loss_mask: 0.1665  d8.loss_dice: 1.8119
05/11 13:38:06 - mmengine - INFO - Iter(train) [16900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:06:36  time: 1.5127  data_time: 0.0284  memory: 39384  grad_norm: 39.8891  loss: 26.4842  loss_cls: 0.5563  loss_mask: 0.1609  loss_dice: 1.7438  d0.loss_cls: 1.0991  d0.loss_mask: 0.2062  d0.loss_dice: 1.9730  d1.loss_cls: 0.8090  d1.loss_mask: 0.1941  d1.loss_dice: 1.9688  d2.loss_cls: 0.6359  d2.loss_mask: 0.1777  d2.loss_dice: 1.9130  d3.loss_cls: 0.5839  d3.loss_mask: 0.1705  d3.loss_dice: 1.8375  d4.loss_cls: 0.5666  d4.loss_mask: 0.1665  d4.loss_dice: 1.7871  d5.loss_cls: 0.5550  d5.loss_mask: 0.1650  d5.loss_dice: 1.7802  d6.loss_cls: 0.5503  d6.loss_mask: 0.1647  d6.loss_dice: 1.7781  d7.loss_cls: 0.5565  d7.loss_mask: 0.1639  d7.loss_dice: 1.7622  d8.loss_cls: 0.5595  d8.loss_mask: 0.1625  d8.loss_dice: 1.7366
05/11 13:39:22 - mmengine - INFO - Iter(train) [16950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:05:16  time: 1.5267  data_time: 0.0278  memory: 38788  grad_norm: 38.7339  loss: 25.7337  loss_cls: 0.5598  loss_mask: 0.1621  loss_dice: 1.6864  d0.loss_cls: 1.0788  d0.loss_mask: 0.2116  d0.loss_dice: 1.8932  d1.loss_cls: 0.8027  d1.loss_mask: 0.1971  d1.loss_dice: 1.8764  d2.loss_cls: 0.6203  d2.loss_mask: 0.1806  d2.loss_dice: 1.8240  d3.loss_cls: 0.5749  d3.loss_mask: 0.1727  d3.loss_dice: 1.7669  d4.loss_cls: 0.5649  d4.loss_mask: 0.1685  d4.loss_dice: 1.7185  d5.loss_cls: 0.5566  d5.loss_mask: 0.1658  d5.loss_dice: 1.7120  d6.loss_cls: 0.5507  d6.loss_mask: 0.1650  d6.loss_dice: 1.7077  d7.loss_cls: 0.5602  d7.loss_mask: 0.1638  d7.loss_dice: 1.6937  d8.loss_cls: 0.5544  d8.loss_mask: 0.1615  d8.loss_dice: 1.6830
05/11 13:40:38 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 13:40:38 - mmengine - INFO - Iter(train) [17000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:03:56  time: 1.5168  data_time: 0.0275  memory: 39560  grad_norm: 42.7997  loss: 26.6166  loss_cls: 0.5670  loss_mask: 0.1699  loss_dice: 1.7485  d0.loss_cls: 1.1034  d0.loss_mask: 0.2143  d0.loss_dice: 1.9671  d1.loss_cls: 0.8035  d1.loss_mask: 0.2044  d1.loss_dice: 1.9645  d2.loss_cls: 0.6256  d2.loss_mask: 0.1872  d2.loss_dice: 1.8998  d3.loss_cls: 0.5867  d3.loss_mask: 0.1810  d3.loss_dice: 1.8290  d4.loss_cls: 0.5764  d4.loss_mask: 0.1756  d4.loss_dice: 1.7906  d5.loss_cls: 0.5623  d5.loss_mask: 0.1745  d5.loss_dice: 1.7824  d6.loss_cls: 0.5597  d6.loss_mask: 0.1732  d6.loss_dice: 1.7835  d7.loss_cls: 0.5676  d7.loss_mask: 0.1721  d7.loss_dice: 1.7634  d8.loss_cls: 0.5608  d8.loss_mask: 0.1703  d8.loss_dice: 1.7522
05/11 13:41:55 - mmengine - INFO - Iter(train) [17050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:02:36  time: 1.5316  data_time: 0.0281  memory: 39479  grad_norm: 41.8052  loss: 28.0055  loss_cls: 0.5771  loss_mask: 0.1673  loss_dice: 1.8682  d0.loss_cls: 1.1269  d0.loss_mask: 0.2166  d0.loss_dice: 2.1198  d1.loss_cls: 0.8250  d1.loss_mask: 0.2050  d1.loss_dice: 2.0929  d2.loss_cls: 0.6497  d2.loss_mask: 0.1844  d2.loss_dice: 2.0255  d3.loss_cls: 0.6046  d3.loss_mask: 0.1757  d3.loss_dice: 1.9589  d4.loss_cls: 0.6060  d4.loss_mask: 0.1707  d4.loss_dice: 1.9139  d5.loss_cls: 0.5675  d5.loss_mask: 0.1711  d5.loss_dice: 1.9063  d6.loss_cls: 0.5671  d6.loss_mask: 0.1704  d6.loss_dice: 1.8976  d7.loss_cls: 0.5733  d7.loss_mask: 0.1691  d7.loss_dice: 1.8876  d8.loss_cls: 0.5715  d8.loss_mask: 0.1668  d8.loss_dice: 1.8690
05/11 13:43:12 - mmengine - INFO - Iter(train) [17100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:01:17  time: 1.5395  data_time: 0.0286  memory: 39326  grad_norm: 51.0890  loss: 27.0627  loss_cls: 0.5954  loss_mask: 0.1697  loss_dice: 1.7544  d0.loss_cls: 1.1263  d0.loss_mask: 0.2203  d0.loss_dice: 1.9980  d1.loss_cls: 0.8394  d1.loss_mask: 0.2110  d1.loss_dice: 1.9802  d2.loss_cls: 0.6712  d2.loss_mask: 0.1891  d2.loss_dice: 1.9146  d3.loss_cls: 0.6197  d3.loss_mask: 0.1821  d3.loss_dice: 1.8460  d4.loss_cls: 0.6114  d4.loss_mask: 0.1775  d4.loss_dice: 1.7974  d5.loss_cls: 0.5923  d5.loss_mask: 0.1740  d5.loss_dice: 1.7903  d6.loss_cls: 0.5850  d6.loss_mask: 0.1744  d6.loss_dice: 1.7839  d7.loss_cls: 0.5983  d7.loss_mask: 0.1714  d7.loss_dice: 1.7695  d8.loss_cls: 0.5998  d8.loss_mask: 0.1706  d8.loss_dice: 1.7493
05/11 13:44:28 - mmengine - INFO - Iter(train) [17150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:59:57  time: 1.5192  data_time: 0.0281  memory: 38917  grad_norm: 48.3992  loss: 25.8922  loss_cls: 0.5671  loss_mask: 0.1482  loss_dice: 1.7010  d0.loss_cls: 1.0883  d0.loss_mask: 0.1925  d0.loss_dice: 1.8854  d1.loss_cls: 0.8114  d1.loss_mask: 0.1794  d1.loss_dice: 1.8989  d2.loss_cls: 0.6450  d2.loss_mask: 0.1656  d2.loss_dice: 1.8473  d3.loss_cls: 0.5970  d3.loss_mask: 0.1587  d3.loss_dice: 1.7859  d4.loss_cls: 0.5865  d4.loss_mask: 0.1541  d4.loss_dice: 1.7287  d5.loss_cls: 0.5714  d5.loss_mask: 0.1528  d5.loss_dice: 1.7332  d6.loss_cls: 0.5630  d6.loss_mask: 0.1514  d6.loss_dice: 1.7300  d7.loss_cls: 0.5688  d7.loss_mask: 0.1496  d7.loss_dice: 1.7119  d8.loss_cls: 0.5698  d8.loss_mask: 0.1485  d8.loss_dice: 1.7008
05/11 13:45:44 - mmengine - INFO - Iter(train) [17200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:58:37  time: 1.5185  data_time: 0.0285  memory: 38726  grad_norm: 44.0262  loss: 28.2746  loss_cls: 0.5850  loss_mask: 0.1635  loss_dice: 1.8811  d0.loss_cls: 1.1184  d0.loss_mask: 0.2094  d0.loss_dice: 2.1232  d1.loss_cls: 0.8513  d1.loss_mask: 0.1991  d1.loss_dice: 2.1253  d2.loss_cls: 0.6776  d2.loss_mask: 0.1837  d2.loss_dice: 2.0537  d3.loss_cls: 0.6134  d3.loss_mask: 0.1732  d3.loss_dice: 1.9832  d4.loss_cls: 0.6103  d4.loss_mask: 0.1678  d4.loss_dice: 1.9338  d5.loss_cls: 0.5851  d5.loss_mask: 0.1664  d5.loss_dice: 1.9238  d6.loss_cls: 0.5764  d6.loss_mask: 0.1670  d6.loss_dice: 1.9204  d7.loss_cls: 0.5803  d7.loss_mask: 0.1657  d7.loss_dice: 1.9014  d8.loss_cls: 0.5887  d8.loss_mask: 0.1635  d8.loss_dice: 1.8828
05/11 13:47:00 - mmengine - INFO - Iter(train) [17250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:57:17  time: 1.5262  data_time: 0.0290  memory: 38726  grad_norm: 44.2695  loss: 27.3041  loss_cls: 0.5657  loss_mask: 0.1710  loss_dice: 1.8111  d0.loss_cls: 1.1112  d0.loss_mask: 0.2190  d0.loss_dice: 2.0222  d1.loss_cls: 0.8191  d1.loss_mask: 0.2054  d1.loss_dice: 2.0217  d2.loss_cls: 0.6440  d2.loss_mask: 0.1859  d2.loss_dice: 1.9621  d3.loss_cls: 0.5948  d3.loss_mask: 0.1787  d3.loss_dice: 1.9017  d4.loss_cls: 0.5832  d4.loss_mask: 0.1758  d4.loss_dice: 1.8535  d5.loss_cls: 0.5693  d5.loss_mask: 0.1746  d5.loss_dice: 1.8385  d6.loss_cls: 0.5670  d6.loss_mask: 0.1724  d6.loss_dice: 1.8355  d7.loss_cls: 0.5687  d7.loss_mask: 0.1718  d7.loss_dice: 1.8254  d8.loss_cls: 0.5744  d8.loss_mask: 0.1709  d8.loss_dice: 1.8098
05/11 13:48:16 - mmengine - INFO - Iter(train) [17300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:55:57  time: 1.5123  data_time: 0.0292  memory: 38570  grad_norm: 49.8191  loss: 27.7586  loss_cls: 0.5662  loss_mask: 0.1912  loss_dice: 1.8209  d0.loss_cls: 1.1241  d0.loss_mask: 0.2539  d0.loss_dice: 2.0628  d1.loss_cls: 0.8211  d1.loss_mask: 0.2327  d1.loss_dice: 2.0665  d2.loss_cls: 0.6510  d2.loss_mask: 0.2106  d2.loss_dice: 1.9848  d3.loss_cls: 0.5930  d3.loss_mask: 0.2041  d3.loss_dice: 1.9163  d4.loss_cls: 0.5869  d4.loss_mask: 0.1987  d4.loss_dice: 1.8558  d5.loss_cls: 0.5736  d5.loss_mask: 0.1968  d5.loss_dice: 1.8523  d6.loss_cls: 0.5627  d6.loss_mask: 0.1969  d6.loss_dice: 1.8533  d7.loss_cls: 0.5704  d7.loss_mask: 0.1951  d7.loss_dice: 1.8382  d8.loss_cls: 0.5672  d8.loss_mask: 0.1925  d8.loss_dice: 1.8192
05/11 13:49:32 - mmengine - INFO - Iter(train) [17350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:54:38  time: 1.5371  data_time: 0.0291  memory: 39596  grad_norm: 49.7510  loss: 28.3621  loss_cls: 0.5936  loss_mask: 0.1876  loss_dice: 1.8718  d0.loss_cls: 1.1264  d0.loss_mask: 0.2361  d0.loss_dice: 2.1180  d1.loss_cls: 0.8321  d1.loss_mask: 0.2211  d1.loss_dice: 2.1048  d2.loss_cls: 0.6578  d2.loss_mask: 0.2050  d2.loss_dice: 2.0308  d3.loss_cls: 0.6094  d3.loss_mask: 0.1976  d3.loss_dice: 1.9632  d4.loss_cls: 0.6101  d4.loss_mask: 0.1944  d4.loss_dice: 1.9023  d5.loss_cls: 0.5974  d5.loss_mask: 0.1937  d5.loss_dice: 1.9032  d6.loss_cls: 0.5886  d6.loss_mask: 0.1937  d6.loss_dice: 1.9002  d7.loss_cls: 0.5952  d7.loss_mask: 0.1908  d7.loss_dice: 1.8837  d8.loss_cls: 0.5915  d8.loss_mask: 0.1908  d8.loss_dice: 1.8712
05/11 13:50:49 - mmengine - INFO - Iter(train) [17400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:53:18  time: 1.5385  data_time: 0.0285  memory: 38914  grad_norm: 50.6112  loss: 27.2198  loss_cls: 0.5976  loss_mask: 0.1669  loss_dice: 1.7681  d0.loss_cls: 1.1199  d0.loss_mask: 0.2198  d0.loss_dice: 2.0072  d1.loss_cls: 0.8507  d1.loss_mask: 0.2095  d1.loss_dice: 2.0068  d2.loss_cls: 0.6699  d2.loss_mask: 0.1851  d2.loss_dice: 1.9329  d3.loss_cls: 0.6248  d3.loss_mask: 0.1772  d3.loss_dice: 1.8612  d4.loss_cls: 0.6228  d4.loss_mask: 0.1720  d4.loss_dice: 1.8159  d5.loss_cls: 0.5929  d5.loss_mask: 0.1726  d5.loss_dice: 1.8004  d6.loss_cls: 0.5919  d6.loss_mask: 0.1703  d6.loss_dice: 1.7993  d7.loss_cls: 0.6018  d7.loss_mask: 0.1691  d7.loss_dice: 1.7785  d8.loss_cls: 0.6005  d8.loss_mask: 0.1674  d8.loss_dice: 1.7668
05/11 13:52:06 - mmengine - INFO - Iter(train) [17450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:51:58  time: 1.5296  data_time: 0.0283  memory: 38478  grad_norm: 42.6232  loss: 26.8449  loss_cls: 0.5971  loss_mask: 0.1740  loss_dice: 1.7430  d0.loss_cls: 1.1026  d0.loss_mask: 0.2278  d0.loss_dice: 1.9506  d1.loss_cls: 0.8388  d1.loss_mask: 0.2110  d1.loss_dice: 1.9386  d2.loss_cls: 0.6639  d2.loss_mask: 0.1904  d2.loss_dice: 1.8747  d3.loss_cls: 0.6235  d3.loss_mask: 0.1831  d3.loss_dice: 1.8211  d4.loss_cls: 0.6027  d4.loss_mask: 0.1790  d4.loss_dice: 1.7843  d5.loss_cls: 0.5913  d5.loss_mask: 0.1785  d5.loss_dice: 1.7722  d6.loss_cls: 0.5894  d6.loss_mask: 0.1780  d6.loss_dice: 1.7717  d7.loss_cls: 0.6039  d7.loss_mask: 0.1769  d7.loss_dice: 1.7553  d8.loss_cls: 0.6018  d8.loss_mask: 0.1738  d8.loss_dice: 1.7459
05/11 13:53:22 - mmengine - INFO - Iter(train) [17500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:50:39  time: 1.5219  data_time: 0.0288  memory: 38600  grad_norm: 45.2743  loss: 26.9188  loss_cls: 0.5744  loss_mask: 0.1691  loss_dice: 1.7658  d0.loss_cls: 1.1050  d0.loss_mask: 0.2207  d0.loss_dice: 1.9947  d1.loss_cls: 0.8313  d1.loss_mask: 0.2050  d1.loss_dice: 1.9833  d2.loss_cls: 0.6521  d2.loss_mask: 0.1847  d2.loss_dice: 1.9162  d3.loss_cls: 0.5982  d3.loss_mask: 0.1812  d3.loss_dice: 1.8574  d4.loss_cls: 0.5881  d4.loss_mask: 0.1753  d4.loss_dice: 1.8042  d5.loss_cls: 0.5704  d5.loss_mask: 0.1735  d5.loss_dice: 1.7951  d6.loss_cls: 0.5654  d6.loss_mask: 0.1716  d6.loss_dice: 1.7923  d7.loss_cls: 0.5772  d7.loss_mask: 0.1702  d7.loss_dice: 1.7806  d8.loss_cls: 0.5802  d8.loss_mask: 0.1694  d8.loss_dice: 1.7663
05/11 13:54:39 - mmengine - INFO - Iter(train) [17550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:49:19  time: 1.5334  data_time: 0.0293  memory: 38919  grad_norm: 42.7030  loss: 28.5070  loss_cls: 0.5880  loss_mask: 0.1729  loss_dice: 1.9084  d0.loss_cls: 1.1192  d0.loss_mask: 0.2210  d0.loss_dice: 2.1427  d1.loss_cls: 0.8480  d1.loss_mask: 0.2140  d1.loss_dice: 2.1035  d2.loss_cls: 0.6706  d2.loss_mask: 0.1907  d2.loss_dice: 2.0498  d3.loss_cls: 0.6129  d3.loss_mask: 0.1841  d3.loss_dice: 1.9946  d4.loss_cls: 0.5946  d4.loss_mask: 0.1787  d4.loss_dice: 1.9589  d5.loss_cls: 0.5815  d5.loss_mask: 0.1783  d5.loss_dice: 1.9409  d6.loss_cls: 0.5818  d6.loss_mask: 0.1781  d6.loss_dice: 1.9376  d7.loss_cls: 0.5889  d7.loss_mask: 0.1761  d7.loss_dice: 1.9219  d8.loss_cls: 0.5916  d8.loss_mask: 0.1741  d8.loss_dice: 1.9039
05/11 13:55:55 - mmengine - INFO - Iter(train) [17600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:48:00  time: 1.5271  data_time: 0.0287  memory: 39457  grad_norm: 49.0422  loss: 27.8293  loss_cls: 0.5827  loss_mask: 0.1744  loss_dice: 1.8381  d0.loss_cls: 1.1321  d0.loss_mask: 0.2205  d0.loss_dice: 2.0783  d1.loss_cls: 0.8407  d1.loss_mask: 0.2086  d1.loss_dice: 2.0642  d2.loss_cls: 0.6533  d2.loss_mask: 0.1921  d2.loss_dice: 1.9922  d3.loss_cls: 0.6025  d3.loss_mask: 0.1823  d3.loss_dice: 1.9298  d4.loss_cls: 0.5859  d4.loss_mask: 0.1793  d4.loss_dice: 1.8887  d5.loss_cls: 0.5725  d5.loss_mask: 0.1777  d5.loss_dice: 1.8784  d6.loss_cls: 0.5735  d6.loss_mask: 0.1771  d6.loss_dice: 1.8749  d7.loss_cls: 0.5927  d7.loss_mask: 0.1748  d7.loss_dice: 1.8575  d8.loss_cls: 0.5866  d8.loss_mask: 0.1747  d8.loss_dice: 1.8432
05/11 13:57:11 - mmengine - INFO - Iter(train) [17650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:46:40  time: 1.5253  data_time: 0.0286  memory: 38603  grad_norm: 51.5523  loss: 26.6994  loss_cls: 0.5829  loss_mask: 0.1730  loss_dice: 1.7382  d0.loss_cls: 1.0827  d0.loss_mask: 0.2200  d0.loss_dice: 1.9766  d1.loss_cls: 0.8102  d1.loss_mask: 0.2077  d1.loss_dice: 1.9505  d2.loss_cls: 0.6607  d2.loss_mask: 0.1916  d2.loss_dice: 1.8807  d3.loss_cls: 0.6068  d3.loss_mask: 0.1838  d3.loss_dice: 1.8250  d4.loss_cls: 0.5828  d4.loss_mask: 0.1805  d4.loss_dice: 1.7808  d5.loss_cls: 0.5751  d5.loss_mask: 0.1796  d5.loss_dice: 1.7659  d6.loss_cls: 0.5744  d6.loss_mask: 0.1784  d6.loss_dice: 1.7745  d7.loss_cls: 0.5868  d7.loss_mask: 0.1755  d7.loss_dice: 1.7557  d8.loss_cls: 0.5862  d8.loss_mask: 0.1736  d8.loss_dice: 1.7390
05/11 13:58:28 - mmengine - INFO - Iter(train) [17700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:45:20  time: 1.5252  data_time: 0.0270  memory: 38829  grad_norm: 47.1102  loss: 26.8799  loss_cls: 0.5770  loss_mask: 0.1721  loss_dice: 1.7575  d0.loss_cls: 1.0991  d0.loss_mask: 0.2283  d0.loss_dice: 2.0051  d1.loss_cls: 0.8190  d1.loss_mask: 0.2070  d1.loss_dice: 1.9912  d2.loss_cls: 0.6568  d2.loss_mask: 0.1934  d2.loss_dice: 1.9013  d3.loss_cls: 0.5897  d3.loss_mask: 0.1805  d3.loss_dice: 1.8472  d4.loss_cls: 0.5722  d4.loss_mask: 0.1761  d4.loss_dice: 1.8043  d5.loss_cls: 0.5614  d5.loss_mask: 0.1771  d5.loss_dice: 1.7939  d6.loss_cls: 0.5662  d6.loss_mask: 0.1759  d6.loss_dice: 1.7949  d7.loss_cls: 0.5793  d7.loss_mask: 0.1740  d7.loss_dice: 1.7699  d8.loss_cls: 0.5784  d8.loss_mask: 0.1725  d8.loss_dice: 1.7586
05/11 13:59:44 - mmengine - INFO - Iter(train) [17750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:44:01  time: 1.5197  data_time: 0.0274  memory: 39140  grad_norm: 53.9128  loss: 26.8504  loss_cls: 0.5775  loss_mask: 0.1700  loss_dice: 1.7552  d0.loss_cls: 1.1075  d0.loss_mask: 0.2228  d0.loss_dice: 1.9965  d1.loss_cls: 0.8182  d1.loss_mask: 0.2109  d1.loss_dice: 1.9848  d2.loss_cls: 0.6545  d2.loss_mask: 0.1865  d2.loss_dice: 1.9199  d3.loss_cls: 0.5904  d3.loss_mask: 0.1782  d3.loss_dice: 1.8531  d4.loss_cls: 0.5758  d4.loss_mask: 0.1737  d4.loss_dice: 1.7996  d5.loss_cls: 0.5680  d5.loss_mask: 0.1727  d5.loss_dice: 1.7862  d6.loss_cls: 0.5625  d6.loss_mask: 0.1730  d6.loss_dice: 1.7861  d7.loss_cls: 0.5778  d7.loss_mask: 0.1724  d7.loss_dice: 1.7715  d8.loss_cls: 0.5806  d8.loss_mask: 0.1708  d8.loss_dice: 1.7538
05/11 14:01:00 - mmengine - INFO - Iter(train) [17800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:42:41  time: 1.5357  data_time: 0.0290  memory: 39079  grad_norm: 44.0133  loss: 27.8423  loss_cls: 0.5824  loss_mask: 0.1744  loss_dice: 1.8576  d0.loss_cls: 1.1204  d0.loss_mask: 0.2256  d0.loss_dice: 2.0707  d1.loss_cls: 0.8264  d1.loss_mask: 0.2131  d1.loss_dice: 2.0845  d2.loss_cls: 0.6674  d2.loss_mask: 0.1938  d2.loss_dice: 2.0086  d3.loss_cls: 0.6005  d3.loss_mask: 0.1821  d3.loss_dice: 1.9371  d4.loss_cls: 0.5738  d4.loss_mask: 0.1765  d4.loss_dice: 1.8872  d5.loss_cls: 0.5638  d5.loss_mask: 0.1756  d5.loss_dice: 1.8769  d6.loss_cls: 0.5673  d6.loss_mask: 0.1745  d6.loss_dice: 1.8724  d7.loss_cls: 0.5781  d7.loss_mask: 0.1743  d7.loss_dice: 1.8702  d8.loss_cls: 0.5880  d8.loss_mask: 0.1747  d8.loss_dice: 1.8445
05/11 14:02:17 - mmengine - INFO - Iter(train) [17850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:41:22  time: 1.5388  data_time: 0.0283  memory: 38457  grad_norm: 49.8350  loss: 27.8672  loss_cls: 0.6016  loss_mask: 0.1736  loss_dice: 1.8410  d0.loss_cls: 1.1058  d0.loss_mask: 0.2272  d0.loss_dice: 2.0707  d1.loss_cls: 0.8370  d1.loss_mask: 0.2115  d1.loss_dice: 2.0711  d2.loss_cls: 0.6763  d2.loss_mask: 0.1926  d2.loss_dice: 1.9917  d3.loss_cls: 0.6159  d3.loss_mask: 0.1832  d3.loss_dice: 1.9205  d4.loss_cls: 0.5963  d4.loss_mask: 0.1799  d4.loss_dice: 1.8713  d5.loss_cls: 0.5884  d5.loss_mask: 0.1794  d5.loss_dice: 1.8611  d6.loss_cls: 0.5825  d6.loss_mask: 0.1777  d6.loss_dice: 1.8685  d7.loss_cls: 0.6010  d7.loss_mask: 0.1748  d7.loss_dice: 1.8523  d8.loss_cls: 0.6019  d8.loss_mask: 0.1740  d8.loss_dice: 1.8385
05/11 14:03:34 - mmengine - INFO - Iter(train) [17900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:40:03  time: 1.5445  data_time: 0.0285  memory: 38634  grad_norm: 46.7762  loss: 25.8104  loss_cls: 0.5818  loss_mask: 0.1688  loss_dice: 1.6473  d0.loss_cls: 1.1018  d0.loss_mask: 0.2202  d0.loss_dice: 1.8930  d1.loss_cls: 0.8322  d1.loss_mask: 0.2060  d1.loss_dice: 1.8762  d2.loss_cls: 0.6548  d2.loss_mask: 0.1869  d2.loss_dice: 1.8059  d3.loss_cls: 0.6025  d3.loss_mask: 0.1806  d3.loss_dice: 1.7394  d4.loss_cls: 0.5827  d4.loss_mask: 0.1734  d4.loss_dice: 1.6893  d5.loss_cls: 0.5715  d5.loss_mask: 0.1735  d5.loss_dice: 1.6878  d6.loss_cls: 0.5731  d6.loss_mask: 0.1720  d6.loss_dice: 1.6762  d7.loss_cls: 0.5799  d7.loss_mask: 0.1696  d7.loss_dice: 1.6657  d8.loss_cls: 0.5810  d8.loss_mask: 0.1675  d8.loss_dice: 1.6499
05/11 14:04:51 - mmengine - INFO - Iter(train) [17950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:38:43  time: 1.5215  data_time: 0.0288  memory: 39277  grad_norm: 56.5990  loss: 27.1864  loss_cls: 0.5750  loss_mask: 0.1685  loss_dice: 1.7816  d0.loss_cls: 1.1095  d0.loss_mask: 0.2161  d0.loss_dice: 2.0397  d1.loss_cls: 0.8255  d1.loss_mask: 0.2072  d1.loss_dice: 2.0255  d2.loss_cls: 0.6466  d2.loss_mask: 0.1881  d2.loss_dice: 1.9557  d3.loss_cls: 0.5994  d3.loss_mask: 0.1817  d3.loss_dice: 1.8818  d4.loss_cls: 0.5778  d4.loss_mask: 0.1753  d4.loss_dice: 1.8286  d5.loss_cls: 0.5668  d5.loss_mask: 0.1740  d5.loss_dice: 1.8201  d6.loss_cls: 0.5654  d6.loss_mask: 0.1726  d6.loss_dice: 1.8182  d7.loss_cls: 0.5796  d7.loss_mask: 0.1711  d7.loss_dice: 1.8023  d8.loss_cls: 0.5786  d8.loss_mask: 0.1686  d8.loss_dice: 1.7857
05/11 14:06:08 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 14:06:08 - mmengine - INFO - Iter(train) [18000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:37:24  time: 1.5441  data_time: 0.0289  memory: 39136  grad_norm: 58.3908  loss: 28.3029  loss_cls: 0.5980  loss_mask: 0.1865  loss_dice: 1.8433  d0.loss_cls: 1.1399  d0.loss_mask: 0.2466  d0.loss_dice: 2.1243  d1.loss_cls: 0.8380  d1.loss_mask: 0.2275  d1.loss_dice: 2.1207  d2.loss_cls: 0.6747  d2.loss_mask: 0.2075  d2.loss_dice: 2.0275  d3.loss_cls: 0.6231  d3.loss_mask: 0.1972  d3.loss_dice: 1.9528  d4.loss_cls: 0.6049  d4.loss_mask: 0.1912  d4.loss_dice: 1.8964  d5.loss_cls: 0.5967  d5.loss_mask: 0.1924  d5.loss_dice: 1.8798  d6.loss_cls: 0.5910  d6.loss_mask: 0.1919  d6.loss_dice: 1.8734  d7.loss_cls: 0.5955  d7.loss_mask: 0.1884  d7.loss_dice: 1.8643  d8.loss_cls: 0.5975  d8.loss_mask: 0.1865  d8.loss_dice: 1.8453
05/11 14:06:08 - mmengine - INFO - Saving checkpoint at 18000 iterations
05/11 14:06:51 - mmengine - INFO - Iter(val) [ 50/100]    eta: 0:00:36  time: 0.7278  data_time: 0.0142  memory: 5704  
05/11 14:07:27 - mmengine - INFO - Iter(val) [100/100]    eta: 0:00:00  time: 0.7290  data_time: 0.0136  memory: 5704  
05/11 14:07:51 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.21s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/11 14:07:58 - mmengine - INFO - start multi processing evaluation ...

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

  0%|          | 0/25552 [00:00<?, ?it/s]
 50%|█████     | 12778/25552 [00:00<00:00, 124742.09it/s]
100%|██████████| 25552/25552 [00:00<00:00, 138543.01it/s]
DONE (t=52.59s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.403
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.789
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.385
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.242
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.473
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.925
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.527
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.922
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.539
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.390
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.589
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.950
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.527
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.527
05/11 14:08:51 - mmengine - INFO - segm_mAP_copypaste: 0.403 0.789 0.385 0.242 0.473 0.925
05/11 14:08:51 - mmengine - INFO - segm_mAR_copypaste: 0.527 0.922 0.539 0.390 0.589 0.950
05/11 14:08:52 - mmengine - INFO - Iter(val) [100/100]    coco/segm_mAP: 0.4030  coco/segm_mAP_50: 0.7890  coco/segm_mAP_75: 0.3850  coco/segm_mAP_s: 0.2420  coco/segm_mAP_m: 0.4730  coco/segm_mAP_l: 0.9250  data_time: 0.0139  time: 0.7270
05/11 14:08:52 - mmengine - INFO - The previous best checkpoint /data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-L_8xb32-24k_coco/best_coco_segm_mAP_50_iter_14000.pth is removed
05/11 14:08:54 - mmengine - INFO - The best checkpoint with 0.7890 coco/segm_mAP_50 at 18000 iter is saved to best_coco_segm_mAP_50_iter_18000.pth.
05/11 14:10:18 - mmengine - INFO - Iter(train) [18050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:36:35  time: 3.4109  data_time: 1.9094  memory: 38568  grad_norm: 48.0552  loss: 27.8820  loss_cls: 0.5857  loss_mask: 0.1796  loss_dice: 1.8346  d0.loss_cls: 1.1012  d0.loss_mask: 0.2301  d0.loss_dice: 2.0732  d1.loss_cls: 0.8519  d1.loss_mask: 0.2226  d1.loss_dice: 2.0618  d2.loss_cls: 0.6712  d2.loss_mask: 0.2010  d2.loss_dice: 1.9945  d3.loss_cls: 0.6138  d3.loss_mask: 0.1931  d3.loss_dice: 1.9196  d4.loss_cls: 0.6045  d4.loss_mask: 0.1885  d4.loss_dice: 1.8655  d5.loss_cls: 0.5931  d5.loss_mask: 0.1862  d5.loss_dice: 1.8560  d6.loss_cls: 0.5917  d6.loss_mask: 0.1842  d6.loss_dice: 1.8583  d7.loss_cls: 0.5937  d7.loss_mask: 0.1821  d7.loss_dice: 1.8456  d8.loss_cls: 0.5866  d8.loss_mask: 0.1811  d8.loss_dice: 1.8308
05/11 14:11:35 - mmengine - INFO - Iter(train) [18100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:35:16  time: 1.5358  data_time: 0.0321  memory: 39053  grad_norm: 54.0838  loss: 27.5141  loss_cls: 0.5899  loss_mask: 0.1641  loss_dice: 1.7951  d0.loss_cls: 1.1186  d0.loss_mask: 0.2148  d0.loss_dice: 2.0632  d1.loss_cls: 0.8463  d1.loss_mask: 0.2045  d1.loss_dice: 2.0469  d2.loss_cls: 0.6811  d2.loss_mask: 0.1844  d2.loss_dice: 1.9598  d3.loss_cls: 0.6445  d3.loss_mask: 0.1752  d3.loss_dice: 1.8743  d4.loss_cls: 0.6176  d4.loss_mask: 0.1709  d4.loss_dice: 1.8412  d5.loss_cls: 0.5955  d5.loss_mask: 0.1697  d5.loss_dice: 1.8293  d6.loss_cls: 0.5894  d6.loss_mask: 0.1682  d6.loss_dice: 1.8310  d7.loss_cls: 0.6057  d7.loss_mask: 0.1670  d7.loss_dice: 1.8123  d8.loss_cls: 0.5964  d8.loss_mask: 0.1649  d8.loss_dice: 1.7922
05/11 14:12:52 - mmengine - INFO - Iter(train) [18150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:33:56  time: 1.5459  data_time: 0.0312  memory: 38721  grad_norm: 46.9793  loss: 26.7110  loss_cls: 0.5890  loss_mask: 0.1747  loss_dice: 1.7322  d0.loss_cls: 1.0912  d0.loss_mask: 0.2208  d0.loss_dice: 1.9518  d1.loss_cls: 0.8254  d1.loss_mask: 0.2129  d1.loss_dice: 1.9534  d2.loss_cls: 0.6532  d2.loss_mask: 0.1929  d2.loss_dice: 1.8709  d3.loss_cls: 0.6501  d3.loss_mask: 0.1823  d3.loss_dice: 1.7939  d4.loss_cls: 0.6072  d4.loss_mask: 0.1800  d4.loss_dice: 1.7670  d5.loss_cls: 0.5870  d5.loss_mask: 0.1790  d5.loss_dice: 1.7571  d6.loss_cls: 0.5853  d6.loss_mask: 0.1776  d6.loss_dice: 1.7568  d7.loss_cls: 0.5956  d7.loss_mask: 0.1771  d7.loss_dice: 1.7485  d8.loss_cls: 0.5954  d8.loss_mask: 0.1752  d8.loss_dice: 1.7276
05/11 14:14:09 - mmengine - INFO - Iter(train) [18200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:32:37  time: 1.5341  data_time: 0.0287  memory: 38953  grad_norm: 50.6280  loss: 26.8773  loss_cls: 0.5825  loss_mask: 0.1737  loss_dice: 1.7412  d0.loss_cls: 1.1085  d0.loss_mask: 0.2236  d0.loss_dice: 1.9748  d1.loss_cls: 0.8406  d1.loss_mask: 0.2098  d1.loss_dice: 1.9745  d2.loss_cls: 0.6626  d2.loss_mask: 0.1948  d2.loss_dice: 1.8981  d3.loss_cls: 0.6394  d3.loss_mask: 0.1834  d3.loss_dice: 1.8193  d4.loss_cls: 0.6019  d4.loss_mask: 0.1771  d4.loss_dice: 1.7828  d5.loss_cls: 0.5851  d5.loss_mask: 0.1777  d5.loss_dice: 1.7763  d6.loss_cls: 0.5789  d6.loss_mask: 0.1774  d6.loss_dice: 1.7727  d7.loss_cls: 0.5857  d7.loss_mask: 0.1745  d7.loss_dice: 1.7620  d8.loss_cls: 0.5833  d8.loss_mask: 0.1744  d8.loss_dice: 1.7407
05/11 14:15:26 - mmengine - INFO - Iter(train) [18250/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:31:17  time: 1.5452  data_time: 0.0271  memory: 38525  grad_norm: 57.6281  loss: 27.7817  loss_cls: 0.5947  loss_mask: 0.1716  loss_dice: 1.8070  d0.loss_cls: 1.1216  d0.loss_mask: 0.2220  d0.loss_dice: 2.0697  d1.loss_cls: 0.8404  d1.loss_mask: 0.2157  d1.loss_dice: 2.0686  d2.loss_cls: 0.6723  d2.loss_mask: 0.1922  d2.loss_dice: 1.9931  d3.loss_cls: 0.6425  d3.loss_mask: 0.1831  d3.loss_dice: 1.9095  d4.loss_cls: 0.6024  d4.loss_mask: 0.1766  d4.loss_dice: 1.8721  d5.loss_cls: 0.5951  d5.loss_mask: 0.1765  d5.loss_dice: 1.8515  d6.loss_cls: 0.5920  d6.loss_mask: 0.1747  d6.loss_dice: 1.8518  d7.loss_cls: 0.5946  d7.loss_mask: 0.1738  d7.loss_dice: 1.8341  d8.loss_cls: 0.5949  d8.loss_mask: 0.1720  d8.loss_dice: 1.8158
05/11 14:16:43 - mmengine - INFO - Iter(train) [18300/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:29:58  time: 1.5469  data_time: 0.0367  memory: 38700  grad_norm: 50.1897  loss: 25.0348  loss_cls: 0.5554  loss_mask: 0.1590  loss_dice: 1.6037  d0.loss_cls: 1.0948  d0.loss_mask: 0.2066  d0.loss_dice: 1.8356  d1.loss_cls: 0.8045  d1.loss_mask: 0.1962  d1.loss_dice: 1.8276  d2.loss_cls: 0.6362  d2.loss_mask: 0.1786  d2.loss_dice: 1.7451  d3.loss_cls: 0.6001  d3.loss_mask: 0.1683  d3.loss_dice: 1.6824  d4.loss_cls: 0.5641  d4.loss_mask: 0.1637  d4.loss_dice: 1.6355  d5.loss_cls: 0.5663  d5.loss_mask: 0.1653  d5.loss_dice: 1.6307  d6.loss_cls: 0.5654  d6.loss_mask: 0.1640  d6.loss_dice: 1.6284  d7.loss_cls: 0.5633  d7.loss_mask: 0.1617  d7.loss_dice: 1.6160  d8.loss_cls: 0.5604  d8.loss_mask: 0.1589  d8.loss_dice: 1.5971
05/11 14:18:01 - mmengine - INFO - Iter(train) [18350/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:28:38  time: 1.5491  data_time: 0.0296  memory: 38927  grad_norm: 59.4292  loss: 28.8808  loss_cls: 0.6113  loss_mask: 0.1840  loss_dice: 1.9087  d0.loss_cls: 1.1118  d0.loss_mask: 0.2292  d0.loss_dice: 2.1431  d1.loss_cls: 0.8512  d1.loss_mask: 0.2218  d1.loss_dice: 2.1439  d2.loss_cls: 0.6837  d2.loss_mask: 0.2042  d2.loss_dice: 2.0674  d3.loss_cls: 0.6541  d3.loss_mask: 0.1965  d3.loss_dice: 1.9850  d4.loss_cls: 0.6337  d4.loss_mask: 0.1904  d4.loss_dice: 1.9405  d5.loss_cls: 0.6132  d5.loss_mask: 0.1902  d5.loss_dice: 1.9448  d6.loss_cls: 0.6196  d6.loss_mask: 0.1894  d6.loss_dice: 1.9338  d7.loss_cls: 0.6164  d7.loss_mask: 0.1866  d7.loss_dice: 1.9194  d8.loss_cls: 0.6130  d8.loss_mask: 0.1859  d8.loss_dice: 1.9080
05/11 14:19:18 - mmengine - INFO - Iter(train) [18400/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:27:19  time: 1.5428  data_time: 0.0263  memory: 38656  grad_norm: 47.9004  loss: 26.2800  loss_cls: 0.5590  loss_mask: 0.1623  loss_dice: 1.7103  d0.loss_cls: 1.1190  d0.loss_mask: 0.2132  d0.loss_dice: 1.9411  d1.loss_cls: 0.8268  d1.loss_mask: 0.2024  d1.loss_dice: 1.9488  d2.loss_cls: 0.6383  d2.loss_mask: 0.1800  d2.loss_dice: 1.8802  d3.loss_cls: 0.6080  d3.loss_mask: 0.1707  d3.loss_dice: 1.7925  d4.loss_cls: 0.5786  d4.loss_mask: 0.1669  d4.loss_dice: 1.7536  d5.loss_cls: 0.5657  d5.loss_mask: 0.1661  d5.loss_dice: 1.7385  d6.loss_cls: 0.5680  d6.loss_mask: 0.1640  d6.loss_dice: 1.7322  d7.loss_cls: 0.5708  d7.loss_mask: 0.1641  d7.loss_dice: 1.7232  d8.loss_cls: 0.5645  d8.loss_mask: 0.1618  d8.loss_dice: 1.7094
05/11 14:20:35 - mmengine - INFO - Iter(train) [18450/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:25:59  time: 1.5424  data_time: 0.0280  memory: 38322  grad_norm: 43.8586  loss: 25.6163  loss_cls: 0.5636  loss_mask: 0.1564  loss_dice: 1.6565  d0.loss_cls: 1.0976  d0.loss_mask: 0.2047  d0.loss_dice: 1.8791  d1.loss_cls: 0.8204  d1.loss_mask: 0.1878  d1.loss_dice: 1.8679  d2.loss_cls: 0.6407  d2.loss_mask: 0.1745  d2.loss_dice: 1.8073  d3.loss_cls: 0.6141  d3.loss_mask: 0.1697  d3.loss_dice: 1.7151  d4.loss_cls: 0.5806  d4.loss_mask: 0.1629  d4.loss_dice: 1.6988  d5.loss_cls: 0.5687  d5.loss_mask: 0.1630  d5.loss_dice: 1.6861  d6.loss_cls: 0.5732  d6.loss_mask: 0.1622  d6.loss_dice: 1.6769  d7.loss_cls: 0.5747  d7.loss_mask: 0.1594  d7.loss_dice: 1.6725  d8.loss_cls: 0.5723  d8.loss_mask: 0.1576  d8.loss_dice: 1.6519
05/11 14:21:53 - mmengine - INFO - Iter(train) [18500/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:24:40  time: 1.5522  data_time: 0.0371  memory: 38762  grad_norm: 50.5046  loss: 26.6383  loss_cls: 0.5710  loss_mask: 0.1613  loss_dice: 1.7331  d0.loss_cls: 1.1168  d0.loss_mask: 0.2100  d0.loss_dice: 1.9888  d1.loss_cls: 0.8246  d1.loss_mask: 0.1953  d1.loss_dice: 2.0026  d2.loss_cls: 0.6412  d2.loss_mask: 0.1792  d2.loss_dice: 1.8998  d3.loss_cls: 0.6107  d3.loss_mask: 0.1707  d3.loss_dice: 1.8139  d4.loss_cls: 0.5890  d4.loss_mask: 0.1660  d4.loss_dice: 1.7677  d5.loss_cls: 0.5743  d5.loss_mask: 0.1670  d5.loss_dice: 1.7749  d6.loss_cls: 0.5781  d6.loss_mask: 0.1657  d6.loss_dice: 1.7652  d7.loss_cls: 0.5763  d7.loss_mask: 0.1612  d7.loss_dice: 1.7634  d8.loss_cls: 0.5740  d8.loss_mask: 0.1605  d8.loss_dice: 1.7361
05/11 14:23:09 - mmengine - INFO - Iter(train) [18550/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:23:20  time: 1.5348  data_time: 0.0295  memory: 38339  grad_norm: 49.7632  loss: 26.6603  loss_cls: 0.5942  loss_mask: 0.1808  loss_dice: 1.6954  d0.loss_cls: 1.1077  d0.loss_mask: 0.2390  d0.loss_dice: 1.9411  d1.loss_cls: 0.8362  d1.loss_mask: 0.2233  d1.loss_dice: 1.9497  d2.loss_cls: 0.6729  d2.loss_mask: 0.2003  d2.loss_dice: 1.8467  d3.loss_cls: 0.6612  d3.loss_mask: 0.1884  d3.loss_dice: 1.7538  d4.loss_cls: 0.6313  d4.loss_mask: 0.1867  d4.loss_dice: 1.7316  d5.loss_cls: 0.6062  d5.loss_mask: 0.1861  d5.loss_dice: 1.7364  d6.loss_cls: 0.6015  d6.loss_mask: 0.1842  d6.loss_dice: 1.7275  d7.loss_cls: 0.6004  d7.loss_mask: 0.1831  d7.loss_dice: 1.7159  d8.loss_cls: 0.5931  d8.loss_mask: 0.1811  d8.loss_dice: 1.7046
05/11 14:24:26 - mmengine - INFO - Iter(train) [18600/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:22:01  time: 1.5394  data_time: 0.0320  memory: 38694  grad_norm: 53.4827  loss: 27.7091  loss_cls: 0.5962  loss_mask: 0.1779  loss_dice: 1.8117  d0.loss_cls: 1.1143  d0.loss_mask: 0.2274  d0.loss_dice: 2.0472  d1.loss_cls: 0.8195  d1.loss_mask: 0.2119  d1.loss_dice: 2.0610  d2.loss_cls: 0.6651  d2.loss_mask: 0.1936  d2.loss_dice: 1.9735  d3.loss_cls: 0.6469  d3.loss_mask: 0.1865  d3.loss_dice: 1.8816  d4.loss_cls: 0.6173  d4.loss_mask: 0.1829  d4.loss_dice: 1.8506  d5.loss_cls: 0.5979  d5.loss_mask: 0.1823  d5.loss_dice: 1.8503  d6.loss_cls: 0.5970  d6.loss_mask: 0.1798  d6.loss_dice: 1.8459  d7.loss_cls: 0.5981  d7.loss_mask: 0.1783  d7.loss_dice: 1.8307  d8.loss_cls: 0.5963  d8.loss_mask: 0.1780  d8.loss_dice: 1.8094
05/11 14:25:44 - mmengine - INFO - Iter(train) [18650/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:20:42  time: 1.5435  data_time: 0.0310  memory: 38206  grad_norm: 60.7230  loss: 24.5660  loss_cls: 0.5585  loss_mask: 0.1652  loss_dice: 1.5484  d0.loss_cls: 1.0774  d0.loss_mask: 0.2123  d0.loss_dice: 1.7623  d1.loss_cls: 0.8224  d1.loss_mask: 0.1983  d1.loss_dice: 1.7719  d2.loss_cls: 0.6359  d2.loss_mask: 0.1836  d2.loss_dice: 1.6925  d3.loss_cls: 0.6155  d3.loss_mask: 0.1762  d3.loss_dice: 1.6086  d4.loss_cls: 0.5790  d4.loss_mask: 0.1710  d4.loss_dice: 1.5929  d5.loss_cls: 0.5565  d5.loss_mask: 0.1727  d5.loss_dice: 1.5849  d6.loss_cls: 0.5615  d6.loss_mask: 0.1697  d6.loss_dice: 1.5794  d7.loss_cls: 0.5639  d7.loss_mask: 0.1681  d7.loss_dice: 1.5633  d8.loss_cls: 0.5618  d8.loss_mask: 0.1649  d8.loss_dice: 1.5474
05/11 14:27:01 - mmengine - INFO - Iter(train) [18700/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:19:22  time: 1.5431  data_time: 0.0340  memory: 38952  grad_norm: 54.6454  loss: 25.5679  loss_cls: 0.5820  loss_mask: 0.1649  loss_dice: 1.6303  d0.loss_cls: 1.0993  d0.loss_mask: 0.2119  d0.loss_dice: 1.8379  d1.loss_cls: 0.8025  d1.loss_mask: 0.2007  d1.loss_dice: 1.8594  d2.loss_cls: 0.6506  d2.loss_mask: 0.1844  d2.loss_dice: 1.7746  d3.loss_cls: 0.6407  d3.loss_mask: 0.1729  d3.loss_dice: 1.6910  d4.loss_cls: 0.6086  d4.loss_mask: 0.1692  d4.loss_dice: 1.6672  d5.loss_cls: 0.5921  d5.loss_mask: 0.1692  d5.loss_dice: 1.6612  d6.loss_cls: 0.5899  d6.loss_mask: 0.1681  d6.loss_dice: 1.6583  d7.loss_cls: 0.5913  d7.loss_mask: 0.1659  d7.loss_dice: 1.6430  d8.loss_cls: 0.5840  d8.loss_mask: 0.1660  d8.loss_dice: 1.6308
05/11 14:28:18 - mmengine - INFO - Iter(train) [18750/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:18:03  time: 1.5525  data_time: 0.0278  memory: 38544  grad_norm: 50.5957  loss: 25.5618  loss_cls: 0.5726  loss_mask: 0.1669  loss_dice: 1.6302  d0.loss_cls: 1.0877  d0.loss_mask: 0.2124  d0.loss_dice: 1.8453  d1.loss_cls: 0.8047  d1.loss_mask: 0.2013  d1.loss_dice: 1.8549  d2.loss_cls: 0.6424  d2.loss_mask: 0.1848  d2.loss_dice: 1.7740  d3.loss_cls: 0.6404  d3.loss_mask: 0.1794  d3.loss_dice: 1.6867  d4.loss_cls: 0.6181  d4.loss_mask: 0.1724  d4.loss_dice: 1.6590  d5.loss_cls: 0.5934  d5.loss_mask: 0.1712  d5.loss_dice: 1.6623  d6.loss_cls: 0.5979  d6.loss_mask: 0.1681  d6.loss_dice: 1.6574  d7.loss_cls: 0.5954  d7.loss_mask: 0.1683  d7.loss_dice: 1.6442  d8.loss_cls: 0.5831  d8.loss_mask: 0.1651  d8.loss_dice: 1.6220
05/11 14:29:35 - mmengine - INFO - Iter(train) [18800/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:16:44  time: 1.5408  data_time: 0.0295  memory: 38305  grad_norm: 48.6550  loss: 24.7241  loss_cls: 0.5433  loss_mask: 0.1605  loss_dice: 1.5919  d0.loss_cls: 1.0777  d0.loss_mask: 0.2079  d0.loss_dice: 1.7827  d1.loss_cls: 0.7924  d1.loss_mask: 0.1944  d1.loss_dice: 1.7909  d2.loss_cls: 0.6212  d2.loss_mask: 0.1762  d2.loss_dice: 1.7231  d3.loss_cls: 0.5952  d3.loss_mask: 0.1700  d3.loss_dice: 1.6635  d4.loss_cls: 0.5689  d4.loss_mask: 0.1650  d4.loss_dice: 1.6299  d5.loss_cls: 0.5512  d5.loss_mask: 0.1636  d5.loss_dice: 1.6222  d6.loss_cls: 0.5414  d6.loss_mask: 0.1627  d6.loss_dice: 1.6200  d7.loss_cls: 0.5509  d7.loss_mask: 0.1620  d7.loss_dice: 1.6020  d8.loss_cls: 0.5473  d8.loss_mask: 0.1616  d8.loss_dice: 1.5843
05/11 14:30:52 - mmengine - INFO - Iter(train) [18850/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:15:24  time: 1.5274  data_time: 0.0303  memory: 38602  grad_norm: 43.0221  loss: 26.7936  loss_cls: 0.5545  loss_mask: 0.1657  loss_dice: 1.7730  d0.loss_cls: 1.0990  d0.loss_mask: 0.2146  d0.loss_dice: 1.9767  d1.loss_cls: 0.8002  d1.loss_mask: 0.2054  d1.loss_dice: 2.0025  d2.loss_cls: 0.6373  d2.loss_mask: 0.1851  d2.loss_dice: 1.9340  d3.loss_cls: 0.6016  d3.loss_mask: 0.1742  d3.loss_dice: 1.8594  d4.loss_cls: 0.5699  d4.loss_mask: 0.1692  d4.loss_dice: 1.8164  d5.loss_cls: 0.5516  d5.loss_mask: 0.1686  d5.loss_dice: 1.8097  d6.loss_cls: 0.5520  d6.loss_mask: 0.1688  d6.loss_dice: 1.7988  d7.loss_cls: 0.5584  d7.loss_mask: 0.1670  d7.loss_dice: 1.7845  d8.loss_cls: 0.5588  d8.loss_mask: 0.1652  d8.loss_dice: 1.7715
05/11 14:32:09 - mmengine - INFO - Iter(train) [18900/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:14:05  time: 1.5431  data_time: 0.0374  memory: 38482  grad_norm: 51.6605  loss: 26.7251  loss_cls: 0.5905  loss_mask: 0.1761  loss_dice: 1.7123  d0.loss_cls: 1.1072  d0.loss_mask: 0.2326  d0.loss_dice: 1.9549  d1.loss_cls: 0.8459  d1.loss_mask: 0.2195  d1.loss_dice: 1.9476  d2.loss_cls: 0.6652  d2.loss_mask: 0.1983  d2.loss_dice: 1.8743  d3.loss_cls: 0.6266  d3.loss_mask: 0.1884  d3.loss_dice: 1.8093  d4.loss_cls: 0.6005  d4.loss_mask: 0.1834  d4.loss_dice: 1.7677  d5.loss_cls: 0.5855  d5.loss_mask: 0.1816  d5.loss_dice: 1.7540  d6.loss_cls: 0.5860  d6.loss_mask: 0.1805  d6.loss_dice: 1.7482  d7.loss_cls: 0.5921  d7.loss_mask: 0.1773  d7.loss_dice: 1.7326  d8.loss_cls: 0.5940  d8.loss_mask: 0.1765  d8.loss_dice: 1.7164
05/11 14:33:26 - mmengine - INFO - Iter(train) [18950/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:12:45  time: 1.5344  data_time: 0.0309  memory: 38512  grad_norm: 62.5725  loss: 26.0414  loss_cls: 0.5644  loss_mask: 0.1719  loss_dice: 1.6796  d0.loss_cls: 1.1005  d0.loss_mask: 0.2216  d0.loss_dice: 1.8792  d1.loss_cls: 0.8275  d1.loss_mask: 0.2068  d1.loss_dice: 1.8971  d2.loss_cls: 0.6479  d2.loss_mask: 0.1904  d2.loss_dice: 1.8232  d3.loss_cls: 0.6219  d3.loss_mask: 0.1837  d3.loss_dice: 1.7492  d4.loss_cls: 0.5900  d4.loss_mask: 0.1811  d4.loss_dice: 1.7210  d5.loss_cls: 0.5712  d5.loss_mask: 0.1778  d5.loss_dice: 1.7162  d6.loss_cls: 0.5647  d6.loss_mask: 0.1767  d6.loss_dice: 1.7083  d7.loss_cls: 0.5821  d7.loss_mask: 0.1741  d7.loss_dice: 1.6919  d8.loss_cls: 0.5707  d8.loss_mask: 0.1712  d8.loss_dice: 1.6796
05/11 14:34:42 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 14:34:42 - mmengine - INFO - Iter(train) [19000/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:11:26  time: 1.5313  data_time: 0.0304  memory: 38502  grad_norm: 47.9956  loss: 27.8482  loss_cls: 0.5772  loss_mask: 0.1723  loss_dice: 1.8298  d0.loss_cls: 1.1247  d0.loss_mask: 0.2158  d0.loss_dice: 2.0624  d1.loss_cls: 0.8249  d1.loss_mask: 0.2065  d1.loss_dice: 2.0805  d2.loss_cls: 0.6506  d2.loss_mask: 0.1874  d2.loss_dice: 1.9996  d3.loss_cls: 0.6276  d3.loss_mask: 0.1826  d3.loss_dice: 1.9190  d4.loss_cls: 0.6090  d4.loss_mask: 0.1808  d4.loss_dice: 1.8885  d5.loss_cls: 0.6019  d5.loss_mask: 0.1775  d5.loss_dice: 1.8758  d6.loss_cls: 0.5895  d6.loss_mask: 0.1749  d6.loss_dice: 1.8740  d7.loss_cls: 0.5937  d7.loss_mask: 0.1732  d7.loss_dice: 1.8617  d8.loss_cls: 0.5806  d8.loss_mask: 0.1720  d8.loss_dice: 1.8343
05/11 14:35:59 - mmengine - INFO - Iter(train) [19050/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:10:06  time: 1.5373  data_time: 0.0268  memory: 38639  grad_norm: 46.7130  loss: 28.3410  loss_cls: 0.6060  loss_mask: 0.1807  loss_dice: 1.8556  d0.loss_cls: 1.1238  d0.loss_mask: 0.2292  d0.loss_dice: 2.0802  d1.loss_cls: 0.8418  d1.loss_mask: 0.2192  d1.loss_dice: 2.1069  d2.loss_cls: 0.6606  d2.loss_mask: 0.1996  d2.loss_dice: 2.0269  d3.loss_cls: 0.6441  d3.loss_mask: 0.1937  d3.loss_dice: 1.9450  d4.loss_cls: 0.6285  d4.loss_mask: 0.1908  d4.loss_dice: 1.9145  d5.loss_cls: 0.6072  d5.loss_mask: 0.1875  d5.loss_dice: 1.9007  d6.loss_cls: 0.5995  d6.loss_mask: 0.1845  d6.loss_dice: 1.8942  d7.loss_cls: 0.6093  d7.loss_mask: 0.1834  d7.loss_dice: 1.8776  d8.loss_cls: 0.6076  d8.loss_mask: 0.1817  d8.loss_dice: 1.8607
05/11 14:37:16 - mmengine - INFO - Iter(train) [19100/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:08:47  time: 1.5364  data_time: 0.0280  memory: 39386  grad_norm: 58.8747  loss: 28.2494  loss_cls: 0.6184  loss_mask: 0.1707  loss_dice: 1.8471  d0.loss_cls: 1.1377  d0.loss_mask: 0.2230  d0.loss_dice: 2.0717  d1.loss_cls: 0.8581  d1.loss_mask: 0.2121  d1.loss_dice: 2.0904  d2.loss_cls: 0.6784  d2.loss_mask: 0.1949  d2.loss_dice: 2.0021  d3.loss_cls: 0.6519  d3.loss_mask: 0.1834  d3.loss_dice: 1.9272  d4.loss_cls: 0.6299  d4.loss_mask: 0.1801  d4.loss_dice: 1.8960  d5.loss_cls: 0.6179  d5.loss_mask: 0.1808  d5.loss_dice: 1.8754  d6.loss_cls: 0.6355  d6.loss_mask: 0.1776  d6.loss_dice: 1.8701  d7.loss_cls: 0.6295  d7.loss_mask: 0.1747  d7.loss_dice: 1.8735  d8.loss_cls: 0.6205  d8.loss_mask: 0.1733  d8.loss_dice: 1.8475
05/11 14:38:33 - mmengine - INFO - Iter(train) [19150/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:07:28  time: 1.5459  data_time: 0.0275  memory: 39167  grad_norm: 78.9246  loss: 27.0472  loss_cls: 0.5841  loss_mask: 0.1685  loss_dice: 1.7543  d0.loss_cls: 1.1209  d0.loss_mask: 0.2108  d0.loss_dice: 1.9884  d1.loss_cls: 0.8279  d1.loss_mask: 0.2066  d1.loss_dice: 1.9878  d2.loss_cls: 0.6503  d2.loss_mask: 0.1890  d2.loss_dice: 1.9113  d3.loss_cls: 0.6295  d3.loss_mask: 0.1797  d3.loss_dice: 1.8345  d4.loss_cls: 0.6180  d4.loss_mask: 0.1731  d4.loss_dice: 1.8125  d5.loss_cls: 0.5996  d5.loss_mask: 0.1733  d5.loss_dice: 1.7989  d6.loss_cls: 0.6012  d6.loss_mask: 0.1719  d6.loss_dice: 1.7919  d7.loss_cls: 0.5939  d7.loss_mask: 0.1709  d7.loss_dice: 1.7866  d8.loss_cls: 0.5860  d8.loss_mask: 0.1693  d8.loss_dice: 1.7564
05/11 14:39:49 - mmengine - INFO - Iter(train) [19200/24000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:06:08  time: 1.5223  data_time: 0.0286  memory: 38719  grad_norm: 54.8898  loss: 27.1061  loss_cls: 0.5574  loss_mask: 0.1557  loss_dice: 1.8024  d0.loss_cls: 1.1144  d0.loss_mask: 0.1954  d0.loss_dice: 2.0188  d1.loss_cls: 0.8211  d1.loss_mask: 0.1871  d1.loss_dice: 2.0213  d2.loss_cls: 0.6405  d2.loss_mask: 0.1688  d2.loss_dice: 1.9495  d3.loss_cls: 0.6206  d3.loss_mask: 0.1595  d3.loss_dice: 1.8791  d4.loss_cls: 0.6016  d4.loss_mask: 0.1610  d4.loss_dice: 1.8507  d5.loss_cls: 0.5820  d5.loss_mask: 0.1587  d5.loss_dice: 1.8382  d6.loss_cls: 0.5750  d6.loss_mask: 0.1580  d6.loss_dice: 1.8338  d7.loss_cls: 0.5650  d7.loss_mask: 0.1552  d7.loss_dice: 1.8201  d8.loss_cls: 0.5610  d8.loss_mask: 0.1546  d8.loss_dice: 1.7997
05/11 14:41:06 - mmengine - INFO - Iter(train) [19250/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 2:04:49  time: 1.5343  data_time: 0.0311  memory: 39185  grad_norm: 51.1784  loss: 25.8637  loss_cls: 0.5782  loss_mask: 0.1692  loss_dice: 1.6716  d0.loss_cls: 1.0805  d0.loss_mask: 0.2103  d0.loss_dice: 1.8678  d1.loss_cls: 0.8038  d1.loss_mask: 0.2004  d1.loss_dice: 1.8506  d2.loss_cls: 0.6351  d2.loss_mask: 0.1850  d2.loss_dice: 1.8087  d3.loss_cls: 0.6043  d3.loss_mask: 0.1762  d3.loss_dice: 1.7562  d4.loss_cls: 0.5959  d4.loss_mask: 0.1734  d4.loss_dice: 1.7167  d5.loss_cls: 0.5809  d5.loss_mask: 0.1739  d5.loss_dice: 1.7075  d6.loss_cls: 0.5850  d6.loss_mask: 0.1728  d6.loss_dice: 1.6969  d7.loss_cls: 0.5776  d7.loss_mask: 0.1723  d7.loss_dice: 1.6939  d8.loss_cls: 0.5720  d8.loss_mask: 0.1710  d8.loss_dice: 1.6761
05/11 14:42:22 - mmengine - INFO - Iter(train) [19300/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 2:03:29  time: 1.5259  data_time: 0.0290  memory: 38755  grad_norm: 56.3660  loss: 26.3034  loss_cls: 0.5664  loss_mask: 0.1710  loss_dice: 1.7120  d0.loss_cls: 1.1058  d0.loss_mask: 0.2153  d0.loss_dice: 1.9446  d1.loss_cls: 0.8167  d1.loss_mask: 0.2061  d1.loss_dice: 1.9361  d2.loss_cls: 0.6273  d2.loss_mask: 0.1874  d2.loss_dice: 1.8595  d3.loss_cls: 0.5984  d3.loss_mask: 0.1784  d3.loss_dice: 1.7886  d4.loss_cls: 0.5869  d4.loss_mask: 0.1768  d4.loss_dice: 1.7499  d5.loss_cls: 0.5704  d5.loss_mask: 0.1756  d5.loss_dice: 1.7347  d6.loss_cls: 0.5730  d6.loss_mask: 0.1760  d6.loss_dice: 1.7309  d7.loss_cls: 0.5720  d7.loss_mask: 0.1744  d7.loss_dice: 1.7219  d8.loss_cls: 0.5620  d8.loss_mask: 0.1733  d8.loss_dice: 1.7121
05/11 14:43:38 - mmengine - INFO - Iter(train) [19350/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 2:02:10  time: 1.5174  data_time: 0.0276  memory: 38816  grad_norm: 55.4694  loss: 27.4865  loss_cls: 0.6030  loss_mask: 0.1691  loss_dice: 1.8015  d0.loss_cls: 1.1132  d0.loss_mask: 0.2124  d0.loss_dice: 2.0279  d1.loss_cls: 0.8287  d1.loss_mask: 0.2043  d1.loss_dice: 2.0284  d2.loss_cls: 0.6552  d2.loss_mask: 0.1879  d2.loss_dice: 1.9550  d3.loss_cls: 0.6332  d3.loss_mask: 0.1790  d3.loss_dice: 1.8750  d4.loss_cls: 0.6173  d4.loss_mask: 0.1759  d4.loss_dice: 1.8463  d5.loss_cls: 0.5989  d5.loss_mask: 0.1747  d5.loss_dice: 1.8317  d6.loss_cls: 0.6109  d6.loss_mask: 0.1752  d6.loss_dice: 1.8227  d7.loss_cls: 0.6024  d7.loss_mask: 0.1734  d7.loss_dice: 1.8167  d8.loss_cls: 0.5969  d8.loss_mask: 0.1705  d8.loss_dice: 1.7992
05/11 14:44:55 - mmengine - INFO - Iter(train) [19400/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 2:00:50  time: 1.5305  data_time: 0.0279  memory: 38768  grad_norm: 49.1310  loss: 26.4099  loss_cls: 0.5843  loss_mask: 0.1670  loss_dice: 1.7100  d0.loss_cls: 1.1128  d0.loss_mask: 0.2124  d0.loss_dice: 1.9354  d1.loss_cls: 0.8286  d1.loss_mask: 0.2020  d1.loss_dice: 1.9252  d2.loss_cls: 0.6386  d2.loss_mask: 0.1842  d2.loss_dice: 1.8489  d3.loss_cls: 0.6219  d3.loss_mask: 0.1775  d3.loss_dice: 1.7751  d4.loss_cls: 0.6114  d4.loss_mask: 0.1747  d4.loss_dice: 1.7488  d5.loss_cls: 0.5846  d5.loss_mask: 0.1719  d5.loss_dice: 1.7436  d6.loss_cls: 0.5936  d6.loss_mask: 0.1711  d6.loss_dice: 1.7359  d7.loss_cls: 0.5902  d7.loss_mask: 0.1701  d7.loss_dice: 1.7289  d8.loss_cls: 0.5830  d8.loss_mask: 0.1667  d8.loss_dice: 1.7114
05/11 14:46:11 - mmengine - INFO - Iter(train) [19450/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:59:31  time: 1.5320  data_time: 0.0285  memory: 38772  grad_norm: 45.9868  loss: 26.4696  loss_cls: 0.5856  loss_mask: 0.1619  loss_dice: 1.7240  d0.loss_cls: 1.1057  d0.loss_mask: 0.2009  d0.loss_dice: 1.9387  d1.loss_cls: 0.8036  d1.loss_mask: 0.1943  d1.loss_dice: 1.9349  d2.loss_cls: 0.6391  d2.loss_mask: 0.1794  d2.loss_dice: 1.8639  d3.loss_cls: 0.6158  d3.loss_mask: 0.1742  d3.loss_dice: 1.7988  d4.loss_cls: 0.6037  d4.loss_mask: 0.1683  d4.loss_dice: 1.7702  d5.loss_cls: 0.5852  d5.loss_mask: 0.1666  d5.loss_dice: 1.7629  d6.loss_cls: 0.5939  d6.loss_mask: 0.1665  d6.loss_dice: 1.7577  d7.loss_cls: 0.5874  d7.loss_mask: 0.1649  d7.loss_dice: 1.7470  d8.loss_cls: 0.5843  d8.loss_mask: 0.1624  d8.loss_dice: 1.7277
05/11 14:47:28 - mmengine - INFO - Iter(train) [19500/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:58:12  time: 1.5414  data_time: 0.0269  memory: 38976  grad_norm: 61.8871  loss: 24.5146  loss_cls: 0.5631  loss_mask: 0.1406  loss_dice: 1.5831  d0.loss_cls: 1.0804  d0.loss_mask: 0.1770  d0.loss_dice: 1.7679  d1.loss_cls: 0.7865  d1.loss_mask: 0.1714  d1.loss_dice: 1.7683  d2.loss_cls: 0.6165  d2.loss_mask: 0.1550  d2.loss_dice: 1.7066  d3.loss_cls: 0.5960  d3.loss_mask: 0.1501  d3.loss_dice: 1.6425  d4.loss_cls: 0.5861  d4.loss_mask: 0.1466  d4.loss_dice: 1.6228  d5.loss_cls: 0.5615  d5.loss_mask: 0.1456  d5.loss_dice: 1.6163  d6.loss_cls: 0.5750  d6.loss_mask: 0.1455  d6.loss_dice: 1.6062  d7.loss_cls: 0.5690  d7.loss_mask: 0.1433  d7.loss_dice: 1.6015  d8.loss_cls: 0.5637  d8.loss_mask: 0.1413  d8.loss_dice: 1.5850
05/11 14:48:46 - mmengine - INFO - Iter(train) [19550/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:56:53  time: 1.5504  data_time: 0.0263  memory: 38927  grad_norm: 46.6985  loss: 26.0365  loss_cls: 0.5840  loss_mask: 0.1660  loss_dice: 1.6705  d0.loss_cls: 1.1154  d0.loss_mask: 0.2104  d0.loss_dice: 1.8880  d1.loss_cls: 0.8194  d1.loss_mask: 0.2024  d1.loss_dice: 1.8818  d2.loss_cls: 0.6394  d2.loss_mask: 0.1842  d2.loss_dice: 1.8135  d3.loss_cls: 0.6220  d3.loss_mask: 0.1789  d3.loss_dice: 1.7444  d4.loss_cls: 0.6063  d4.loss_mask: 0.1741  d4.loss_dice: 1.7224  d5.loss_cls: 0.5876  d5.loss_mask: 0.1731  d5.loss_dice: 1.7053  d6.loss_cls: 0.5957  d6.loss_mask: 0.1716  d6.loss_dice: 1.6968  d7.loss_cls: 0.5944  d7.loss_mask: 0.1687  d7.loss_dice: 1.6892  d8.loss_cls: 0.5827  d8.loss_mask: 0.1672  d8.loss_dice: 1.6808
05/11 14:50:02 - mmengine - INFO - Iter(train) [19600/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:55:33  time: 1.5207  data_time: 0.0303  memory: 38818  grad_norm: 62.4990  loss: 26.1994  loss_cls: 0.5791  loss_mask: 0.1615  loss_dice: 1.7113  d0.loss_cls: 1.0919  d0.loss_mask: 0.2013  d0.loss_dice: 1.9129  d1.loss_cls: 0.8048  d1.loss_mask: 0.1926  d1.loss_dice: 1.9128  d2.loss_cls: 0.6340  d2.loss_mask: 0.1762  d2.loss_dice: 1.8375  d3.loss_cls: 0.6123  d3.loss_mask: 0.1729  d3.loss_dice: 1.7744  d4.loss_cls: 0.5982  d4.loss_mask: 0.1692  d4.loss_dice: 1.7532  d5.loss_cls: 0.5815  d5.loss_mask: 0.1669  d5.loss_dice: 1.7405  d6.loss_cls: 0.5905  d6.loss_mask: 0.1653  d6.loss_dice: 1.7309  d7.loss_cls: 0.5845  d7.loss_mask: 0.1643  d7.loss_dice: 1.7301  d8.loss_cls: 0.5806  d8.loss_mask: 0.1617  d8.loss_dice: 1.7069
05/11 14:51:19 - mmengine - INFO - Iter(train) [19650/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:54:14  time: 1.5382  data_time: 0.0302  memory: 38621  grad_norm: 47.2369  loss: 25.9946  loss_cls: 0.5719  loss_mask: 0.1552  loss_dice: 1.6954  d0.loss_cls: 1.1114  d0.loss_mask: 0.1931  d0.loss_dice: 1.9134  d1.loss_cls: 0.8113  d1.loss_mask: 0.1842  d1.loss_dice: 1.9106  d2.loss_cls: 0.6286  d2.loss_mask: 0.1689  d2.loss_dice: 1.8356  d3.loss_cls: 0.6124  d3.loss_mask: 0.1637  d3.loss_dice: 1.7621  d4.loss_cls: 0.5949  d4.loss_mask: 0.1596  d4.loss_dice: 1.7380  d5.loss_cls: 0.5779  d5.loss_mask: 0.1588  d5.loss_dice: 1.7273  d6.loss_cls: 0.5800  d6.loss_mask: 0.1583  d6.loss_dice: 1.7171  d7.loss_cls: 0.5749  d7.loss_mask: 0.1569  d7.loss_dice: 1.7154  d8.loss_cls: 0.5711  d8.loss_mask: 0.1550  d8.loss_dice: 1.6915
05/11 14:52:35 - mmengine - INFO - Iter(train) [19700/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:52:55  time: 1.5290  data_time: 0.0312  memory: 38567  grad_norm: 52.8575  loss: 27.7150  loss_cls: 0.5927  loss_mask: 0.1831  loss_dice: 1.8094  d0.loss_cls: 1.1300  d0.loss_mask: 0.2230  d0.loss_dice: 2.0390  d1.loss_cls: 0.8267  d1.loss_mask: 0.2142  d1.loss_dice: 2.0306  d2.loss_cls: 0.6495  d2.loss_mask: 0.1984  d2.loss_dice: 1.9585  d3.loss_cls: 0.6354  d3.loss_mask: 0.1916  d3.loss_dice: 1.8909  d4.loss_cls: 0.6187  d4.loss_mask: 0.1886  d4.loss_dice: 1.8604  d5.loss_cls: 0.6005  d5.loss_mask: 0.1868  d5.loss_dice: 1.8472  d6.loss_cls: 0.6067  d6.loss_mask: 0.1859  d6.loss_dice: 1.8415  d7.loss_cls: 0.6019  d7.loss_mask: 0.1868  d7.loss_dice: 1.8295  d8.loss_cls: 0.5916  d8.loss_mask: 0.1839  d8.loss_dice: 1.8121
05/11 14:53:52 - mmengine - INFO - Iter(train) [19750/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:51:36  time: 1.5395  data_time: 0.0270  memory: 38756  grad_norm: 46.6729  loss: 26.5168  loss_cls: 0.5779  loss_mask: 0.1583  loss_dice: 1.7211  d0.loss_cls: 1.1448  d0.loss_mask: 0.1972  d0.loss_dice: 1.9522  d1.loss_cls: 0.8333  d1.loss_mask: 0.1920  d1.loss_dice: 1.9543  d2.loss_cls: 0.6478  d2.loss_mask: 0.1732  d2.loss_dice: 1.8666  d3.loss_cls: 0.6308  d3.loss_mask: 0.1671  d3.loss_dice: 1.7890  d4.loss_cls: 0.6048  d4.loss_mask: 0.1644  d4.loss_dice: 1.7743  d5.loss_cls: 0.5903  d5.loss_mask: 0.1633  d5.loss_dice: 1.7573  d6.loss_cls: 0.5926  d6.loss_mask: 0.1617  d6.loss_dice: 1.7490  d7.loss_cls: 0.5922  d7.loss_mask: 0.1605  d7.loss_dice: 1.7397  d8.loss_cls: 0.5808  d8.loss_mask: 0.1578  d8.loss_dice: 1.7226
05/11 14:55:09 - mmengine - INFO - Iter(train) [19800/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:50:16  time: 1.5307  data_time: 0.0271  memory: 38902  grad_norm: 61.8891  loss: 26.5462  loss_cls: 0.5825  loss_mask: 0.1641  loss_dice: 1.7300  d0.loss_cls: 1.1143  d0.loss_mask: 0.2050  d0.loss_dice: 1.9568  d1.loss_cls: 0.8130  d1.loss_mask: 0.1957  d1.loss_dice: 1.9429  d2.loss_cls: 0.6318  d2.loss_mask: 0.1805  d2.loss_dice: 1.8688  d3.loss_cls: 0.6208  d3.loss_mask: 0.1737  d3.loss_dice: 1.7925  d4.loss_cls: 0.6009  d4.loss_mask: 0.1708  d4.loss_dice: 1.7782  d5.loss_cls: 0.5853  d5.loss_mask: 0.1699  d5.loss_dice: 1.7649  d6.loss_cls: 0.5940  d6.loss_mask: 0.1677  d6.loss_dice: 1.7591  d7.loss_cls: 0.5918  d7.loss_mask: 0.1660  d7.loss_dice: 1.7481  d8.loss_cls: 0.5811  d8.loss_mask: 0.1643  d8.loss_dice: 1.7318
05/11 14:56:25 - mmengine - INFO - Iter(train) [19850/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:48:57  time: 1.5282  data_time: 0.0265  memory: 39322  grad_norm: 55.6949  loss: 26.6326  loss_cls: 0.5881  loss_mask: 0.1616  loss_dice: 1.7284  d0.loss_cls: 1.1255  d0.loss_mask: 0.2054  d0.loss_dice: 1.9545  d1.loss_cls: 0.8437  d1.loss_mask: 0.1925  d1.loss_dice: 1.9463  d2.loss_cls: 0.6594  d2.loss_mask: 0.1768  d2.loss_dice: 1.8641  d3.loss_cls: 0.6326  d3.loss_mask: 0.1720  d3.loss_dice: 1.7944  d4.loss_cls: 0.6133  d4.loss_mask: 0.1686  d4.loss_dice: 1.7735  d5.loss_cls: 0.5911  d5.loss_mask: 0.1667  d5.loss_dice: 1.7573  d6.loss_cls: 0.5991  d6.loss_mask: 0.1668  d6.loss_dice: 1.7581  d7.loss_cls: 0.5970  d7.loss_mask: 0.1639  d7.loss_dice: 1.7500  d8.loss_cls: 0.5875  d8.loss_mask: 0.1619  d8.loss_dice: 1.7328
05/11 14:57:43 - mmengine - INFO - Iter(train) [19900/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:47:38  time: 1.5537  data_time: 0.0282  memory: 38873  grad_norm: 49.0366  loss: 25.4809  loss_cls: 0.5730  loss_mask: 0.1566  loss_dice: 1.6479  d0.loss_cls: 1.1035  d0.loss_mask: 0.1970  d0.loss_dice: 1.8489  d1.loss_cls: 0.8142  d1.loss_mask: 0.1850  d1.loss_dice: 1.8460  d2.loss_cls: 0.6267  d2.loss_mask: 0.1707  d2.loss_dice: 1.7733  d3.loss_cls: 0.6122  d3.loss_mask: 0.1642  d3.loss_dice: 1.7083  d4.loss_cls: 0.5945  d4.loss_mask: 0.1611  d4.loss_dice: 1.6864  d5.loss_cls: 0.5772  d5.loss_mask: 0.1596  d5.loss_dice: 1.6812  d6.loss_cls: 0.5875  d6.loss_mask: 0.1587  d6.loss_dice: 1.6660  d7.loss_cls: 0.5810  d7.loss_mask: 0.1593  d7.loss_dice: 1.6648  d8.loss_cls: 0.5708  d8.loss_mask: 0.1579  d8.loss_dice: 1.6476
05/11 14:58:59 - mmengine - INFO - Iter(train) [19950/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:46:19  time: 1.5172  data_time: 0.0314  memory: 38606  grad_norm: 57.8338  loss: 26.2189  loss_cls: 0.5661  loss_mask: 0.1599  loss_dice: 1.7155  d0.loss_cls: 1.1038  d0.loss_mask: 0.1996  d0.loss_dice: 1.9353  d1.loss_cls: 0.8149  d1.loss_mask: 0.1908  d1.loss_dice: 1.9256  d2.loss_cls: 0.6204  d2.loss_mask: 0.1760  d2.loss_dice: 1.8480  d3.loss_cls: 0.6087  d3.loss_mask: 0.1677  d3.loss_dice: 1.7739  d4.loss_cls: 0.5908  d4.loss_mask: 0.1653  d4.loss_dice: 1.7609  d5.loss_cls: 0.5757  d5.loss_mask: 0.1639  d5.loss_dice: 1.7435  d6.loss_cls: 0.5874  d6.loss_mask: 0.1633  d6.loss_dice: 1.7392  d7.loss_cls: 0.5786  d7.loss_mask: 0.1631  d7.loss_dice: 1.7325  d8.loss_cls: 0.5676  d8.loss_mask: 0.1608  d8.loss_dice: 1.7201
05/11 15:00:15 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 15:00:15 - mmengine - INFO - Iter(train) [20000/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:45:00  time: 1.5211  data_time: 0.0312  memory: 38586  grad_norm: 56.3218  loss: 25.8378  loss_cls: 0.5764  loss_mask: 0.1645  loss_dice: 1.6661  d0.loss_cls: 1.1068  d0.loss_mask: 0.2044  d0.loss_dice: 1.8776  d1.loss_cls: 0.8134  d1.loss_mask: 0.1944  d1.loss_dice: 1.8671  d2.loss_cls: 0.6331  d2.loss_mask: 0.1799  d2.loss_dice: 1.7983  d3.loss_cls: 0.6182  d3.loss_mask: 0.1741  d3.loss_dice: 1.7290  d4.loss_cls: 0.5934  d4.loss_mask: 0.1727  d4.loss_dice: 1.7160  d5.loss_cls: 0.5848  d5.loss_mask: 0.1716  d5.loss_dice: 1.7008  d6.loss_cls: 0.5912  d6.loss_mask: 0.1697  d6.loss_dice: 1.6901  d7.loss_cls: 0.5811  d7.loss_mask: 0.1674  d7.loss_dice: 1.6910  d8.loss_cls: 0.5674  d8.loss_mask: 0.1652  d8.loss_dice: 1.6720
05/11 15:00:15 - mmengine - INFO - Saving checkpoint at 20000 iterations
05/11 15:00:58 - mmengine - INFO - Iter(val) [ 50/100]    eta: 0:00:36  time: 0.7365  data_time: 0.0141  memory: 5704  
05/11 15:01:34 - mmengine - INFO - Iter(val) [100/100]    eta: 0:00:00  time: 0.7237  data_time: 0.0132  memory: 5704  
05/11 15:01:57 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.20s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/11 15:02:05 - mmengine - INFO - start multi processing evaluation ...

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

  0%|          | 0/25552 [00:00<?, ?it/s]
 50%|█████     | 12778/25552 [00:00<00:00, 125475.71it/s]
100%|██████████| 25552/25552 [00:00<00:00, 139654.73it/s]
DONE (t=53.58s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.393
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.742
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.370
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.250
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.452
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.871
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.522
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.914
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.508
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.392
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.580
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.933
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.522
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.522
05/11 15:02:58 - mmengine - INFO - segm_mAP_copypaste: 0.393 0.742 0.370 0.250 0.452 0.871
05/11 15:02:58 - mmengine - INFO - segm_mAR_copypaste: 0.522 0.914 0.508 0.392 0.580 0.933
05/11 15:02:59 - mmengine - INFO - Iter(val) [100/100]    coco/segm_mAP: 0.3930  coco/segm_mAP_50: 0.7420  coco/segm_mAP_75: 0.3700  coco/segm_mAP_s: 0.2500  coco/segm_mAP_m: 0.4520  coco/segm_mAP_l: 0.8710  data_time: 0.0136  time: 0.7287
05/11 15:04:16 - mmengine - INFO - Iter(train) [20050/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:43:57  time: 3.2240  data_time: 1.7300  memory: 38506  grad_norm: 55.2523  loss: 25.1235  loss_cls: 0.5753  loss_mask: 0.1620  loss_dice: 1.6106  d0.loss_cls: 1.0924  d0.loss_mask: 0.2016  d0.loss_dice: 1.8106  d1.loss_cls: 0.7897  d1.loss_mask: 0.1893  d1.loss_dice: 1.8024  d2.loss_cls: 0.6113  d2.loss_mask: 0.1754  d2.loss_dice: 1.7386  d3.loss_cls: 0.6006  d3.loss_mask: 0.1703  d3.loss_dice: 1.6776  d4.loss_cls: 0.5803  d4.loss_mask: 0.1668  d4.loss_dice: 1.6594  d5.loss_cls: 0.5735  d5.loss_mask: 0.1670  d5.loss_dice: 1.6511  d6.loss_cls: 0.5840  d6.loss_mask: 0.1652  d6.loss_dice: 1.6456  d7.loss_cls: 0.5745  d7.loss_mask: 0.1633  d7.loss_dice: 1.6331  d8.loss_cls: 0.5744  d8.loss_mask: 0.1620  d8.loss_dice: 1.6158
05/11 15:05:32 - mmengine - INFO - Iter(train) [20100/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:42:38  time: 1.5333  data_time: 0.0313  memory: 38851  grad_norm: 51.0701  loss: 25.3358  loss_cls: 0.5896  loss_mask: 0.1491  loss_dice: 1.6283  d0.loss_cls: 1.0966  d0.loss_mask: 0.1845  d0.loss_dice: 1.8288  d1.loss_cls: 0.8187  d1.loss_mask: 0.1755  d1.loss_dice: 1.8202  d2.loss_cls: 0.6421  d2.loss_mask: 0.1622  d2.loss_dice: 1.7546  d3.loss_cls: 0.6257  d3.loss_mask: 0.1558  d3.loss_dice: 1.6932  d4.loss_cls: 0.5963  d4.loss_mask: 0.1545  d4.loss_dice: 1.6760  d5.loss_cls: 0.5926  d5.loss_mask: 0.1532  d5.loss_dice: 1.6632  d6.loss_cls: 0.5994  d6.loss_mask: 0.1531  d6.loss_dice: 1.6604  d7.loss_cls: 0.5883  d7.loss_mask: 0.1506  d7.loss_dice: 1.6513  d8.loss_cls: 0.5825  d8.loss_mask: 0.1496  d8.loss_dice: 1.6400
05/11 15:06:49 - mmengine - INFO - Iter(train) [20150/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:41:18  time: 1.5323  data_time: 0.0316  memory: 39347  grad_norm: 66.0210  loss: 25.7636  loss_cls: 0.5641  loss_mask: 0.1676  loss_dice: 1.6635  d0.loss_cls: 1.1407  d0.loss_mask: 0.2126  d0.loss_dice: 1.8821  d1.loss_cls: 0.7975  d1.loss_mask: 0.1984  d1.loss_dice: 1.8724  d2.loss_cls: 0.6131  d2.loss_mask: 0.1822  d2.loss_dice: 1.7964  d3.loss_cls: 0.6035  d3.loss_mask: 0.1763  d3.loss_dice: 1.7261  d4.loss_cls: 0.5817  d4.loss_mask: 0.1726  d4.loss_dice: 1.7095  d5.loss_cls: 0.5717  d5.loss_mask: 0.1727  d5.loss_dice: 1.6983  d6.loss_cls: 0.5820  d6.loss_mask: 0.1718  d6.loss_dice: 1.6903  d7.loss_cls: 0.5720  d7.loss_mask: 0.1688  d7.loss_dice: 1.6843  d8.loss_cls: 0.5605  d8.loss_mask: 0.1681  d8.loss_dice: 1.6628
05/11 15:08:05 - mmengine - INFO - Iter(train) [20200/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:39:59  time: 1.5240  data_time: 0.0314  memory: 38322  grad_norm: 51.1017  loss: 25.2456  loss_cls: 0.5820  loss_mask: 0.1505  loss_dice: 1.6304  d0.loss_cls: 1.1025  d0.loss_mask: 0.1875  d0.loss_dice: 1.8373  d1.loss_cls: 0.7999  d1.loss_mask: 0.1803  d1.loss_dice: 1.8314  d2.loss_cls: 0.6221  d2.loss_mask: 0.1627  d2.loss_dice: 1.7551  d3.loss_cls: 0.6112  d3.loss_mask: 0.1573  d3.loss_dice: 1.6926  d4.loss_cls: 0.5849  d4.loss_mask: 0.1545  d4.loss_dice: 1.6734  d5.loss_cls: 0.5747  d5.loss_mask: 0.1541  d5.loss_dice: 1.6684  d6.loss_cls: 0.5863  d6.loss_mask: 0.1525  d6.loss_dice: 1.6592  d7.loss_cls: 0.5753  d7.loss_mask: 0.1519  d7.loss_dice: 1.6520  d8.loss_cls: 0.5691  d8.loss_mask: 0.1511  d8.loss_dice: 1.6354
05/11 15:09:22 - mmengine - INFO - Iter(train) [20250/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:38:39  time: 1.5335  data_time: 0.0316  memory: 38404  grad_norm: 49.7862  loss: 25.5920  loss_cls: 0.5884  loss_mask: 0.1557  loss_dice: 1.6439  d0.loss_cls: 1.0864  d0.loss_mask: 0.1963  d0.loss_dice: 1.8510  d1.loss_cls: 0.8112  d1.loss_mask: 0.1872  d1.loss_dice: 1.8421  d2.loss_cls: 0.6287  d2.loss_mask: 0.1708  d2.loss_dice: 1.7712  d3.loss_cls: 0.6123  d3.loss_mask: 0.1660  d3.loss_dice: 1.7191  d4.loss_cls: 0.5940  d4.loss_mask: 0.1625  d4.loss_dice: 1.7044  d5.loss_cls: 0.5904  d5.loss_mask: 0.1608  d5.loss_dice: 1.6864  d6.loss_cls: 0.6034  d6.loss_mask: 0.1601  d6.loss_dice: 1.6833  d7.loss_cls: 0.5900  d7.loss_mask: 0.1588  d7.loss_dice: 1.6761  d8.loss_cls: 0.5813  d8.loss_mask: 0.1572  d8.loss_dice: 1.6529
05/11 15:10:39 - mmengine - INFO - Iter(train) [20300/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:37:20  time: 1.5371  data_time: 0.0321  memory: 39309  grad_norm: 48.8444  loss: 27.1855  loss_cls: 0.5908  loss_mask: 0.1749  loss_dice: 1.7722  d0.loss_cls: 1.1407  d0.loss_mask: 0.2155  d0.loss_dice: 2.0142  d1.loss_cls: 0.8245  d1.loss_mask: 0.2078  d1.loss_dice: 1.9893  d2.loss_cls: 0.6468  d2.loss_mask: 0.1885  d2.loss_dice: 1.9047  d3.loss_cls: 0.6312  d3.loss_mask: 0.1817  d3.loss_dice: 1.8434  d4.loss_cls: 0.6100  d4.loss_mask: 0.1792  d4.loss_dice: 1.8150  d5.loss_cls: 0.5954  d5.loss_mask: 0.1788  d5.loss_dice: 1.8047  d6.loss_cls: 0.6049  d6.loss_mask: 0.1775  d6.loss_dice: 1.7985  d7.loss_cls: 0.5937  d7.loss_mask: 0.1768  d7.loss_dice: 1.7907  d8.loss_cls: 0.5867  d8.loss_mask: 0.1749  d8.loss_dice: 1.7724
05/11 15:11:55 - mmengine - INFO - Iter(train) [20350/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:36:01  time: 1.5333  data_time: 0.0322  memory: 38656  grad_norm: 47.9933  loss: 26.6562  loss_cls: 0.5833  loss_mask: 0.1701  loss_dice: 1.7283  d0.loss_cls: 1.1306  d0.loss_mask: 0.2168  d0.loss_dice: 1.9769  d1.loss_cls: 0.8131  d1.loss_mask: 0.2063  d1.loss_dice: 1.9652  d2.loss_cls: 0.6347  d2.loss_mask: 0.1845  d2.loss_dice: 1.8757  d3.loss_cls: 0.6139  d3.loss_mask: 0.1785  d3.loss_dice: 1.8035  d4.loss_cls: 0.5956  d4.loss_mask: 0.1760  d4.loss_dice: 1.7734  d5.loss_cls: 0.5818  d5.loss_mask: 0.1744  d5.loss_dice: 1.7670  d6.loss_cls: 0.5914  d6.loss_mask: 0.1735  d6.loss_dice: 1.7592  d7.loss_cls: 0.5846  d7.loss_mask: 0.1720  d7.loss_dice: 1.7461  d8.loss_cls: 0.5797  d8.loss_mask: 0.1706  d8.loss_dice: 1.7294
05/11 15:13:11 - mmengine - INFO - Iter(train) [20400/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:34:41  time: 1.5244  data_time: 0.0318  memory: 38244  grad_norm: 55.6306  loss: 25.8223  loss_cls: 0.5803  loss_mask: 0.1460  loss_dice: 1.6975  d0.loss_cls: 1.0785  d0.loss_mask: 0.1813  d0.loss_dice: 1.9065  d1.loss_cls: 0.7872  d1.loss_mask: 0.1745  d1.loss_dice: 1.8921  d2.loss_cls: 0.6253  d2.loss_mask: 0.1605  d2.loss_dice: 1.8221  d3.loss_cls: 0.6079  d3.loss_mask: 0.1538  d3.loss_dice: 1.7599  d4.loss_cls: 0.5832  d4.loss_mask: 0.1508  d4.loss_dice: 1.7434  d5.loss_cls: 0.5753  d5.loss_mask: 0.1503  d5.loss_dice: 1.7295  d6.loss_cls: 0.5818  d6.loss_mask: 0.1496  d6.loss_dice: 1.7267  d7.loss_cls: 0.5766  d7.loss_mask: 0.1483  d7.loss_dice: 1.7145  d8.loss_cls: 0.5744  d8.loss_mask: 0.1467  d8.loss_dice: 1.6980
05/11 15:14:28 - mmengine - INFO - Iter(train) [20450/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:33:22  time: 1.5312  data_time: 0.0314  memory: 39061  grad_norm: 45.2731  loss: 26.8213  loss_cls: 0.5979  loss_mask: 0.1743  loss_dice: 1.7321  d0.loss_cls: 1.1193  d0.loss_mask: 0.2186  d0.loss_dice: 1.9695  d1.loss_cls: 0.8242  d1.loss_mask: 0.2094  d1.loss_dice: 1.9483  d2.loss_cls: 0.6458  d2.loss_mask: 0.1899  d2.loss_dice: 1.8651  d3.loss_cls: 0.6342  d3.loss_mask: 0.1843  d3.loss_dice: 1.7997  d4.loss_cls: 0.6177  d4.loss_mask: 0.1797  d4.loss_dice: 1.7823  d5.loss_cls: 0.5992  d5.loss_mask: 0.1790  d5.loss_dice: 1.7696  d6.loss_cls: 0.6002  d6.loss_mask: 0.1764  d6.loss_dice: 1.7656  d7.loss_cls: 0.5995  d7.loss_mask: 0.1766  d7.loss_dice: 1.7546  d8.loss_cls: 0.5974  d8.loss_mask: 0.1756  d8.loss_dice: 1.7352
05/11 15:15:45 - mmengine - INFO - Iter(train) [20500/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:32:03  time: 1.5342  data_time: 0.0305  memory: 38918  grad_norm: 55.1843  loss: 26.8722  loss_cls: 0.5940  loss_mask: 0.1727  loss_dice: 1.7504  d0.loss_cls: 1.0995  d0.loss_mask: 0.2151  d0.loss_dice: 1.9685  d1.loss_cls: 0.8016  d1.loss_mask: 0.2066  d1.loss_dice: 1.9686  d2.loss_cls: 0.6313  d2.loss_mask: 0.1876  d2.loss_dice: 1.8945  d3.loss_cls: 0.6200  d3.loss_mask: 0.1819  d3.loss_dice: 1.8295  d4.loss_cls: 0.6054  d4.loss_mask: 0.1785  d4.loss_dice: 1.8018  d5.loss_cls: 0.5927  d5.loss_mask: 0.1759  d5.loss_dice: 1.7921  d6.loss_cls: 0.5944  d6.loss_mask: 0.1748  d6.loss_dice: 1.7843  d7.loss_cls: 0.5925  d7.loss_mask: 0.1742  d7.loss_dice: 1.7707  d8.loss_cls: 0.5922  d8.loss_mask: 0.1735  d8.loss_dice: 1.7473
05/11 15:17:01 - mmengine - INFO - Iter(train) [20550/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:30:43  time: 1.5310  data_time: 0.0309  memory: 39075  grad_norm: 45.5247  loss: 27.5561  loss_cls: 0.5939  loss_mask: 0.1672  loss_dice: 1.8074  d0.loss_cls: 1.1354  d0.loss_mask: 0.2092  d0.loss_dice: 2.0480  d1.loss_cls: 0.8388  d1.loss_mask: 0.2011  d1.loss_dice: 2.0481  d2.loss_cls: 0.6517  d2.loss_mask: 0.1821  d2.loss_dice: 1.9534  d3.loss_cls: 0.6337  d3.loss_mask: 0.1764  d3.loss_dice: 1.8826  d4.loss_cls: 0.6088  d4.loss_mask: 0.1718  d4.loss_dice: 1.8605  d5.loss_cls: 0.5903  d5.loss_mask: 0.1720  d5.loss_dice: 1.8490  d6.loss_cls: 0.5975  d6.loss_mask: 0.1707  d6.loss_dice: 1.8380  d7.loss_cls: 0.5981  d7.loss_mask: 0.1690  d7.loss_dice: 1.8292  d8.loss_cls: 0.5931  d8.loss_mask: 0.1682  d8.loss_dice: 1.8104
05/11 15:18:17 - mmengine - INFO - Iter(train) [20600/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:29:24  time: 1.5237  data_time: 0.0305  memory: 39795  grad_norm: 43.1911  loss: 27.8084  loss_cls: 0.5813  loss_mask: 0.1807  loss_dice: 1.8266  d0.loss_cls: 1.1471  d0.loss_mask: 0.2245  d0.loss_dice: 2.0767  d1.loss_cls: 0.8193  d1.loss_mask: 0.2135  d1.loss_dice: 2.0662  d2.loss_cls: 0.6397  d2.loss_mask: 0.1955  d2.loss_dice: 1.9751  d3.loss_cls: 0.6242  d3.loss_mask: 0.1892  d3.loss_dice: 1.9080  d4.loss_cls: 0.6016  d4.loss_mask: 0.1863  d4.loss_dice: 1.8817  d5.loss_cls: 0.5864  d5.loss_mask: 0.1851  d5.loss_dice: 1.8707  d6.loss_cls: 0.5865  d6.loss_mask: 0.1834  d6.loss_dice: 1.8573  d7.loss_cls: 0.5858  d7.loss_mask: 0.1829  d7.loss_dice: 1.8435  d8.loss_cls: 0.5783  d8.loss_mask: 0.1820  d8.loss_dice: 1.8292
05/11 15:19:35 - mmengine - INFO - Iter(train) [20650/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:28:05  time: 1.5458  data_time: 0.0314  memory: 38460  grad_norm: 64.4286  loss: 24.5455  loss_cls: 0.5558  loss_mask: 0.1500  loss_dice: 1.5801  d0.loss_cls: 1.1018  d0.loss_mask: 0.1899  d0.loss_dice: 1.7869  d1.loss_cls: 0.7992  d1.loss_mask: 0.1790  d1.loss_dice: 1.7711  d2.loss_cls: 0.6153  d2.loss_mask: 0.1622  d2.loss_dice: 1.6986  d3.loss_cls: 0.5958  d3.loss_mask: 0.1568  d3.loss_dice: 1.6341  d4.loss_cls: 0.5713  d4.loss_mask: 0.1547  d4.loss_dice: 1.6181  d5.loss_cls: 0.5558  d5.loss_mask: 0.1540  d5.loss_dice: 1.6070  d6.loss_cls: 0.5630  d6.loss_mask: 0.1532  d6.loss_dice: 1.6028  d7.loss_cls: 0.5609  d7.loss_mask: 0.1523  d7.loss_dice: 1.5908  d8.loss_cls: 0.5560  d8.loss_mask: 0.1503  d8.loss_dice: 1.5786
05/11 15:20:52 - mmengine - INFO - Iter(train) [20700/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:26:46  time: 1.5379  data_time: 0.0359  memory: 38885  grad_norm: 45.5689  loss: 26.2446  loss_cls: 0.5843  loss_mask: 0.1673  loss_dice: 1.6990  d0.loss_cls: 1.1212  d0.loss_mask: 0.2109  d0.loss_dice: 1.9200  d1.loss_cls: 0.8257  d1.loss_mask: 0.2006  d1.loss_dice: 1.9104  d2.loss_cls: 0.6294  d2.loss_mask: 0.1827  d2.loss_dice: 1.8294  d3.loss_cls: 0.6180  d3.loss_mask: 0.1756  d3.loss_dice: 1.7651  d4.loss_cls: 0.5972  d4.loss_mask: 0.1731  d4.loss_dice: 1.7456  d5.loss_cls: 0.5856  d5.loss_mask: 0.1710  d5.loss_dice: 1.7350  d6.loss_cls: 0.5884  d6.loss_mask: 0.1693  d6.loss_dice: 1.7248  d7.loss_cls: 0.5863  d7.loss_mask: 0.1694  d7.loss_dice: 1.7112  d8.loss_cls: 0.5820  d8.loss_mask: 0.1675  d8.loss_dice: 1.6988
05/11 15:22:08 - mmengine - INFO - Iter(train) [20750/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:25:27  time: 1.5345  data_time: 0.0312  memory: 39527  grad_norm: 51.9680  loss: 24.5985  loss_cls: 0.5578  loss_mask: 0.1536  loss_dice: 1.5872  d0.loss_cls: 1.0883  d0.loss_mask: 0.1888  d0.loss_dice: 1.7861  d1.loss_cls: 0.7772  d1.loss_mask: 0.1822  d1.loss_dice: 1.7755  d2.loss_cls: 0.6070  d2.loss_mask: 0.1652  d2.loss_dice: 1.7070  d3.loss_cls: 0.5899  d3.loss_mask: 0.1622  d3.loss_dice: 1.6487  d4.loss_cls: 0.5693  d4.loss_mask: 0.1590  d4.loss_dice: 1.6280  d5.loss_cls: 0.5593  d5.loss_mask: 0.1581  d5.loss_dice: 1.6097  d6.loss_cls: 0.5616  d6.loss_mask: 0.1566  d6.loss_dice: 1.6094  d7.loss_cls: 0.5593  d7.loss_mask: 0.1553  d7.loss_dice: 1.6003  d8.loss_cls: 0.5565  d8.loss_mask: 0.1540  d8.loss_dice: 1.5855
05/11 15:23:25 - mmengine - INFO - Iter(train) [20800/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:24:07  time: 1.5247  data_time: 0.0309  memory: 38686  grad_norm: 59.0103  loss: 26.4229  loss_cls: 0.5949  loss_mask: 0.1595  loss_dice: 1.7252  d0.loss_cls: 1.0985  d0.loss_mask: 0.1980  d0.loss_dice: 1.9352  d1.loss_cls: 0.8111  d1.loss_mask: 0.1880  d1.loss_dice: 1.9305  d2.loss_cls: 0.6413  d2.loss_mask: 0.1718  d2.loss_dice: 1.8533  d3.loss_cls: 0.6231  d3.loss_mask: 0.1690  d3.loss_dice: 1.7841  d4.loss_cls: 0.6068  d4.loss_mask: 0.1637  d4.loss_dice: 1.7663  d5.loss_cls: 0.5909  d5.loss_mask: 0.1636  d5.loss_dice: 1.7582  d6.loss_cls: 0.5961  d6.loss_mask: 0.1625  d6.loss_dice: 1.7546  d7.loss_cls: 0.5955  d7.loss_mask: 0.1615  d7.loss_dice: 1.7405  d8.loss_cls: 0.5951  d8.loss_mask: 0.1596  d8.loss_dice: 1.7244
05/11 15:24:41 - mmengine - INFO - Iter(train) [20850/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:22:48  time: 1.5275  data_time: 0.0314  memory: 38523  grad_norm: 49.2741  loss: 25.1227  loss_cls: 0.5737  loss_mask: 0.1573  loss_dice: 1.6089  d0.loss_cls: 1.1079  d0.loss_mask: 0.2025  d0.loss_dice: 1.8225  d1.loss_cls: 0.8100  d1.loss_mask: 0.1869  d1.loss_dice: 1.8165  d2.loss_cls: 0.6298  d2.loss_mask: 0.1701  d2.loss_dice: 1.7295  d3.loss_cls: 0.6068  d3.loss_mask: 0.1655  d3.loss_dice: 1.6758  d4.loss_cls: 0.5934  d4.loss_mask: 0.1625  d4.loss_dice: 1.6484  d5.loss_cls: 0.5781  d5.loss_mask: 0.1608  d5.loss_dice: 1.6389  d6.loss_cls: 0.5796  d6.loss_mask: 0.1607  d6.loss_dice: 1.6393  d7.loss_cls: 0.5751  d7.loss_mask: 0.1600  d7.loss_dice: 1.6224  d8.loss_cls: 0.5709  d8.loss_mask: 0.1582  d8.loss_dice: 1.6108
05/11 15:25:58 - mmengine - INFO - Iter(train) [20900/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:21:29  time: 1.5308  data_time: 0.0309  memory: 38204  grad_norm: 47.9035  loss: 25.2182  loss_cls: 0.5698  loss_mask: 0.1536  loss_dice: 1.6290  d0.loss_cls: 1.0989  d0.loss_mask: 0.1929  d0.loss_dice: 1.8382  d1.loss_cls: 0.8040  d1.loss_mask: 0.1817  d1.loss_dice: 1.8315  d2.loss_cls: 0.6193  d2.loss_mask: 0.1669  d2.loss_dice: 1.7525  d3.loss_cls: 0.5973  d3.loss_mask: 0.1623  d3.loss_dice: 1.6990  d4.loss_cls: 0.5862  d4.loss_mask: 0.1603  d4.loss_dice: 1.6664  d5.loss_cls: 0.5714  d5.loss_mask: 0.1590  d5.loss_dice: 1.6628  d6.loss_cls: 0.5744  d6.loss_mask: 0.1580  d6.loss_dice: 1.6589  d7.loss_cls: 0.5684  d7.loss_mask: 0.1566  d7.loss_dice: 1.6485  d8.loss_cls: 0.5681  d8.loss_mask: 0.1543  d8.loss_dice: 1.6279
05/11 15:27:14 - mmengine - INFO - Iter(train) [20950/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:20:10  time: 1.5305  data_time: 0.0313  memory: 38322  grad_norm: 43.5763  loss: 24.9151  loss_cls: 0.5688  loss_mask: 0.1582  loss_dice: 1.5977  d0.loss_cls: 1.0899  d0.loss_mask: 0.1947  d0.loss_dice: 1.8099  d1.loss_cls: 0.7980  d1.loss_mask: 0.1831  d1.loss_dice: 1.7951  d2.loss_cls: 0.6189  d2.loss_mask: 0.1714  d2.loss_dice: 1.7178  d3.loss_cls: 0.6023  d3.loss_mask: 0.1660  d3.loss_dice: 1.6606  d4.loss_cls: 0.5910  d4.loss_mask: 0.1626  d4.loss_dice: 1.6326  d5.loss_cls: 0.5765  d5.loss_mask: 0.1613  d5.loss_dice: 1.6282  d6.loss_cls: 0.5802  d6.loss_mask: 0.1611  d6.loss_dice: 1.6155  d7.loss_cls: 0.5709  d7.loss_mask: 0.1599  d7.loss_dice: 1.6157  d8.loss_cls: 0.5700  d8.loss_mask: 0.1586  d8.loss_dice: 1.5989
05/11 15:28:31 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 15:28:31 - mmengine - INFO - Iter(train) [21000/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:18:50  time: 1.5307  data_time: 0.0306  memory: 38577  grad_norm: 49.8157  loss: 25.9245  loss_cls: 0.5755  loss_mask: 0.1557  loss_dice: 1.6920  d0.loss_cls: 1.1124  d0.loss_mask: 0.1923  d0.loss_dice: 1.9116  d1.loss_cls: 0.8010  d1.loss_mask: 0.1840  d1.loss_dice: 1.8925  d2.loss_cls: 0.6251  d2.loss_mask: 0.1700  d2.loss_dice: 1.8253  d3.loss_cls: 0.6053  d3.loss_mask: 0.1623  d3.loss_dice: 1.7629  d4.loss_cls: 0.5855  d4.loss_mask: 0.1592  d4.loss_dice: 1.7408  d5.loss_cls: 0.5715  d5.loss_mask: 0.1581  d5.loss_dice: 1.7232  d6.loss_cls: 0.5798  d6.loss_mask: 0.1578  d6.loss_dice: 1.7173  d7.loss_cls: 0.5742  d7.loss_mask: 0.1571  d7.loss_dice: 1.7100  d8.loss_cls: 0.5715  d8.loss_mask: 0.1557  d8.loss_dice: 1.6949
05/11 15:29:48 - mmengine - INFO - Iter(train) [21050/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:17:31  time: 1.5416  data_time: 0.0302  memory: 39424  grad_norm: 47.2838  loss: 26.7951  loss_cls: 0.5813  loss_mask: 0.1586  loss_dice: 1.7548  d0.loss_cls: 1.1331  d0.loss_mask: 0.1965  d0.loss_dice: 1.9985  d1.loss_cls: 0.8037  d1.loss_mask: 0.1897  d1.loss_dice: 1.9861  d2.loss_cls: 0.6358  d2.loss_mask: 0.1738  d2.loss_dice: 1.8958  d3.loss_cls: 0.6120  d3.loss_mask: 0.1662  d3.loss_dice: 1.8395  d4.loss_cls: 0.5955  d4.loss_mask: 0.1635  d4.loss_dice: 1.8065  d5.loss_cls: 0.5843  d5.loss_mask: 0.1626  d5.loss_dice: 1.7926  d6.loss_cls: 0.5915  d6.loss_mask: 0.1623  d6.loss_dice: 1.7842  d7.loss_cls: 0.5881  d7.loss_mask: 0.1615  d7.loss_dice: 1.7777  d8.loss_cls: 0.5762  d8.loss_mask: 0.1593  d8.loss_dice: 1.7640
05/11 15:31:04 - mmengine - INFO - Iter(train) [21100/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:16:12  time: 1.5314  data_time: 0.0310  memory: 38588  grad_norm: 50.4677  loss: 25.5191  loss_cls: 0.5539  loss_mask: 0.1618  loss_dice: 1.6674  d0.loss_cls: 1.0920  d0.loss_mask: 0.2025  d0.loss_dice: 1.8667  d1.loss_cls: 0.7854  d1.loss_mask: 0.1921  d1.loss_dice: 1.8600  d2.loss_cls: 0.6113  d2.loss_mask: 0.1768  d2.loss_dice: 1.7825  d3.loss_cls: 0.5910  d3.loss_mask: 0.1687  d3.loss_dice: 1.7247  d4.loss_cls: 0.5746  d4.loss_mask: 0.1673  d4.loss_dice: 1.7088  d5.loss_cls: 0.5591  d5.loss_mask: 0.1654  d5.loss_dice: 1.6953  d6.loss_cls: 0.5752  d6.loss_mask: 0.1642  d6.loss_dice: 1.6875  d7.loss_cls: 0.5629  d7.loss_mask: 0.1638  d7.loss_dice: 1.6780  d8.loss_cls: 0.5568  d8.loss_mask: 0.1611  d8.loss_dice: 1.6625
05/11 15:32:21 - mmengine - INFO - Iter(train) [21150/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:14:53  time: 1.5245  data_time: 0.0308  memory: 38527  grad_norm: 45.2486  loss: 26.4937  loss_cls: 0.5738  loss_mask: 0.1635  loss_dice: 1.7363  d0.loss_cls: 1.1212  d0.loss_mask: 0.2033  d0.loss_dice: 1.9617  d1.loss_cls: 0.8150  d1.loss_mask: 0.1966  d1.loss_dice: 1.9418  d2.loss_cls: 0.6260  d2.loss_mask: 0.1756  d2.loss_dice: 1.8659  d3.loss_cls: 0.6076  d3.loss_mask: 0.1696  d3.loss_dice: 1.8090  d4.loss_cls: 0.5857  d4.loss_mask: 0.1665  d4.loss_dice: 1.7840  d5.loss_cls: 0.5765  d5.loss_mask: 0.1659  d5.loss_dice: 1.7668  d6.loss_cls: 0.5824  d6.loss_mask: 0.1653  d6.loss_dice: 1.7631  d7.loss_cls: 0.5822  d7.loss_mask: 0.1646  d7.loss_dice: 1.7501  d8.loss_cls: 0.5729  d8.loss_mask: 0.1633  d8.loss_dice: 1.7375
05/11 15:33:38 - mmengine - INFO - Iter(train) [21200/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:13:34  time: 1.5412  data_time: 0.0318  memory: 38566  grad_norm: 52.2954  loss: 24.6085  loss_cls: 0.5628  loss_mask: 0.1502  loss_dice: 1.5753  d0.loss_cls: 1.0988  d0.loss_mask: 0.1893  d0.loss_dice: 1.7937  d1.loss_cls: 0.7964  d1.loss_mask: 0.1823  d1.loss_dice: 1.7707  d2.loss_cls: 0.6144  d2.loss_mask: 0.1654  d2.loss_dice: 1.7064  d3.loss_cls: 0.5992  d3.loss_mask: 0.1595  d3.loss_dice: 1.6390  d4.loss_cls: 0.5784  d4.loss_mask: 0.1568  d4.loss_dice: 1.6182  d5.loss_cls: 0.5624  d5.loss_mask: 0.1557  d5.loss_dice: 1.6066  d6.loss_cls: 0.5745  d6.loss_mask: 0.1542  d6.loss_dice: 1.6021  d7.loss_cls: 0.5653  d7.loss_mask: 0.1534  d7.loss_dice: 1.5930  d8.loss_cls: 0.5598  d8.loss_mask: 0.1514  d8.loss_dice: 1.5733
05/11 15:34:54 - mmengine - INFO - Iter(train) [21250/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:12:15  time: 1.5251  data_time: 0.0316  memory: 38917  grad_norm: 47.3105  loss: 26.9026  loss_cls: 0.5891  loss_mask: 0.1789  loss_dice: 1.7433  d0.loss_cls: 1.1209  d0.loss_mask: 0.2261  d0.loss_dice: 1.9954  d1.loss_cls: 0.8289  d1.loss_mask: 0.2154  d1.loss_dice: 1.9738  d2.loss_cls: 0.6399  d2.loss_mask: 0.1918  d2.loss_dice: 1.8773  d3.loss_cls: 0.6236  d3.loss_mask: 0.1861  d3.loss_dice: 1.8048  d4.loss_cls: 0.6043  d4.loss_mask: 0.1830  d4.loss_dice: 1.7822  d5.loss_cls: 0.5911  d5.loss_mask: 0.1829  d5.loss_dice: 1.7762  d6.loss_cls: 0.5990  d6.loss_mask: 0.1813  d6.loss_dice: 1.7671  d7.loss_cls: 0.5966  d7.loss_mask: 0.1801  d7.loss_dice: 1.7566  d8.loss_cls: 0.5888  d8.loss_mask: 0.1791  d8.loss_dice: 1.7392
05/11 15:36:12 - mmengine - INFO - Iter(train) [21300/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:10:56  time: 1.5542  data_time: 0.0317  memory: 38544  grad_norm: 48.9591  loss: 25.6488  loss_cls: 0.5757  loss_mask: 0.1589  loss_dice: 1.6509  d0.loss_cls: 1.1160  d0.loss_mask: 0.1994  d0.loss_dice: 1.8953  d1.loss_cls: 0.8075  d1.loss_mask: 0.1928  d1.loss_dice: 1.8746  d2.loss_cls: 0.6252  d2.loss_mask: 0.1716  d2.loss_dice: 1.7897  d3.loss_cls: 0.6122  d3.loss_mask: 0.1657  d3.loss_dice: 1.7198  d4.loss_cls: 0.5875  d4.loss_mask: 0.1629  d4.loss_dice: 1.6952  d5.loss_cls: 0.5737  d5.loss_mask: 0.1618  d5.loss_dice: 1.6836  d6.loss_cls: 0.5891  d6.loss_mask: 0.1611  d6.loss_dice: 1.6783  d7.loss_cls: 0.5805  d7.loss_mask: 0.1609  d7.loss_dice: 1.6692  d8.loss_cls: 0.5783  d8.loss_mask: 0.1595  d8.loss_dice: 1.6518
05/11 15:37:28 - mmengine - INFO - Iter(train) [21350/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:09:37  time: 1.5354  data_time: 0.0311  memory: 38420  grad_norm: 57.3152  loss: 25.2579  loss_cls: 0.5728  loss_mask: 0.1630  loss_dice: 1.6154  d0.loss_cls: 1.1082  d0.loss_mask: 0.2063  d0.loss_dice: 1.8440  d1.loss_cls: 0.7986  d1.loss_mask: 0.1945  d1.loss_dice: 1.8268  d2.loss_cls: 0.6224  d2.loss_mask: 0.1760  d2.loss_dice: 1.7452  d3.loss_cls: 0.6086  d3.loss_mask: 0.1706  d3.loss_dice: 1.6836  d4.loss_cls: 0.5854  d4.loss_mask: 0.1671  d4.loss_dice: 1.6623  d5.loss_cls: 0.5710  d5.loss_mask: 0.1659  d5.loss_dice: 1.6525  d6.loss_cls: 0.5801  d6.loss_mask: 0.1657  d6.loss_dice: 1.6461  d7.loss_cls: 0.5781  d7.loss_mask: 0.1655  d7.loss_dice: 1.6315  d8.loss_cls: 0.5711  d8.loss_mask: 0.1639  d8.loss_dice: 1.6156
05/11 15:38:46 - mmengine - INFO - Iter(train) [21400/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:08:18  time: 1.5480  data_time: 0.0373  memory: 39391  grad_norm: 51.7093  loss: 27.1989  loss_cls: 0.5811  loss_mask: 0.1787  loss_dice: 1.7826  d0.loss_cls: 1.1251  d0.loss_mask: 0.2254  d0.loss_dice: 2.0188  d1.loss_cls: 0.8028  d1.loss_mask: 0.2147  d1.loss_dice: 2.0092  d2.loss_cls: 0.6298  d2.loss_mask: 0.1950  d2.loss_dice: 1.9247  d3.loss_cls: 0.6114  d3.loss_mask: 0.1879  d3.loss_dice: 1.8533  d4.loss_cls: 0.5900  d4.loss_mask: 0.1834  d4.loss_dice: 1.8295  d5.loss_cls: 0.5747  d5.loss_mask: 0.1835  d5.loss_dice: 1.8178  d6.loss_cls: 0.5847  d6.loss_mask: 0.1823  d6.loss_dice: 1.7989  d7.loss_cls: 0.5842  d7.loss_mask: 0.1804  d7.loss_dice: 1.8052  d8.loss_cls: 0.5766  d8.loss_mask: 0.1793  d8.loss_dice: 1.7880
05/11 15:40:03 - mmengine - INFO - Iter(train) [21450/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:06:59  time: 1.5374  data_time: 0.0307  memory: 38957  grad_norm: 49.3325  loss: 25.5050  loss_cls: 0.5768  loss_mask: 0.1611  loss_dice: 1.6366  d0.loss_cls: 1.1292  d0.loss_mask: 0.1991  d0.loss_dice: 1.8653  d1.loss_cls: 0.8181  d1.loss_mask: 0.1927  d1.loss_dice: 1.8426  d2.loss_cls: 0.6326  d2.loss_mask: 0.1756  d2.loss_dice: 1.7608  d3.loss_cls: 0.6108  d3.loss_mask: 0.1705  d3.loss_dice: 1.7032  d4.loss_cls: 0.5952  d4.loss_mask: 0.1671  d4.loss_dice: 1.6770  d5.loss_cls: 0.5835  d5.loss_mask: 0.1656  d5.loss_dice: 1.6599  d6.loss_cls: 0.5915  d6.loss_mask: 0.1653  d6.loss_dice: 1.6585  d7.loss_cls: 0.5854  d7.loss_mask: 0.1641  d7.loss_dice: 1.6456  d8.loss_cls: 0.5805  d8.loss_mask: 0.1617  d8.loss_dice: 1.6289
05/11 15:41:19 - mmengine - INFO - Iter(train) [21500/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:05:40  time: 1.5334  data_time: 0.0306  memory: 39045  grad_norm: 56.8071  loss: 28.3422  loss_cls: 0.5930  loss_mask: 0.1717  loss_dice: 1.8818  d0.loss_cls: 1.1379  d0.loss_mask: 0.2102  d0.loss_dice: 2.1373  d1.loss_cls: 0.8259  d1.loss_mask: 0.2061  d1.loss_dice: 2.1115  d2.loss_cls: 0.6410  d2.loss_mask: 0.1894  d2.loss_dice: 2.0376  d3.loss_cls: 0.6231  d3.loss_mask: 0.1828  d3.loss_dice: 1.9687  d4.loss_cls: 0.6096  d4.loss_mask: 0.1784  d4.loss_dice: 1.9329  d5.loss_cls: 0.5928  d5.loss_mask: 0.1761  d5.loss_dice: 1.9115  d6.loss_cls: 0.6050  d6.loss_mask: 0.1763  d6.loss_dice: 1.9134  d7.loss_cls: 0.6049  d7.loss_mask: 0.1738  d7.loss_dice: 1.8986  d8.loss_cls: 0.5945  d8.loss_mask: 0.1719  d8.loss_dice: 1.8845
05/11 15:42:36 - mmengine - INFO - Iter(train) [21550/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:04:21  time: 1.5377  data_time: 0.0312  memory: 38824  grad_norm: 45.3585  loss: 27.9501  loss_cls: 0.5936  loss_mask: 0.1710  loss_dice: 1.8274  d0.loss_cls: 1.1697  d0.loss_mask: 0.2191  d0.loss_dice: 2.1010  d1.loss_cls: 0.8308  d1.loss_mask: 0.2092  d1.loss_dice: 2.0985  d2.loss_cls: 0.6548  d2.loss_mask: 0.1888  d2.loss_dice: 1.9979  d3.loss_cls: 0.6278  d3.loss_mask: 0.1788  d3.loss_dice: 1.9215  d4.loss_cls: 0.6128  d4.loss_mask: 0.1755  d4.loss_dice: 1.8778  d5.loss_cls: 0.6020  d5.loss_mask: 0.1745  d5.loss_dice: 1.8634  d6.loss_cls: 0.6071  d6.loss_mask: 0.1747  d6.loss_dice: 1.8568  d7.loss_cls: 0.6034  d7.loss_mask: 0.1735  d7.loss_dice: 1.8437  d8.loss_cls: 0.5960  d8.loss_mask: 0.1710  d8.loss_dice: 1.8280
05/11 15:43:54 - mmengine - INFO - Iter(train) [21600/24000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:03:02  time: 1.5486  data_time: 0.0314  memory: 38900  grad_norm: 42.6339  loss: 26.2489  loss_cls: 0.5833  loss_mask: 0.1542  loss_dice: 1.7157  d0.loss_cls: 1.1315  d0.loss_mask: 0.1945  d0.loss_dice: 1.9331  d1.loss_cls: 0.8260  d1.loss_mask: 0.1875  d1.loss_dice: 1.9295  d2.loss_cls: 0.6415  d2.loss_mask: 0.1681  d2.loss_dice: 1.8378  d3.loss_cls: 0.6216  d3.loss_mask: 0.1611  d3.loss_dice: 1.7776  d4.loss_cls: 0.5981  d4.loss_mask: 0.1574  d4.loss_dice: 1.7467  d5.loss_cls: 0.5888  d5.loss_mask: 0.1564  d5.loss_dice: 1.7313  d6.loss_cls: 0.5978  d6.loss_mask: 0.1570  d6.loss_dice: 1.7335  d7.loss_cls: 0.5898  d7.loss_mask: 0.1561  d7.loss_dice: 1.7248  d8.loss_cls: 0.5786  d8.loss_mask: 0.1548  d8.loss_dice: 1.7147
05/11 15:45:11 - mmengine - INFO - Iter(train) [21650/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 1:01:43  time: 1.5411  data_time: 0.0366  memory: 38549  grad_norm: 45.1149  loss: 26.1264  loss_cls: 0.5737  loss_mask: 0.1506  loss_dice: 1.7192  d0.loss_cls: 1.1036  d0.loss_mask: 0.1869  d0.loss_dice: 1.9298  d1.loss_cls: 0.7901  d1.loss_mask: 0.1788  d1.loss_dice: 1.9290  d2.loss_cls: 0.6232  d2.loss_mask: 0.1622  d2.loss_dice: 1.8501  d3.loss_cls: 0.6038  d3.loss_mask: 0.1567  d3.loss_dice: 1.7916  d4.loss_cls: 0.5900  d4.loss_mask: 0.1540  d4.loss_dice: 1.7581  d5.loss_cls: 0.5772  d5.loss_mask: 0.1545  d5.loss_dice: 1.7553  d6.loss_cls: 0.5855  d6.loss_mask: 0.1536  d6.loss_dice: 1.7433  d7.loss_cls: 0.5793  d7.loss_mask: 0.1530  d7.loss_dice: 1.7323  d8.loss_cls: 0.5729  d8.loss_mask: 0.1509  d8.loss_dice: 1.7172
05/11 15:46:28 - mmengine - INFO - Iter(train) [21700/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 1:00:24  time: 1.5438  data_time: 0.0302  memory: 38717  grad_norm: 42.4622  loss: 25.8241  loss_cls: 0.5760  loss_mask: 0.1628  loss_dice: 1.6655  d0.loss_cls: 1.1267  d0.loss_mask: 0.2055  d0.loss_dice: 1.8930  d1.loss_cls: 0.8183  d1.loss_mask: 0.1952  d1.loss_dice: 1.8823  d2.loss_cls: 0.6337  d2.loss_mask: 0.1758  d2.loss_dice: 1.7949  d3.loss_cls: 0.6133  d3.loss_mask: 0.1687  d3.loss_dice: 1.7392  d4.loss_cls: 0.5920  d4.loss_mask: 0.1655  d4.loss_dice: 1.7022  d5.loss_cls: 0.5778  d5.loss_mask: 0.1660  d5.loss_dice: 1.6933  d6.loss_cls: 0.5859  d6.loss_mask: 0.1661  d6.loss_dice: 1.6936  d7.loss_cls: 0.5818  d7.loss_mask: 0.1651  d7.loss_dice: 1.6782  d8.loss_cls: 0.5750  d8.loss_mask: 0.1635  d8.loss_dice: 1.6670
05/11 15:47:44 - mmengine - INFO - Iter(train) [21750/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:59:05  time: 1.5250  data_time: 0.0313  memory: 38352  grad_norm: 45.7498  loss: 26.0353  loss_cls: 0.5699  loss_mask: 0.1638  loss_dice: 1.7032  d0.loss_cls: 1.1033  d0.loss_mask: 0.2020  d0.loss_dice: 1.8966  d1.loss_cls: 0.8004  d1.loss_mask: 0.1933  d1.loss_dice: 1.8969  d2.loss_cls: 0.6165  d2.loss_mask: 0.1769  d2.loss_dice: 1.8287  d3.loss_cls: 0.6001  d3.loss_mask: 0.1707  d3.loss_dice: 1.7683  d4.loss_cls: 0.5782  d4.loss_mask: 0.1682  d4.loss_dice: 1.7419  d5.loss_cls: 0.5715  d5.loss_mask: 0.1684  d5.loss_dice: 1.7333  d6.loss_cls: 0.5778  d6.loss_mask: 0.1674  d6.loss_dice: 1.7269  d7.loss_cls: 0.5772  d7.loss_mask: 0.1661  d7.loss_dice: 1.7266  d8.loss_cls: 0.5695  d8.loss_mask: 0.1642  d8.loss_dice: 1.7073
05/11 15:49:01 - mmengine - INFO - Iter(train) [21800/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:57:46  time: 1.5411  data_time: 0.0299  memory: 38670  grad_norm: 42.6737  loss: 25.7995  loss_cls: 0.5766  loss_mask: 0.1719  loss_dice: 1.6512  d0.loss_cls: 1.1213  d0.loss_mask: 0.2147  d0.loss_dice: 1.8941  d1.loss_cls: 0.8087  d1.loss_mask: 0.2063  d1.loss_dice: 1.8723  d2.loss_cls: 0.6261  d2.loss_mask: 0.1870  d2.loss_dice: 1.7865  d3.loss_cls: 0.6118  d3.loss_mask: 0.1811  d3.loss_dice: 1.7230  d4.loss_cls: 0.5934  d4.loss_mask: 0.1764  d4.loss_dice: 1.6904  d5.loss_cls: 0.5825  d5.loss_mask: 0.1763  d5.loss_dice: 1.6781  d6.loss_cls: 0.5847  d6.loss_mask: 0.1765  d6.loss_dice: 1.6771  d7.loss_cls: 0.5853  d7.loss_mask: 0.1745  d7.loss_dice: 1.6662  d8.loss_cls: 0.5818  d8.loss_mask: 0.1724  d8.loss_dice: 1.6512
05/11 15:50:18 - mmengine - INFO - Iter(train) [21850/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:56:27  time: 1.5307  data_time: 0.0284  memory: 38329  grad_norm: 54.4449  loss: 25.4366  loss_cls: 0.5645  loss_mask: 0.1476  loss_dice: 1.6585  d0.loss_cls: 1.1053  d0.loss_mask: 0.1859  d0.loss_dice: 1.8707  d1.loss_cls: 0.7929  d1.loss_mask: 0.1799  d1.loss_dice: 1.8547  d2.loss_cls: 0.6184  d2.loss_mask: 0.1623  d2.loss_dice: 1.7880  d3.loss_cls: 0.5945  d3.loss_mask: 0.1563  d3.loss_dice: 1.7238  d4.loss_cls: 0.5809  d4.loss_mask: 0.1535  d4.loss_dice: 1.7013  d5.loss_cls: 0.5671  d5.loss_mask: 0.1523  d5.loss_dice: 1.6877  d6.loss_cls: 0.5752  d6.loss_mask: 0.1519  d6.loss_dice: 1.6854  d7.loss_cls: 0.5754  d7.loss_mask: 0.1508  d7.loss_dice: 1.6732  d8.loss_cls: 0.5644  d8.loss_mask: 0.1486  d8.loss_dice: 1.6655
05/11 15:51:36 - mmengine - INFO - Iter(train) [21900/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:55:08  time: 1.5620  data_time: 0.0285  memory: 39886  grad_norm: 44.2552  loss: 27.7591  loss_cls: 0.5840  loss_mask: 0.1577  loss_dice: 1.8340  d0.loss_cls: 1.1790  d0.loss_mask: 0.1987  d0.loss_dice: 2.0994  d1.loss_cls: 0.8212  d1.loss_mask: 0.1914  d1.loss_dice: 2.0802  d2.loss_cls: 0.6474  d2.loss_mask: 0.1730  d2.loss_dice: 1.9923  d3.loss_cls: 0.6246  d3.loss_mask: 0.1656  d3.loss_dice: 1.9259  d4.loss_cls: 0.6038  d4.loss_mask: 0.1625  d4.loss_dice: 1.8800  d5.loss_cls: 0.5937  d5.loss_mask: 0.1612  d5.loss_dice: 1.8700  d6.loss_cls: 0.6024  d6.loss_mask: 0.1607  d6.loss_dice: 1.8645  d7.loss_cls: 0.5932  d7.loss_mask: 0.1594  d7.loss_dice: 1.8549  d8.loss_cls: 0.5852  d8.loss_mask: 0.1580  d8.loss_dice: 1.8353
05/11 15:52:52 - mmengine - INFO - Iter(train) [21950/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:53:49  time: 1.5210  data_time: 0.0311  memory: 38829  grad_norm: 40.4010  loss: 24.7449  loss_cls: 0.5506  loss_mask: 0.1648  loss_dice: 1.5941  d0.loss_cls: 1.0896  d0.loss_mask: 0.2058  d0.loss_dice: 1.8003  d1.loss_cls: 0.7707  d1.loss_mask: 0.1934  d1.loss_dice: 1.7952  d2.loss_cls: 0.5957  d2.loss_mask: 0.1775  d2.loss_dice: 1.7108  d3.loss_cls: 0.5829  d3.loss_mask: 0.1711  d3.loss_dice: 1.6524  d4.loss_cls: 0.5668  d4.loss_mask: 0.1691  d4.loss_dice: 1.6294  d5.loss_cls: 0.5578  d5.loss_mask: 0.1674  d5.loss_dice: 1.6163  d6.loss_cls: 0.5636  d6.loss_mask: 0.1677  d6.loss_dice: 1.6161  d7.loss_cls: 0.5584  d7.loss_mask: 0.1664  d7.loss_dice: 1.6023  d8.loss_cls: 0.5535  d8.loss_mask: 0.1643  d8.loss_dice: 1.5910
05/11 15:54:09 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 15:54:09 - mmengine - INFO - Iter(train) [22000/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:52:30  time: 1.5366  data_time: 0.0315  memory: 38598  grad_norm: 47.9002  loss: 24.7353  loss_cls: 0.5582  loss_mask: 0.1541  loss_dice: 1.5963  d0.loss_cls: 1.0909  d0.loss_mask: 0.1926  d0.loss_dice: 1.7967  d1.loss_cls: 0.7968  d1.loss_mask: 0.1851  d1.loss_dice: 1.7801  d2.loss_cls: 0.6153  d2.loss_mask: 0.1668  d2.loss_dice: 1.7041  d3.loss_cls: 0.5926  d3.loss_mask: 0.1618  d3.loss_dice: 1.6563  d4.loss_cls: 0.5771  d4.loss_mask: 0.1580  d4.loss_dice: 1.6280  d5.loss_cls: 0.5679  d5.loss_mask: 0.1578  d5.loss_dice: 1.6201  d6.loss_cls: 0.5701  d6.loss_mask: 0.1578  d6.loss_dice: 1.6183  d7.loss_cls: 0.5655  d7.loss_mask: 0.1562  d7.loss_dice: 1.6092  d8.loss_cls: 0.5583  d8.loss_mask: 0.1547  d8.loss_dice: 1.5887
05/11 15:54:09 - mmengine - INFO - Saving checkpoint at 22000 iterations
05/11 15:54:52 - mmengine - INFO - Iter(val) [ 50/100]    eta: 0:00:36  time: 0.7377  data_time: 0.0136  memory: 5704  
05/11 15:55:29 - mmengine - INFO - Iter(val) [100/100]    eta: 0:00:00  time: 0.7328  data_time: 0.0131  memory: 5704  
05/11 15:55:53 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.26s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/11 15:56:00 - mmengine - INFO - start multi processing evaluation ...

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

  0%|          | 0/25552 [00:00<?, ?it/s]
 50%|█████     | 12777/25552 [00:00<00:00, 126584.35it/s]
100%|██████████| 25552/25552 [00:00<00:00, 138147.27it/s]
DONE (t=52.29s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.405
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.758
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.377
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.262
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.455
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.893
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.534
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.906
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.523
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.418
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.583
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.933
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.534
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.534
05/11 15:56:52 - mmengine - INFO - segm_mAP_copypaste: 0.405 0.758 0.377 0.262 0.455 0.893
05/11 15:56:52 - mmengine - INFO - segm_mAR_copypaste: 0.534 0.906 0.523 0.418 0.583 0.933
05/11 15:56:53 - mmengine - INFO - Iter(val) [100/100]    coco/segm_mAP: 0.4050  coco/segm_mAP_50: 0.7580  coco/segm_mAP_75: 0.3770  coco/segm_mAP_s: 0.2620  coco/segm_mAP_m: 0.4550  coco/segm_mAP_l: 0.8930  data_time: 0.0133  time: 0.7338
05/11 15:58:09 - mmengine - INFO - Iter(train) [22050/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:51:18  time: 3.2013  data_time: 1.7095  memory: 38965  grad_norm: 45.4682  loss: 25.3297  loss_cls: 0.5570  loss_mask: 0.1480  loss_dice: 1.6476  d0.loss_cls: 1.1111  d0.loss_mask: 0.1884  d0.loss_dice: 1.8769  d1.loss_cls: 0.7995  d1.loss_mask: 0.1826  d1.loss_dice: 1.8638  d2.loss_cls: 0.6206  d2.loss_mask: 0.1625  d2.loss_dice: 1.7789  d3.loss_cls: 0.5981  d3.loss_mask: 0.1571  d3.loss_dice: 1.7106  d4.loss_cls: 0.5756  d4.loss_mask: 0.1538  d4.loss_dice: 1.6842  d5.loss_cls: 0.5643  d5.loss_mask: 0.1528  d5.loss_dice: 1.6776  d6.loss_cls: 0.5726  d6.loss_mask: 0.1523  d6.loss_dice: 1.6677  d7.loss_cls: 0.5703  d7.loss_mask: 0.1507  d7.loss_dice: 1.6575  d8.loss_cls: 0.5592  d8.loss_mask: 0.1485  d8.loss_dice: 1.6396
05/11 15:59:26 - mmengine - INFO - Iter(train) [22100/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:49:59  time: 1.5354  data_time: 0.0308  memory: 38378  grad_norm: 52.3625  loss: 27.0050  loss_cls: 0.5917  loss_mask: 0.1680  loss_dice: 1.7746  d0.loss_cls: 1.1100  d0.loss_mask: 0.2092  d0.loss_dice: 1.9906  d1.loss_cls: 0.8096  d1.loss_mask: 0.2034  d1.loss_dice: 1.9785  d2.loss_cls: 0.6444  d2.loss_mask: 0.1829  d2.loss_dice: 1.8995  d3.loss_cls: 0.6215  d3.loss_mask: 0.1768  d3.loss_dice: 1.8398  d4.loss_cls: 0.6053  d4.loss_mask: 0.1728  d4.loss_dice: 1.8106  d5.loss_cls: 0.5902  d5.loss_mask: 0.1724  d5.loss_dice: 1.8013  d6.loss_cls: 0.6010  d6.loss_mask: 0.1721  d6.loss_dice: 1.7986  d7.loss_cls: 0.5987  d7.loss_mask: 0.1697  d7.loss_dice: 1.7841  d8.loss_cls: 0.5855  d8.loss_mask: 0.1686  d8.loss_dice: 1.7735
05/11 16:00:43 - mmengine - INFO - Iter(train) [22150/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:48:40  time: 1.5452  data_time: 0.0304  memory: 39020  grad_norm: 42.1324  loss: 26.3836  loss_cls: 0.5796  loss_mask: 0.1613  loss_dice: 1.7115  d0.loss_cls: 1.1373  d0.loss_mask: 0.2020  d0.loss_dice: 1.9567  d1.loss_cls: 0.8200  d1.loss_mask: 0.1936  d1.loss_dice: 1.9366  d2.loss_cls: 0.6432  d2.loss_mask: 0.1741  d2.loss_dice: 1.8570  d3.loss_cls: 0.6167  d3.loss_mask: 0.1695  d3.loss_dice: 1.7842  d4.loss_cls: 0.5969  d4.loss_mask: 0.1662  d4.loss_dice: 1.7534  d5.loss_cls: 0.5868  d5.loss_mask: 0.1647  d5.loss_dice: 1.7447  d6.loss_cls: 0.5882  d6.loss_mask: 0.1643  d6.loss_dice: 1.7387  d7.loss_cls: 0.5836  d7.loss_mask: 0.1631  d7.loss_dice: 1.7323  d8.loss_cls: 0.5797  d8.loss_mask: 0.1619  d8.loss_dice: 1.7156
05/11 16:02:00 - mmengine - INFO - Iter(train) [22200/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:47:21  time: 1.5468  data_time: 0.0295  memory: 38451  grad_norm: 49.2013  loss: 25.9578  loss_cls: 0.5843  loss_mask: 0.1564  loss_dice: 1.6905  d0.loss_cls: 1.1104  d0.loss_mask: 0.1942  d0.loss_dice: 1.8957  d1.loss_cls: 0.8161  d1.loss_mask: 0.1867  d1.loss_dice: 1.8894  d2.loss_cls: 0.6365  d2.loss_mask: 0.1706  d2.loss_dice: 1.8117  d3.loss_cls: 0.6115  d3.loss_mask: 0.1641  d3.loss_dice: 1.7474  d4.loss_cls: 0.5941  d4.loss_mask: 0.1609  d4.loss_dice: 1.7241  d5.loss_cls: 0.5825  d5.loss_mask: 0.1604  d5.loss_dice: 1.7158  d6.loss_cls: 0.5950  d6.loss_mask: 0.1598  d6.loss_dice: 1.7112  d7.loss_cls: 0.5883  d7.loss_mask: 0.1589  d7.loss_dice: 1.7083  d8.loss_cls: 0.5826  d8.loss_mask: 0.1571  d8.loss_dice: 1.6932
05/11 16:03:17 - mmengine - INFO - Iter(train) [22250/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:46:02  time: 1.5347  data_time: 0.0294  memory: 38936  grad_norm: 48.8050  loss: 26.7897  loss_cls: 0.5874  loss_mask: 0.1711  loss_dice: 1.7419  d0.loss_cls: 1.1295  d0.loss_mask: 0.2166  d0.loss_dice: 1.9857  d1.loss_cls: 0.8076  d1.loss_mask: 0.2063  d1.loss_dice: 1.9667  d2.loss_cls: 0.6296  d2.loss_mask: 0.1877  d2.loss_dice: 1.8867  d3.loss_cls: 0.6148  d3.loss_mask: 0.1806  d3.loss_dice: 1.8185  d4.loss_cls: 0.5985  d4.loss_mask: 0.1760  d4.loss_dice: 1.7864  d5.loss_cls: 0.5899  d5.loss_mask: 0.1751  d5.loss_dice: 1.7753  d6.loss_cls: 0.5968  d6.loss_mask: 0.1748  d6.loss_dice: 1.7632  d7.loss_cls: 0.5981  d7.loss_mask: 0.1727  d7.loss_dice: 1.7513  d8.loss_cls: 0.5872  d8.loss_mask: 0.1715  d8.loss_dice: 1.7423
05/11 16:04:34 - mmengine - INFO - Iter(train) [22300/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:44:43  time: 1.5430  data_time: 0.0296  memory: 38547  grad_norm: 45.5396  loss: 25.7281  loss_cls: 0.5813  loss_mask: 0.1587  loss_dice: 1.6696  d0.loss_cls: 1.0928  d0.loss_mask: 0.1973  d0.loss_dice: 1.8690  d1.loss_cls: 0.8038  d1.loss_mask: 0.1886  d1.loss_dice: 1.8608  d2.loss_cls: 0.6291  d2.loss_mask: 0.1724  d2.loss_dice: 1.7906  d3.loss_cls: 0.6146  d3.loss_mask: 0.1663  d3.loss_dice: 1.7359  d4.loss_cls: 0.5923  d4.loss_mask: 0.1644  d4.loss_dice: 1.7087  d5.loss_cls: 0.5797  d5.loss_mask: 0.1634  d5.loss_dice: 1.6985  d6.loss_cls: 0.5950  d6.loss_mask: 0.1626  d6.loss_dice: 1.6927  d7.loss_cls: 0.5921  d7.loss_mask: 0.1606  d7.loss_dice: 1.6787  d8.loss_cls: 0.5794  d8.loss_mask: 0.1585  d8.loss_dice: 1.6706
05/11 16:05:51 - mmengine - INFO - Iter(train) [22350/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:43:24  time: 1.5384  data_time: 0.0287  memory: 38220  grad_norm: 45.5611  loss: 24.5620  loss_cls: 0.5630  loss_mask: 0.1440  loss_dice: 1.5754  d0.loss_cls: 1.1007  d0.loss_mask: 0.1796  d0.loss_dice: 1.7895  d1.loss_cls: 0.8015  d1.loss_mask: 0.1728  d1.loss_dice: 1.7749  d2.loss_cls: 0.6156  d2.loss_mask: 0.1583  d2.loss_dice: 1.7038  d3.loss_cls: 0.6034  d3.loss_mask: 0.1527  d3.loss_dice: 1.6427  d4.loss_cls: 0.5807  d4.loss_mask: 0.1496  d4.loss_dice: 1.6144  d5.loss_cls: 0.5631  d5.loss_mask: 0.1488  d5.loss_dice: 1.6036  d6.loss_cls: 0.5745  d6.loss_mask: 0.1480  d6.loss_dice: 1.6001  d7.loss_cls: 0.5706  d7.loss_mask: 0.1469  d7.loss_dice: 1.5945  d8.loss_cls: 0.5651  d8.loss_mask: 0.1444  d8.loss_dice: 1.5797
05/11 16:07:08 - mmengine - INFO - Iter(train) [22400/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:42:05  time: 1.5380  data_time: 0.0293  memory: 38799  grad_norm: 48.5346  loss: 26.2051  loss_cls: 0.5657  loss_mask: 0.1692  loss_dice: 1.6961  d0.loss_cls: 1.1377  d0.loss_mask: 0.2110  d0.loss_dice: 1.9359  d1.loss_cls: 0.8139  d1.loss_mask: 0.2003  d1.loss_dice: 1.9201  d2.loss_cls: 0.6289  d2.loss_mask: 0.1842  d2.loss_dice: 1.8368  d3.loss_cls: 0.6032  d3.loss_mask: 0.1769  d3.loss_dice: 1.7712  d4.loss_cls: 0.5871  d4.loss_mask: 0.1739  d4.loss_dice: 1.7382  d5.loss_cls: 0.5755  d5.loss_mask: 0.1729  d5.loss_dice: 1.7271  d6.loss_cls: 0.5787  d6.loss_mask: 0.1721  d6.loss_dice: 1.7271  d7.loss_cls: 0.5738  d7.loss_mask: 0.1721  d7.loss_dice: 1.7174  d8.loss_cls: 0.5683  d8.loss_mask: 0.1697  d8.loss_dice: 1.7001
05/11 16:08:25 - mmengine - INFO - Iter(train) [22450/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:40:46  time: 1.5375  data_time: 0.0294  memory: 38922  grad_norm: 43.5418  loss: 27.5945  loss_cls: 0.5916  loss_mask: 0.1696  loss_dice: 1.8251  d0.loss_cls: 1.1311  d0.loss_mask: 0.2150  d0.loss_dice: 2.0340  d1.loss_cls: 0.8225  d1.loss_mask: 0.2025  d1.loss_dice: 2.0299  d2.loss_cls: 0.6442  d2.loss_mask: 0.1837  d2.loss_dice: 1.9535  d3.loss_cls: 0.6262  d3.loss_mask: 0.1774  d3.loss_dice: 1.8910  d4.loss_cls: 0.6078  d4.loss_mask: 0.1743  d4.loss_dice: 1.8643  d5.loss_cls: 0.5971  d5.loss_mask: 0.1743  d5.loss_dice: 1.8532  d6.loss_cls: 0.6071  d6.loss_mask: 0.1736  d6.loss_dice: 1.8457  d7.loss_cls: 0.6005  d7.loss_mask: 0.1721  d7.loss_dice: 1.8384  d8.loss_cls: 0.5951  d8.loss_mask: 0.1703  d8.loss_dice: 1.8234
05/11 16:09:42 - mmengine - INFO - Iter(train) [22500/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:39:27  time: 1.5442  data_time: 0.0288  memory: 38624  grad_norm: 48.0886  loss: 25.4261  loss_cls: 0.5743  loss_mask: 0.1552  loss_dice: 1.6467  d0.loss_cls: 1.1025  d0.loss_mask: 0.1909  d0.loss_dice: 1.8564  d1.loss_cls: 0.7929  d1.loss_mask: 0.1838  d1.loss_dice: 1.8403  d2.loss_cls: 0.6272  d2.loss_mask: 0.1666  d2.loss_dice: 1.7708  d3.loss_cls: 0.6061  d3.loss_mask: 0.1621  d3.loss_dice: 1.7138  d4.loss_cls: 0.5885  d4.loss_mask: 0.1586  d4.loss_dice: 1.6852  d5.loss_cls: 0.5777  d5.loss_mask: 0.1583  d5.loss_dice: 1.6754  d6.loss_cls: 0.5855  d6.loss_mask: 0.1577  d6.loss_dice: 1.6687  d7.loss_cls: 0.5813  d7.loss_mask: 0.1571  d7.loss_dice: 1.6631  d8.loss_cls: 0.5767  d8.loss_mask: 0.1561  d8.loss_dice: 1.6466
05/11 16:10:59 - mmengine - INFO - Iter(train) [22550/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:38:08  time: 1.5293  data_time: 0.0279  memory: 39093  grad_norm: 43.0203  loss: 27.2844  loss_cls: 0.5884  loss_mask: 0.1675  loss_dice: 1.7842  d0.loss_cls: 1.1532  d0.loss_mask: 0.2143  d0.loss_dice: 2.0195  d1.loss_cls: 0.8305  d1.loss_mask: 0.2028  d1.loss_dice: 2.0114  d2.loss_cls: 0.6517  d2.loss_mask: 0.1812  d2.loss_dice: 1.9237  d3.loss_cls: 0.6232  d3.loss_mask: 0.1763  d3.loss_dice: 1.8658  d4.loss_cls: 0.6076  d4.loss_mask: 0.1727  d4.loss_dice: 1.8299  d5.loss_cls: 0.5946  d5.loss_mask: 0.1723  d5.loss_dice: 1.8173  d6.loss_cls: 0.5953  d6.loss_mask: 0.1725  d6.loss_dice: 1.8176  d7.loss_cls: 0.5970  d7.loss_mask: 0.1701  d7.loss_dice: 1.7977  d8.loss_cls: 0.5901  d8.loss_mask: 0.1678  d8.loss_dice: 1.7883
05/11 16:12:15 - mmengine - INFO - Iter(train) [22600/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:36:49  time: 1.5313  data_time: 0.0299  memory: 39452  grad_norm: 42.7900  loss: 26.9641  loss_cls: 0.5820  loss_mask: 0.1629  loss_dice: 1.7573  d0.loss_cls: 1.1588  d0.loss_mask: 0.2102  d0.loss_dice: 2.0036  d1.loss_cls: 0.8201  d1.loss_mask: 0.1980  d1.loss_dice: 1.9997  d2.loss_cls: 0.6414  d2.loss_mask: 0.1805  d2.loss_dice: 1.9076  d3.loss_cls: 0.6208  d3.loss_mask: 0.1723  d3.loss_dice: 1.8367  d4.loss_cls: 0.5977  d4.loss_mask: 0.1692  d4.loss_dice: 1.8054  d5.loss_cls: 0.5865  d5.loss_mask: 0.1673  d5.loss_dice: 1.7935  d6.loss_cls: 0.5931  d6.loss_mask: 0.1669  d6.loss_dice: 1.7903  d7.loss_cls: 0.5903  d7.loss_mask: 0.1658  d7.loss_dice: 1.7793  d8.loss_cls: 0.5840  d8.loss_mask: 0.1632  d8.loss_dice: 1.7599
05/11 16:13:33 - mmengine - INFO - Iter(train) [22650/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:35:30  time: 1.5464  data_time: 0.0305  memory: 38612  grad_norm: 46.0482  loss: 25.8117  loss_cls: 0.5676  loss_mask: 0.1587  loss_dice: 1.6821  d0.loss_cls: 1.1181  d0.loss_mask: 0.1974  d0.loss_dice: 1.9028  d1.loss_cls: 0.7899  d1.loss_mask: 0.1910  d1.loss_dice: 1.8925  d2.loss_cls: 0.6123  d2.loss_mask: 0.1739  d2.loss_dice: 1.8145  d3.loss_cls: 0.5998  d3.loss_mask: 0.1667  d3.loss_dice: 1.7522  d4.loss_cls: 0.5830  d4.loss_mask: 0.1636  d4.loss_dice: 1.7214  d5.loss_cls: 0.5699  d5.loss_mask: 0.1626  d5.loss_dice: 1.7087  d6.loss_cls: 0.5760  d6.loss_mask: 0.1617  d6.loss_dice: 1.7047  d7.loss_cls: 0.5744  d7.loss_mask: 0.1606  d7.loss_dice: 1.6958  d8.loss_cls: 0.5681  d8.loss_mask: 0.1592  d8.loss_dice: 1.6828
05/11 16:14:49 - mmengine - INFO - Iter(train) [22700/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:34:11  time: 1.5308  data_time: 0.0293  memory: 38770  grad_norm: 56.3896  loss: 27.0477  loss_cls: 0.5894  loss_mask: 0.1841  loss_dice: 1.7449  d0.loss_cls: 1.1312  d0.loss_mask: 0.2326  d0.loss_dice: 1.9969  d1.loss_cls: 0.8166  d1.loss_mask: 0.2206  d1.loss_dice: 1.9837  d2.loss_cls: 0.6395  d2.loss_mask: 0.2005  d2.loss_dice: 1.8920  d3.loss_cls: 0.6186  d3.loss_mask: 0.1952  d3.loss_dice: 1.8301  d4.loss_cls: 0.6016  d4.loss_mask: 0.1919  d4.loss_dice: 1.7899  d5.loss_cls: 0.5892  d5.loss_mask: 0.1906  d5.loss_dice: 1.7792  d6.loss_cls: 0.5971  d6.loss_mask: 0.1898  d6.loss_dice: 1.7753  d7.loss_cls: 0.5916  d7.loss_mask: 0.1879  d7.loss_dice: 1.7669  d8.loss_cls: 0.5874  d8.loss_mask: 0.1850  d8.loss_dice: 1.7485
05/11 16:16:06 - mmengine - INFO - Iter(train) [22750/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:32:52  time: 1.5353  data_time: 0.0290  memory: 39127  grad_norm: 53.1964  loss: 28.1909  loss_cls: 0.5984  loss_mask: 0.1652  loss_dice: 1.8685  d0.loss_cls: 1.1566  d0.loss_mask: 0.2074  d0.loss_dice: 2.1192  d1.loss_cls: 0.8312  d1.loss_mask: 0.2000  d1.loss_dice: 2.1073  d2.loss_cls: 0.6559  d2.loss_mask: 0.1797  d2.loss_dice: 2.0115  d3.loss_cls: 0.6380  d3.loss_mask: 0.1728  d3.loss_dice: 1.9453  d4.loss_cls: 0.6188  d4.loss_mask: 0.1690  d4.loss_dice: 1.9097  d5.loss_cls: 0.6032  d5.loss_mask: 0.1683  d5.loss_dice: 1.8941  d6.loss_cls: 0.6130  d6.loss_mask: 0.1680  d6.loss_dice: 1.8969  d7.loss_cls: 0.6089  d7.loss_mask: 0.1667  d7.loss_dice: 1.8821  d8.loss_cls: 0.5995  d8.loss_mask: 0.1653  d8.loss_dice: 1.8705
05/11 16:17:23 - mmengine - INFO - Iter(train) [22800/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:31:33  time: 1.5448  data_time: 0.0308  memory: 38542  grad_norm: 49.1434  loss: 26.0990  loss_cls: 0.5977  loss_mask: 0.1539  loss_dice: 1.6758  d0.loss_cls: 1.1414  d0.loss_mask: 0.1916  d0.loss_dice: 1.9246  d1.loss_cls: 0.8358  d1.loss_mask: 0.1866  d1.loss_dice: 1.9062  d2.loss_cls: 0.6531  d2.loss_mask: 0.1678  d2.loss_dice: 1.8178  d3.loss_cls: 0.6281  d3.loss_mask: 0.1611  d3.loss_dice: 1.7504  d4.loss_cls: 0.6098  d4.loss_mask: 0.1584  d4.loss_dice: 1.7189  d5.loss_cls: 0.6015  d5.loss_mask: 0.1579  d5.loss_dice: 1.7072  d6.loss_cls: 0.6070  d6.loss_mask: 0.1576  d6.loss_dice: 1.7029  d7.loss_cls: 0.5999  d7.loss_mask: 0.1562  d7.loss_dice: 1.7010  d8.loss_cls: 0.5953  d8.loss_mask: 0.1543  d8.loss_dice: 1.6793
05/11 16:18:40 - mmengine - INFO - Iter(train) [22850/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:30:14  time: 1.5382  data_time: 0.0317  memory: 39111  grad_norm: 54.4449  loss: 25.7010  loss_cls: 0.5612  loss_mask: 0.1611  loss_dice: 1.6708  d0.loss_cls: 1.1191  d0.loss_mask: 0.1998  d0.loss_dice: 1.8995  d1.loss_cls: 0.8050  d1.loss_mask: 0.1898  d1.loss_dice: 1.8818  d2.loss_cls: 0.6183  d2.loss_mask: 0.1749  d2.loss_dice: 1.7952  d3.loss_cls: 0.6002  d3.loss_mask: 0.1685  d3.loss_dice: 1.7382  d4.loss_cls: 0.5770  d4.loss_mask: 0.1655  d4.loss_dice: 1.7029  d5.loss_cls: 0.5668  d5.loss_mask: 0.1646  d5.loss_dice: 1.6947  d6.loss_cls: 0.5746  d6.loss_mask: 0.1643  d6.loss_dice: 1.6971  d7.loss_cls: 0.5685  d7.loss_mask: 0.1633  d7.loss_dice: 1.6838  d8.loss_cls: 0.5609  d8.loss_mask: 0.1615  d8.loss_dice: 1.6722
05/11 16:19:57 - mmengine - INFO - Iter(train) [22900/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:28:55  time: 1.5420  data_time: 0.0316  memory: 38687  grad_norm: 51.7454  loss: 25.8033  loss_cls: 0.5787  loss_mask: 0.1553  loss_dice: 1.6756  d0.loss_cls: 1.1092  d0.loss_mask: 0.1955  d0.loss_dice: 1.8978  d1.loss_cls: 0.7932  d1.loss_mask: 0.1867  d1.loss_dice: 1.8932  d2.loss_cls: 0.6304  d2.loss_mask: 0.1682  d2.loss_dice: 1.8050  d3.loss_cls: 0.6084  d3.loss_mask: 0.1634  d3.loss_dice: 1.7442  d4.loss_cls: 0.5871  d4.loss_mask: 0.1596  d4.loss_dice: 1.7126  d5.loss_cls: 0.5771  d5.loss_mask: 0.1599  d5.loss_dice: 1.7069  d6.loss_cls: 0.5856  d6.loss_mask: 0.1587  d6.loss_dice: 1.7002  d7.loss_cls: 0.5824  d7.loss_mask: 0.1578  d7.loss_dice: 1.6987  d8.loss_cls: 0.5785  d8.loss_mask: 0.1557  d8.loss_dice: 1.6774
05/11 16:21:14 - mmengine - INFO - Iter(train) [22950/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:27:36  time: 1.5410  data_time: 0.0316  memory: 38558  grad_norm: 47.4865  loss: 25.6658  loss_cls: 0.5822  loss_mask: 0.1563  loss_dice: 1.6614  d0.loss_cls: 1.1203  d0.loss_mask: 0.1945  d0.loss_dice: 1.8641  d1.loss_cls: 0.8107  d1.loss_mask: 0.1856  d1.loss_dice: 1.8613  d2.loss_cls: 0.6328  d2.loss_mask: 0.1677  d2.loss_dice: 1.7828  d3.loss_cls: 0.6089  d3.loss_mask: 0.1634  d3.loss_dice: 1.7249  d4.loss_cls: 0.5949  d4.loss_mask: 0.1603  d4.loss_dice: 1.7044  d5.loss_cls: 0.5799  d5.loss_mask: 0.1596  d5.loss_dice: 1.6972  d6.loss_cls: 0.5874  d6.loss_mask: 0.1591  d6.loss_dice: 1.6847  d7.loss_cls: 0.5879  d7.loss_mask: 0.1580  d7.loss_dice: 1.6785  d8.loss_cls: 0.5781  d8.loss_mask: 0.1569  d8.loss_dice: 1.6619
05/11 16:22:30 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 16:22:30 - mmengine - INFO - Iter(train) [23000/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:26:17  time: 1.5237  data_time: 0.0315  memory: 39257  grad_norm: 60.7678  loss: 27.6175  loss_cls: 0.5861  loss_mask: 0.1641  loss_dice: 1.8349  d0.loss_cls: 1.1238  d0.loss_mask: 0.2019  d0.loss_dice: 2.0607  d1.loss_cls: 0.8062  d1.loss_mask: 0.1961  d1.loss_dice: 2.0577  d2.loss_cls: 0.6373  d2.loss_mask: 0.1787  d2.loss_dice: 1.9792  d3.loss_cls: 0.6217  d3.loss_mask: 0.1729  d3.loss_dice: 1.9110  d4.loss_cls: 0.5989  d4.loss_mask: 0.1703  d4.loss_dice: 1.8807  d5.loss_cls: 0.5885  d5.loss_mask: 0.1685  d5.loss_dice: 1.8600  d6.loss_cls: 0.5984  d6.loss_mask: 0.1677  d6.loss_dice: 1.8571  d7.loss_cls: 0.5917  d7.loss_mask: 0.1666  d7.loss_dice: 1.8531  d8.loss_cls: 0.5848  d8.loss_mask: 0.1654  d8.loss_dice: 1.8336
05/11 16:23:48 - mmengine - INFO - Iter(train) [23050/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:24:58  time: 1.5590  data_time: 0.0314  memory: 39756  grad_norm: 59.0357  loss: 26.4944  loss_cls: 0.5880  loss_mask: 0.1584  loss_dice: 1.7321  d0.loss_cls: 1.1217  d0.loss_mask: 0.1987  d0.loss_dice: 1.9514  d1.loss_cls: 0.8091  d1.loss_mask: 0.1891  d1.loss_dice: 1.9299  d2.loss_cls: 0.6362  d2.loss_mask: 0.1741  d2.loss_dice: 1.8625  d3.loss_cls: 0.6238  d3.loss_mask: 0.1652  d3.loss_dice: 1.8005  d4.loss_cls: 0.6008  d4.loss_mask: 0.1623  d4.loss_dice: 1.7747  d5.loss_cls: 0.5894  d5.loss_mask: 0.1629  d5.loss_dice: 1.7600  d6.loss_cls: 0.5936  d6.loss_mask: 0.1619  d6.loss_dice: 1.7609  d7.loss_cls: 0.5927  d7.loss_mask: 0.1614  d7.loss_dice: 1.7499  d8.loss_cls: 0.5827  d8.loss_mask: 0.1598  d8.loss_dice: 1.7407
05/11 16:25:05 - mmengine - INFO - Iter(train) [23100/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:23:39  time: 1.5320  data_time: 0.0321  memory: 39602  grad_norm: 49.4880  loss: 27.4889  loss_cls: 0.6025  loss_mask: 0.1672  loss_dice: 1.7950  d0.loss_cls: 1.1497  d0.loss_mask: 0.2124  d0.loss_dice: 2.0416  d1.loss_cls: 0.8310  d1.loss_mask: 0.2012  d1.loss_dice: 2.0305  d2.loss_cls: 0.6502  d2.loss_mask: 0.1835  d2.loss_dice: 1.9440  d3.loss_cls: 0.6303  d3.loss_mask: 0.1772  d3.loss_dice: 1.8763  d4.loss_cls: 0.6137  d4.loss_mask: 0.1739  d4.loss_dice: 1.8407  d5.loss_cls: 0.6038  d5.loss_mask: 0.1730  d5.loss_dice: 1.8290  d6.loss_cls: 0.6087  d6.loss_mask: 0.1720  d6.loss_dice: 1.8251  d7.loss_cls: 0.6062  d7.loss_mask: 0.1705  d7.loss_dice: 1.8165  d8.loss_cls: 0.6000  d8.loss_mask: 0.1679  d8.loss_dice: 1.7953
05/11 16:26:22 - mmengine - INFO - Iter(train) [23150/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:22:20  time: 1.5454  data_time: 0.0299  memory: 38902  grad_norm: 46.5709  loss: 28.0200  loss_cls: 0.5955  loss_mask: 0.1605  loss_dice: 1.8711  d0.loss_cls: 1.1346  d0.loss_mask: 0.2012  d0.loss_dice: 2.1086  d1.loss_cls: 0.8212  d1.loss_mask: 0.1944  d1.loss_dice: 2.0925  d2.loss_cls: 0.6378  d2.loss_mask: 0.1761  d2.loss_dice: 2.0163  d3.loss_cls: 0.6228  d3.loss_mask: 0.1701  d3.loss_dice: 1.9461  d4.loss_cls: 0.6054  d4.loss_mask: 0.1672  d4.loss_dice: 1.9100  d5.loss_cls: 0.5970  d5.loss_mask: 0.1653  d5.loss_dice: 1.8987  d6.loss_cls: 0.5988  d6.loss_mask: 0.1648  d6.loss_dice: 1.8933  d7.loss_cls: 0.5973  d7.loss_mask: 0.1636  d7.loss_dice: 1.8855  d8.loss_cls: 0.5952  d8.loss_mask: 0.1622  d8.loss_dice: 1.8671
05/11 16:27:39 - mmengine - INFO - Iter(train) [23200/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:21:01  time: 1.5294  data_time: 0.0315  memory: 38511  grad_norm: 57.2490  loss: 26.4449  loss_cls: 0.5798  loss_mask: 0.1735  loss_dice: 1.7219  d0.loss_cls: 1.1090  d0.loss_mask: 0.2147  d0.loss_dice: 1.9365  d1.loss_cls: 0.8039  d1.loss_mask: 0.2048  d1.loss_dice: 1.9225  d2.loss_cls: 0.6307  d2.loss_mask: 0.1869  d2.loss_dice: 1.8462  d3.loss_cls: 0.6144  d3.loss_mask: 0.1824  d3.loss_dice: 1.7857  d4.loss_cls: 0.5965  d4.loss_mask: 0.1798  d4.loss_dice: 1.7617  d5.loss_cls: 0.5847  d5.loss_mask: 0.1777  d5.loss_dice: 1.7495  d6.loss_cls: 0.5885  d6.loss_mask: 0.1765  d6.loss_dice: 1.7458  d7.loss_cls: 0.5874  d7.loss_mask: 0.1758  d7.loss_dice: 1.7387  d8.loss_cls: 0.5778  d8.loss_mask: 0.1739  d8.loss_dice: 1.7177
05/11 16:28:57 - mmengine - INFO - Iter(train) [23250/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:19:42  time: 1.5593  data_time: 0.0313  memory: 38877  grad_norm: 44.2333  loss: 24.7189  loss_cls: 0.5636  loss_mask: 0.1523  loss_dice: 1.5823  d0.loss_cls: 1.1142  d0.loss_mask: 0.1907  d0.loss_dice: 1.8056  d1.loss_cls: 0.7884  d1.loss_mask: 0.1836  d1.loss_dice: 1.8018  d2.loss_cls: 0.6062  d2.loss_mask: 0.1659  d2.loss_dice: 1.7126  d3.loss_cls: 0.5939  d3.loss_mask: 0.1606  d3.loss_dice: 1.6506  d4.loss_cls: 0.5792  d4.loss_mask: 0.1577  d4.loss_dice: 1.6224  d5.loss_cls: 0.5698  d5.loss_mask: 0.1565  d5.loss_dice: 1.6053  d6.loss_cls: 0.5732  d6.loss_mask: 0.1560  d6.loss_dice: 1.6059  d7.loss_cls: 0.5720  d7.loss_mask: 0.1545  d7.loss_dice: 1.5975  d8.loss_cls: 0.5634  d8.loss_mask: 0.1520  d8.loss_dice: 1.5813
05/11 16:30:13 - mmengine - INFO - Iter(train) [23300/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:18:23  time: 1.5271  data_time: 0.0319  memory: 39192  grad_norm: 55.7322  loss: 26.2364  loss_cls: 0.5736  loss_mask: 0.1649  loss_dice: 1.7105  d0.loss_cls: 1.1262  d0.loss_mask: 0.2037  d0.loss_dice: 1.9448  d1.loss_cls: 0.7998  d1.loss_mask: 0.1954  d1.loss_dice: 1.9220  d2.loss_cls: 0.6223  d2.loss_mask: 0.1773  d2.loss_dice: 1.8408  d3.loss_cls: 0.6068  d3.loss_mask: 0.1717  d3.loss_dice: 1.7715  d4.loss_cls: 0.5881  d4.loss_mask: 0.1695  d4.loss_dice: 1.7534  d5.loss_cls: 0.5743  d5.loss_mask: 0.1685  d5.loss_dice: 1.7426  d6.loss_cls: 0.5761  d6.loss_mask: 0.1682  d6.loss_dice: 1.7408  d7.loss_cls: 0.5774  d7.loss_mask: 0.1676  d7.loss_dice: 1.7304  d8.loss_cls: 0.5738  d8.loss_mask: 0.1650  d8.loss_dice: 1.7091
05/11 16:31:30 - mmengine - INFO - Iter(train) [23350/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:17:04  time: 1.5418  data_time: 0.0318  memory: 39221  grad_norm: 41.6372  loss: 25.9311  loss_cls: 0.5745  loss_mask: 0.1754  loss_dice: 1.6606  d0.loss_cls: 1.1353  d0.loss_mask: 0.2201  d0.loss_dice: 1.8995  d1.loss_cls: 0.8021  d1.loss_mask: 0.2085  d1.loss_dice: 1.8848  d2.loss_cls: 0.6316  d2.loss_mask: 0.1877  d2.loss_dice: 1.7886  d3.loss_cls: 0.6116  d3.loss_mask: 0.1847  d3.loss_dice: 1.7350  d4.loss_cls: 0.5946  d4.loss_mask: 0.1818  d4.loss_dice: 1.6963  d5.loss_cls: 0.5848  d5.loss_mask: 0.1808  d5.loss_dice: 1.6877  d6.loss_cls: 0.5865  d6.loss_mask: 0.1796  d6.loss_dice: 1.6919  d7.loss_cls: 0.5849  d7.loss_mask: 0.1791  d7.loss_dice: 1.6711  d8.loss_cls: 0.5769  d8.loss_mask: 0.1765  d8.loss_dice: 1.6586
05/11 16:32:47 - mmengine - INFO - Iter(train) [23400/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:15:45  time: 1.5372  data_time: 0.0319  memory: 38773  grad_norm: 49.0304  loss: 26.1188  loss_cls: 0.5867  loss_mask: 0.1620  loss_dice: 1.6871  d0.loss_cls: 1.1183  d0.loss_mask: 0.2061  d0.loss_dice: 1.9055  d1.loss_cls: 0.8141  d1.loss_mask: 0.1942  d1.loss_dice: 1.8929  d2.loss_cls: 0.6372  d2.loss_mask: 0.1768  d2.loss_dice: 1.8209  d3.loss_cls: 0.6233  d3.loss_mask: 0.1710  d3.loss_dice: 1.7594  d4.loss_cls: 0.6062  d4.loss_mask: 0.1682  d4.loss_dice: 1.7286  d5.loss_cls: 0.5962  d5.loss_mask: 0.1670  d5.loss_dice: 1.7152  d6.loss_cls: 0.5982  d6.loss_mask: 0.1661  d6.loss_dice: 1.7159  d7.loss_cls: 0.5971  d7.loss_mask: 0.1644  d7.loss_dice: 1.7021  d8.loss_cls: 0.5854  d8.loss_mask: 0.1631  d8.loss_dice: 1.6897
05/11 16:34:04 - mmengine - INFO - Iter(train) [23450/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:14:27  time: 1.5489  data_time: 0.0420  memory: 39371  grad_norm: 50.9768  loss: 26.6325  loss_cls: 0.5715  loss_mask: 0.1562  loss_dice: 1.7474  d0.loss_cls: 1.1393  d0.loss_mask: 0.1975  d0.loss_dice: 1.9970  d1.loss_cls: 0.8084  d1.loss_mask: 0.1887  d1.loss_dice: 1.9751  d2.loss_cls: 0.6326  d2.loss_mask: 0.1696  d2.loss_dice: 1.8912  d3.loss_cls: 0.6093  d3.loss_mask: 0.1652  d3.loss_dice: 1.8293  d4.loss_cls: 0.5885  d4.loss_mask: 0.1621  d4.loss_dice: 1.7936  d5.loss_cls: 0.5761  d5.loss_mask: 0.1599  d5.loss_dice: 1.7796  d6.loss_cls: 0.5817  d6.loss_mask: 0.1579  d6.loss_dice: 1.7718  d7.loss_cls: 0.5811  d7.loss_mask: 0.1579  d7.loss_dice: 1.7661  d8.loss_cls: 0.5731  d8.loss_mask: 0.1564  d8.loss_dice: 1.7483
05/11 16:35:21 - mmengine - INFO - Iter(train) [23500/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:13:08  time: 1.5290  data_time: 0.0304  memory: 38681  grad_norm: 44.0336  loss: 25.3882  loss_cls: 0.5674  loss_mask: 0.1634  loss_dice: 1.6367  d0.loss_cls: 1.0984  d0.loss_mask: 0.2058  d0.loss_dice: 1.8504  d1.loss_cls: 0.7980  d1.loss_mask: 0.1946  d1.loss_dice: 1.8372  d2.loss_cls: 0.6189  d2.loss_mask: 0.1766  d2.loss_dice: 1.7583  d3.loss_cls: 0.6015  d3.loss_mask: 0.1700  d3.loss_dice: 1.6962  d4.loss_cls: 0.5831  d4.loss_mask: 0.1670  d4.loss_dice: 1.6775  d5.loss_cls: 0.5738  d5.loss_mask: 0.1658  d5.loss_dice: 1.6689  d6.loss_cls: 0.5756  d6.loss_mask: 0.1664  d6.loss_dice: 1.6645  d7.loss_cls: 0.5743  d7.loss_mask: 0.1652  d7.loss_dice: 1.6615  d8.loss_cls: 0.5701  d8.loss_mask: 0.1632  d8.loss_dice: 1.6379
05/11 16:36:38 - mmengine - INFO - Iter(train) [23550/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:11:49  time: 1.5351  data_time: 0.0305  memory: 38569  grad_norm: 48.3610  loss: 25.8364  loss_cls: 0.5642  loss_mask: 0.1525  loss_dice: 1.6987  d0.loss_cls: 1.1048  d0.loss_mask: 0.1926  d0.loss_dice: 1.9060  d1.loss_cls: 0.7823  d1.loss_mask: 0.1830  d1.loss_dice: 1.8971  d2.loss_cls: 0.6134  d2.loss_mask: 0.1652  d2.loss_dice: 1.8214  d3.loss_cls: 0.5945  d3.loss_mask: 0.1606  d3.loss_dice: 1.7690  d4.loss_cls: 0.5846  d4.loss_mask: 0.1574  d4.loss_dice: 1.7342  d5.loss_cls: 0.5748  d5.loss_mask: 0.1554  d5.loss_dice: 1.7258  d6.loss_cls: 0.5782  d6.loss_mask: 0.1555  d6.loss_dice: 1.7187  d7.loss_cls: 0.5721  d7.loss_mask: 0.1546  d7.loss_dice: 1.7101  d8.loss_cls: 0.5676  d8.loss_mask: 0.1526  d8.loss_dice: 1.6895
05/11 16:37:54 - mmengine - INFO - Iter(train) [23600/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:10:30  time: 1.5374  data_time: 0.0314  memory: 38234  grad_norm: 46.2080  loss: 24.6595  loss_cls: 0.5556  loss_mask: 0.1593  loss_dice: 1.5791  d0.loss_cls: 1.1038  d0.loss_mask: 0.2019  d0.loss_dice: 1.7879  d1.loss_cls: 0.7778  d1.loss_mask: 0.1890  d1.loss_dice: 1.7868  d2.loss_cls: 0.6021  d2.loss_mask: 0.1740  d2.loss_dice: 1.7089  d3.loss_cls: 0.5851  d3.loss_mask: 0.1678  d3.loss_dice: 1.6457  d4.loss_cls: 0.5728  d4.loss_mask: 0.1645  d4.loss_dice: 1.6195  d5.loss_cls: 0.5550  d5.loss_mask: 0.1637  d5.loss_dice: 1.6112  d6.loss_cls: 0.5595  d6.loss_mask: 0.1631  d6.loss_dice: 1.6079  d7.loss_cls: 0.5631  d7.loss_mask: 0.1622  d7.loss_dice: 1.5953  d8.loss_cls: 0.5567  d8.loss_mask: 0.1602  d8.loss_dice: 1.5802
05/11 16:39:12 - mmengine - INFO - Iter(train) [23650/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:09:11  time: 1.5412  data_time: 0.0359  memory: 39434  grad_norm: 49.3651  loss: 25.7341  loss_cls: 0.5741  loss_mask: 0.1587  loss_dice: 1.6746  d0.loss_cls: 1.1039  d0.loss_mask: 0.1963  d0.loss_dice: 1.8878  d1.loss_cls: 0.7918  d1.loss_mask: 0.1897  d1.loss_dice: 1.8787  d2.loss_cls: 0.6160  d2.loss_mask: 0.1703  d2.loss_dice: 1.7996  d3.loss_cls: 0.6008  d3.loss_mask: 0.1660  d3.loss_dice: 1.7411  d4.loss_cls: 0.5878  d4.loss_mask: 0.1630  d4.loss_dice: 1.7145  d5.loss_cls: 0.5728  d5.loss_mask: 0.1618  d5.loss_dice: 1.7025  d6.loss_cls: 0.5785  d6.loss_mask: 0.1624  d6.loss_dice: 1.7004  d7.loss_cls: 0.5793  d7.loss_mask: 0.1611  d7.loss_dice: 1.6872  d8.loss_cls: 0.5751  d8.loss_mask: 0.1593  d8.loss_dice: 1.6790
05/11 16:40:29 - mmengine - INFO - Iter(train) [23700/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:07:52  time: 1.5393  data_time: 0.0286  memory: 39388  grad_norm: 46.3793  loss: 26.6505  loss_cls: 0.5687  loss_mask: 0.1727  loss_dice: 1.7467  d0.loss_cls: 1.1300  d0.loss_mask: 0.2164  d0.loss_dice: 1.9717  d1.loss_cls: 0.8007  d1.loss_mask: 0.2059  d1.loss_dice: 1.9533  d2.loss_cls: 0.6201  d2.loss_mask: 0.1862  d2.loss_dice: 1.8724  d3.loss_cls: 0.6037  d3.loss_mask: 0.1811  d3.loss_dice: 1.8122  d4.loss_cls: 0.5843  d4.loss_mask: 0.1777  d4.loss_dice: 1.7888  d5.loss_cls: 0.5767  d5.loss_mask: 0.1762  d5.loss_dice: 1.7731  d6.loss_cls: 0.5814  d6.loss_mask: 0.1758  d6.loss_dice: 1.7705  d7.loss_cls: 0.5759  d7.loss_mask: 0.1751  d7.loss_dice: 1.7635  d8.loss_cls: 0.5691  d8.loss_mask: 0.1740  d8.loss_dice: 1.7465
05/11 16:41:45 - mmengine - INFO - Iter(train) [23750/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:06:34  time: 1.5311  data_time: 0.0282  memory: 38569  grad_norm: 53.1988  loss: 26.7933  loss_cls: 0.5875  loss_mask: 0.1706  loss_dice: 1.7497  d0.loss_cls: 1.1129  d0.loss_mask: 0.2152  d0.loss_dice: 1.9740  d1.loss_cls: 0.8121  d1.loss_mask: 0.2057  d1.loss_dice: 1.9651  d2.loss_cls: 0.6331  d2.loss_mask: 0.1845  d2.loss_dice: 1.8768  d3.loss_cls: 0.6165  d3.loss_mask: 0.1781  d3.loss_dice: 1.8194  d4.loss_cls: 0.6008  d4.loss_mask: 0.1755  d4.loss_dice: 1.7853  d5.loss_cls: 0.5901  d5.loss_mask: 0.1739  d5.loss_dice: 1.7798  d6.loss_cls: 0.5948  d6.loss_mask: 0.1731  d6.loss_dice: 1.7753  d7.loss_cls: 0.5982  d7.loss_mask: 0.1724  d7.loss_dice: 1.7635  d8.loss_cls: 0.5889  d8.loss_mask: 0.1716  d8.loss_dice: 1.7491
05/11 16:43:02 - mmengine - INFO - Iter(train) [23800/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:05:15  time: 1.5373  data_time: 0.0280  memory: 39197  grad_norm: 55.8753  loss: 25.5975  loss_cls: 0.5781  loss_mask: 0.1534  loss_dice: 1.6515  d0.loss_cls: 1.1192  d0.loss_mask: 0.1916  d0.loss_dice: 1.8862  d1.loss_cls: 0.8111  d1.loss_mask: 0.1847  d1.loss_dice: 1.8688  d2.loss_cls: 0.6317  d2.loss_mask: 0.1645  d2.loss_dice: 1.7883  d3.loss_cls: 0.6085  d3.loss_mask: 0.1591  d3.loss_dice: 1.7240  d4.loss_cls: 0.5884  d4.loss_mask: 0.1569  d4.loss_dice: 1.6940  d5.loss_cls: 0.5787  d5.loss_mask: 0.1565  d5.loss_dice: 1.6811  d6.loss_cls: 0.5870  d6.loss_mask: 0.1557  d6.loss_dice: 1.6827  d7.loss_cls: 0.5845  d7.loss_mask: 0.1545  d7.loss_dice: 1.6732  d8.loss_cls: 0.5743  d8.loss_mask: 0.1541  d8.loss_dice: 1.6550
05/11 16:44:19 - mmengine - INFO - Iter(train) [23850/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:03:56  time: 1.5492  data_time: 0.0341  memory: 39455  grad_norm: 42.0906  loss: 27.0333  loss_cls: 0.5929  loss_mask: 0.1829  loss_dice: 1.7482  d0.loss_cls: 1.1458  d0.loss_mask: 0.2279  d0.loss_dice: 1.9904  d1.loss_cls: 0.8196  d1.loss_mask: 0.2145  d1.loss_dice: 1.9723  d2.loss_cls: 0.6413  d2.loss_mask: 0.1957  d2.loss_dice: 1.8847  d3.loss_cls: 0.6200  d3.loss_mask: 0.1912  d3.loss_dice: 1.8197  d4.loss_cls: 0.6040  d4.loss_mask: 0.1880  d4.loss_dice: 1.7916  d5.loss_cls: 0.6019  d5.loss_mask: 0.1863  d5.loss_dice: 1.7737  d6.loss_cls: 0.6032  d6.loss_mask: 0.1852  d6.loss_dice: 1.7742  d7.loss_cls: 0.5987  d7.loss_mask: 0.1857  d7.loss_dice: 1.7649  d8.loss_cls: 0.5981  d8.loss_mask: 0.1833  d8.loss_dice: 1.7475
05/11 16:45:37 - mmengine - INFO - Iter(train) [23900/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:02:37  time: 1.5483  data_time: 0.0284  memory: 39131  grad_norm: 42.7883  loss: 26.8062  loss_cls: 0.5876  loss_mask: 0.1651  loss_dice: 1.7569  d0.loss_cls: 1.1380  d0.loss_mask: 0.2017  d0.loss_dice: 1.9677  d1.loss_cls: 0.8077  d1.loss_mask: 0.1946  d1.loss_dice: 1.9680  d2.loss_cls: 0.6325  d2.loss_mask: 0.1793  d2.loss_dice: 1.8866  d3.loss_cls: 0.6202  d3.loss_mask: 0.1729  d3.loss_dice: 1.8183  d4.loss_cls: 0.6023  d4.loss_mask: 0.1702  d4.loss_dice: 1.7954  d5.loss_cls: 0.5844  d5.loss_mask: 0.1696  d5.loss_dice: 1.7893  d6.loss_cls: 0.5909  d6.loss_mask: 0.1694  d6.loss_dice: 1.7841  d7.loss_cls: 0.5922  d7.loss_mask: 0.1681  d7.loss_dice: 1.7758  d8.loss_cls: 0.5904  d8.loss_mask: 0.1660  d8.loss_dice: 1.7611
05/11 16:46:54 - mmengine - INFO - Iter(train) [23950/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:01:18  time: 1.5430  data_time: 0.0297  memory: 39159  grad_norm: 42.5892  loss: 25.9422  loss_cls: 0.5938  loss_mask: 0.1540  loss_dice: 1.6838  d0.loss_cls: 1.1192  d0.loss_mask: 0.1906  d0.loss_dice: 1.8829  d1.loss_cls: 0.8094  d1.loss_mask: 0.1835  d1.loss_dice: 1.8843  d2.loss_cls: 0.6387  d2.loss_mask: 0.1660  d2.loss_dice: 1.8114  d3.loss_cls: 0.6221  d3.loss_mask: 0.1606  d3.loss_dice: 1.7474  d4.loss_cls: 0.6055  d4.loss_mask: 0.1572  d4.loss_dice: 1.7191  d5.loss_cls: 0.5979  d5.loss_mask: 0.1572  d5.loss_dice: 1.7064  d6.loss_cls: 0.6061  d6.loss_mask: 0.1562  d6.loss_dice: 1.7035  d7.loss_cls: 0.6000  d7.loss_mask: 0.1551  d7.loss_dice: 1.6937  d8.loss_cls: 0.5943  d8.loss_mask: 0.1546  d8.loss_dice: 1.6876
05/11 16:48:10 - mmengine - INFO - Exp name: mask2former_swin-L_8xb32-24k_coco_20250511_060300
05/11 16:48:10 - mmengine - INFO - Iter(train) [24000/24000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:00:00  time: 1.5293  data_time: 0.0281  memory: 38993  grad_norm: 47.0529  loss: 25.1331  loss_cls: 0.5658  loss_mask: 0.1526  loss_dice: 1.6245  d0.loss_cls: 1.0905  d0.loss_mask: 0.1953  d0.loss_dice: 1.8462  d1.loss_cls: 0.8008  d1.loss_mask: 0.1832  d1.loss_dice: 1.8295  d2.loss_cls: 0.6188  d2.loss_mask: 0.1648  d2.loss_dice: 1.7512  d3.loss_cls: 0.5954  d3.loss_mask: 0.1585  d3.loss_dice: 1.6897  d4.loss_cls: 0.5788  d4.loss_mask: 0.1567  d4.loss_dice: 1.6666  d5.loss_cls: 0.5683  d5.loss_mask: 0.1553  d5.loss_dice: 1.6505  d6.loss_cls: 0.5742  d6.loss_mask: 0.1545  d6.loss_dice: 1.6475  d7.loss_cls: 0.5744  d7.loss_mask: 0.1549  d7.loss_dice: 1.6408  d8.loss_cls: 0.5672  d8.loss_mask: 0.1527  d8.loss_dice: 1.6237
05/11 16:48:10 - mmengine - INFO - Saving checkpoint at 24000 iterations
05/11 16:48:54 - mmengine - INFO - Iter(val) [ 50/100]    eta: 0:00:36  time: 0.7319  data_time: 0.0145  memory: 5704  
05/11 16:49:31 - mmengine - INFO - Iter(val) [100/100]    eta: 0:00:00  time: 0.7330  data_time: 0.0140  memory: 5704  
05/11 16:49:54 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.22s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/11 16:50:01 - mmengine - INFO - start multi processing evaluation ...

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

0it [00:00, ?it/s]
0it [00:00, ?it/s]

  0%|          | 0/25552 [00:00<?, ?it/s]
 50%|█████     | 12778/25552 [00:00<00:00, 125345.71it/s]
100%|██████████| 25552/25552 [00:00<00:00, 139679.67it/s]
DONE (t=52.66s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.401
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.751
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.363
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.260
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.462
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.859
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.539
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.922
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.516
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.410
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.599
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.933
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.539
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.539
05/11 16:50:54 - mmengine - INFO - segm_mAP_copypaste: 0.401 0.751 0.363 0.260 0.462 0.859
05/11 16:50:54 - mmengine - INFO - segm_mAR_copypaste: 0.539 0.922 0.516 0.410 0.599 0.933
05/11 16:50:55 - mmengine - INFO - Iter(val) [100/100]    coco/segm_mAP: 0.4010  coco/segm_mAP_50: 0.7510  coco/segm_mAP_75: 0.3630  coco/segm_mAP_s: 0.2600  coco/segm_mAP_m: 0.4620  coco/segm_mAP_l: 0.8590  data_time: 0.0142  time: 0.7310
[rank0]:[W511 16:50:56.255683624 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Completed training for: mask2former_swin-L_8xb32-24k_coco.py

# TestLoop

/data1/max/instance_segmentation/mapchallenge-instance-segmentation
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.
  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
05/11 16:55:43 - mmengine - [4m[97mINFO[0m - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.2 (main, Jul 16 2024, 09:34:26) [GCC 11.4.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 239150084
    GPU 0: NVIDIA L40S
    CUDA_HOME: None
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.4.1+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.19.1+cu121
    OpenCV: 4.11.0
    MMEngine: 0.10.6

Runtime environment:
    cudnn_benchmark: False
    dist_cfg: {'backend': 'nccl'}
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    seed: 239150084
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

05/11 16:55:44 - mmengine - [4m[97mINFO[0m - Config:
_delete_ = True
auto_scale_lr = dict(base_batch_size=16, enable=False)
backbone_embed_multi = dict(decay_mult=0.0, lr_mult=0.1)
backbone_norm_multi = dict(decay_mult=0.0, lr_mult=0.1)
backend_args = None
base_batch_size = 16
batch_augments = [
    dict(
        img_pad_value=0,
        mask_pad_value=0,
        pad_mask=True,
        pad_seg=False,
        size=(
            320,
            320,
        ),
        type='BatchFixedSizePad'),
]
batch_size = 16
custom_keys = dict({
    'absolute_pos_embed':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone':
    dict(decay_mult=1.0, lr_mult=0.1),
    'backbone.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.patch_embed.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.0.blocks.0.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.0.blocks.1.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.0.downsample.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.1.blocks.0.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.1.blocks.1.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.1.downsample.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.0.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.1.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.10.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.11.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.12.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.13.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.14.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.15.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.16.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.17.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.2.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.3.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.4.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.5.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.6.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.7.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.8.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.9.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.downsample.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.3.blocks.0.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.3.blocks.1.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'level_embed':
    dict(decay_mult=0.0, lr_mult=1.0),
    'query_embed':
    dict(decay_mult=0.0, lr_mult=1.0),
    'query_feat':
    dict(decay_mult=0.0, lr_mult=1.0),
    'relative_position_bias_table':
    dict(decay_mult=0.0, lr_mult=0.1)
})
data_root = '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/'
dataset_type = 'SatelliteDataset'
default_hooks = dict(
    checkpoint=dict(
        by_epoch=False,
        interval=2000,
        max_keep_ckpts=3,
        rule='greater',
        save_best='coco/segm_mAP_50',
        save_last=True,
        type='CheckpointHook'),
    early_stopping=dict(
        min_delta=0.005,
        monitor='coco/segm_mAP_50',
        patience=6,
        type='EarlyStoppingHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(
        draw=True,
        show=True,
        test_out_dir='work_dirs/mask2former_swin-L_8xb32-24k_coco/',
        type='DetVisualizationHook',
        wait_time=2))
default_scope = 'mmdet'
depths = [
    2,
    2,
    18,
    2,
]
dynamic_intervals = [
    (
        23040.0,
        24000,
    ),
]
embed_multi = dict(decay_mult=0.0, lr_mult=1.0)
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
image_size = (
    320,
    320,
)
interval = 2000
launcher = 'none'
load_from = '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-L_8xb32-24k_coco/best_coco_segm_mAP_50_iter_18000.pth'
log_level = 'INFO'
log_processor = dict(by_epoch=False, type='LogProcessor', window_size=50)
max_iters = 24000
mean = [
    88.03,
    104.33,
    115.77,
]
model = dict(
    backbone=dict(
        attn_drop_rate=0.0,
        convert_weights=True,
        depths=[
            2,
            2,
            18,
            2,
        ],
        drop_path_rate=0.3,
        drop_rate=0.0,
        embed_dims=192,
        frozen_stages=-1,
        init_cfg=dict(
            checkpoint=
            'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth',
            type='Pretrained'),
        mlp_ratio=4,
        num_heads=[
            6,
            12,
            24,
            48,
        ],
        out_indices=(
            0,
            1,
            2,
            3,
        ),
        patch_norm=True,
        qk_scale=None,
        qkv_bias=True,
        type='SwinTransformer',
        window_size=12,
        with_cp=False),
    data_preprocessor=dict(
        batch_augments=[
            dict(
                img_pad_value=0,
                mask_pad_value=0,
                pad_mask=True,
                pad_seg=False,
                size=(
                    320,
                    320,
                ),
                type='BatchFixedSizePad'),
        ],
        bgr_to_rgb=True,
        mask_pad_value=0,
        mean=[
            88.03,
            104.33,
            115.77,
        ],
        pad_mask=True,
        pad_seg=False,
        pad_size_divisor=32,
        seg_pad_value=255,
        std=[
            44.37,
            43.48,
            41.56,
        ],
        type='DetDataPreprocessor'),
    init_cfg=None,
    panoptic_fusion_head=dict(
        init_cfg=None,
        loss_panoptic=None,
        num_stuff_classes=0,
        num_things_classes=1,
        type='MaskFormerFusionHead'),
    panoptic_head=dict(
        enforce_decoder_input_project=False,
        feat_channels=256,
        in_channels=[
            192,
            384,
            768,
            1536,
        ],
        loss_cls=dict(
            class_weight=[
                1.0,
                1.0,
            ],
            loss_weight=2.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=False),
        loss_dice=dict(
            activate=True,
            eps=1.0,
            loss_weight=5.0,
            naive_dice=True,
            reduction='mean',
            type='DiceLoss',
            use_sigmoid=True),
        loss_mask=dict(
            loss_weight=5.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=True),
        num_queries=100,
        num_stuff_classes=0,
        num_things_classes=1,
        num_transformer_feat_level=4,
        out_channels=256,
        pixel_decoder=dict(
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                layer_cfg=dict(
                    ffn_cfg=dict(
                        act_cfg=dict(inplace=True, type='ReLU'),
                        embed_dims=256,
                        feedforward_channels=2048,
                        ffn_drop=0.0,
                        num_fcs=2),
                    self_attn_cfg=dict(
                        batch_first=True,
                        dropout=0.0,
                        embed_dims=256,
                        num_heads=8,
                        num_levels=4,
                        num_points=4)),
                num_layers=6),
            norm_cfg=dict(num_groups=32, type='GN'),
            num_outs=4,
            positional_encoding=dict(normalize=True, num_feats=128),
            type='MSDeformAttnPixelDecoder'),
        positional_encoding=dict(normalize=True, num_feats=128),
        strides=[
            4,
            8,
            16,
            32,
        ],
        transformer_decoder=dict(
            init_cfg=None,
            layer_cfg=dict(
                cross_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8),
                ffn_cfg=dict(
                    act_cfg=dict(inplace=True, type='ReLU'),
                    embed_dims=256,
                    feedforward_channels=2048,
                    ffn_drop=0.0,
                    num_fcs=2),
                self_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8)),
            num_layers=9,
            return_intermediate=True),
        type='Mask2FormerHead'),
    test_cfg=dict(
        filter_low_score=True,
        instance_on=True,
        iou_thr=0.5,
        max_per_image=100,
        panoptic_on=False,
        semantic_on=False),
    train_cfg=dict(
        assigner=dict(
            match_costs=[
                dict(type='ClassificationCost', weight=2.0),
                dict(
                    type='CrossEntropyLossCost', use_sigmoid=True, weight=5.0),
                dict(eps=1.0, pred_act=True, type='DiceCost', weight=5.0),
            ],
            type='HungarianAssigner'),
        importance_sample_ratio=0.75,
        num_points=12544,
        oversample_ratio=3.0,
        sampler=dict(type='MaskPseudoSampler')),
    type='Mask2Former')
num_classes = 1
num_stuff_classes = 1
num_things_classes = 1
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.01, norm_type=2),
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        eps=1e-08,
        lr=0.0001,
        type='AdamW',
        weight_decay=0.05),
    paramwise_cfg=dict(
        custom_keys=dict({
            'absolute_pos_embed':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone':
            dict(decay_mult=1.0, lr_mult=0.1),
            'backbone.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.patch_embed.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.0.blocks.0.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.0.blocks.1.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.0.downsample.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.1.blocks.0.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.1.blocks.1.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.1.downsample.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.0.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.1.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.10.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.11.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.12.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.13.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.14.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.15.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.16.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.17.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.2.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.3.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.4.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.5.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.6.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.7.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.8.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.9.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.downsample.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.3.blocks.0.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.3.blocks.1.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'level_embed':
            dict(decay_mult=0.0, lr_mult=1.0),
            'query_embed':
            dict(decay_mult=0.0, lr_mult=1.0),
            'query_feat':
            dict(decay_mult=0.0, lr_mult=1.0),
            'relative_position_bias_table':
            dict(decay_mult=0.0, lr_mult=0.1)
        }),
        norm_decay_mult=0.0),
    type='OptimWrapper')
param_scheduler = dict(
    begin=0,
    by_epoch=False,
    end=24000,
    gamma=0.1,
    milestones=[
        19200.0,
        21600.0,
    ],
    type='MultiStepLR')
pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth'
std = [
    44.37,
    43.48,
    41.56,
]
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=16,
    dataset=dict(
        ann_file=
        '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/test/annotations/annotation_non_augmented.json',
        backend_args=None,
        data_prefix=dict(
            img=
            '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/test/images/'
        ),
        data_root='/usr/instance_segmentation/',
        filter_cfg=dict(filter_empty_gt=True, min_size=15),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                320,
                320,
            ), type='Resize'),
            dict(
                pad_val=dict(img=(
                    114,
                    114,
                    114,
                )),
                size=(
                    320,
                    320,
                ),
                type='Pad'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='SatelliteDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    ann_file=
    '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/test/annotations/annotation_non_augmented.json',
    backend_args=None,
    format_only=False,
    metric=[
        'segm',
    ],
    type='CocoMetric')
test_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        320,
        320,
    ), type='Resize'),
    dict(pad_val=dict(img=(
        114,
        114,
        114,
    )), size=(
        320,
        320,
    ), type='Pad'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(
    dynamic_intervals=[
        (
            23040.0,
            24000,
        ),
    ],
    max_iters=24000,
    type='IterBasedTrainLoop',
    val_interval=2000)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=16,
    dataset=dict(
        ann_file=
        '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/train/annotations/annotation_non_augmented.json',
        backend_args=None,
        data_prefix=dict(
            img=
            '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/train/images/'
        ),
        data_root='/usr/instance_segmentation/',
        filter_cfg=dict(filter_empty_gt=True, min_size=15),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=True,
                with_mask=True),
            dict(img_scale=(
                320,
                320,
            ), pad_val=114.0, type='CachedMosaic'),
            dict(
                keep_ratio=True,
                ratio_range=(
                    0.1,
                    2.0,
                ),
                scale=(
                    320,
                    320,
                ),
                type='RandomResize'),
            dict(
                allow_negative_crop=True,
                crop_size=(
                    320,
                    320,
                ),
                recompute_bbox=True,
                type='RandomCrop'),
            dict(type='YOLOXHSVRandomAug'),
            dict(prob=0.5, type='RandomFlip'),
            dict(
                pad_val=dict(img=(
                    114,
                    114,
                    114,
                )),
                size=(
                    320,
                    320,
                ),
                type='Pad'),
            dict(
                img_scale=(
                    320,
                    320,
                ),
                max_cached_images=20,
                pad_val=(
                    114,
                    114,
                    114,
                ),
                ratio_range=(
                    1.0,
                    1.0,
                ),
                type='CachedMixUp'),
            dict(min_gt_bbox_wh=(
                1,
                1,
            ), type='FilterAnnotations'),
            dict(type='PackDetInputs'),
        ],
        type='SatelliteDataset'),
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='InfiniteSampler'))
train_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(
        poly2mask=False,
        type='LoadAnnotations',
        with_bbox=True,
        with_mask=True),
    dict(img_scale=(
        320,
        320,
    ), pad_val=114.0, type='CachedMosaic'),
    dict(
        keep_ratio=True,
        ratio_range=(
            0.1,
            2.0,
        ),
        scale=(
            320,
            320,
        ),
        type='RandomResize'),
    dict(
        allow_negative_crop=True,
        crop_size=(
            320,
            320,
        ),
        recompute_bbox=True,
        type='RandomCrop'),
    dict(type='YOLOXHSVRandomAug'),
    dict(prob=0.5, type='RandomFlip'),
    dict(pad_val=dict(img=(
        114,
        114,
        114,
    )), size=(
        320,
        320,
    ), type='Pad'),
    dict(
        img_scale=(
            320,
            320,
        ),
        max_cached_images=20,
        pad_val=(
            114,
            114,
            114,
        ),
        ratio_range=(
            1.0,
            1.0,
        ),
        type='CachedMixUp'),
    dict(min_gt_bbox_wh=(
        1,
        1,
    ), type='FilterAnnotations'),
    dict(type='PackDetInputs'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=16,
    dataset=dict(
        ann_file=
        '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/val/annotations/annotation_non_augmented.json',
        backend_args=None,
        data_prefix=dict(
            img=
            '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/val/images/'
        ),
        data_root='/usr/instance_segmentation/',
        filter_cfg=dict(filter_empty_gt=True, min_size=15),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                320,
                320,
            ), type='Resize'),
            dict(
                pad_val=dict(img=(
                    114,
                    114,
                    114,
                )),
                size=(
                    320,
                    320,
                ),
                type='Pad'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='SatelliteDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    ann_file=
    '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/val/annotations/annotation_non_augmented.json',
    backend_args=None,
    format_only=False,
    metric=[
        'segm',
    ],
    type='CocoMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = 'work_dirs/mask2former_swin-L_8xb32-24k_coco'

05/11 16:55:46 - mmengine - [4m[97mINFO[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
/data1/max/instance_segmentation/mmdetection/mmdet/engine/hooks/visualization_hook.py:68: UserWarning: The show is True, it means that only the prediction results are visualized without storing data, so vis_backends needs to be excluded.
  warnings.warn('The show is True, it means that only '
05/11 16:55:46 - mmengine - [4m[97mINFO[0m - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
(LOWEST      ) EarlyStoppingHook                  
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
(LOWEST      ) EarlyStoppingHook                  
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
loading annotations into memory...
Done (t=0.35s)
creating index...
index created!
loading annotations into memory...
Done (t=0.37s)
creating index...
index created!
loading annotations into memory...
Done (t=2.70s)
creating index...
index created!
Loads checkpoint by local backend from path: /data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-L_8xb32-24k_coco/best_coco_segm_mAP_50_iter_18000.pth
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/mmengine/runner/checkpoint.py:347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(filename, map_location=map_location)
05/11 16:55:58 - mmengine - [4m[97mINFO[0m - Load checkpoint from /data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-L_8xb32-24k_coco/best_coco_segm_mAP_50_iter_18000.pth
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/mmengine/visualization/visualizer.py:760: UserWarning: Warning: The bbox is out of bounds, the drawn bbox may not be in the image
  warnings.warn(
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/mmengine/visualization/visualizer.py:831: UserWarning: Warning: The polygon is out of bounds, the drawn polygon may not be in the image
  warnings.warn(
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/mmengine/visualization/visualizer.py:760: UserWarning: Warning: The bbox is out of bounds, the drawn bbox may not be in the image
  warnings.warn(
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/mmengine/visualization/visualizer.py:831: UserWarning: Warning: The polygon is out of bounds, the drawn polygon may not be in the image
  warnings.warn(
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/mmengine/visualization/visualizer.py:508: UserWarning: Warning: The text is out of bounds, the drawn text may not be in the image
  warnings.warn(
05/11 16:57:39 - mmengine - [4m[97mINFO[0m - Iter(test) [ 50/400]    eta: 0:11:42  time: 2.0080  data_time: 1.2901  memory: 3142  
05/11 16:59:20 - mmengine - [4m[97mINFO[0m - Iter(test) [100/400]    eta: 0:10:02  time: 2.0056  data_time: 1.2970  memory: 3142  
05/11 17:01:02 - mmengine - [4m[97mINFO[0m - Iter(test) [150/400]    eta: 0:08:25  time: 2.0568  data_time: 1.3359  memory: 3142  
05/11 17:02:45 - mmengine - [4m[97mINFO[0m - Iter(test) [200/400]    eta: 0:06:46  time: 2.0573  data_time: 1.3249  memory: 3142  
05/11 17:04:25 - mmengine - [4m[97mINFO[0m - Iter(test) [250/400]    eta: 0:05:03  time: 1.9857  data_time: 1.2688  memory: 3142  
05/11 17:06:03 - mmengine - [4m[97mINFO[0m - Iter(test) [300/400]    eta: 0:03:21  time: 1.9620  data_time: 1.2253  memory: 3142  
05/11 17:07:39 - mmengine - [4m[97mINFO[0m - Iter(test) [350/400]    eta: 0:01:39  time: 1.9164  data_time: 1.1945  memory: 3142  
05/11 17:09:19 - mmengine - [4m[97mINFO[0m - Iter(test) [400/400]    eta: 0:00:00  time: 2.0135  data_time: 1.3085  memory: 3142  
05/11 17:09:39 - mmengine - [4m[97mINFO[0m - Evaluating segm...
Loading and preparing results...
DONE (t=3.18s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/11 17:09:46 - mmengine - [4m[97mINFO[0m - start multi processing evaluation ...
0it [00:00, ?it/s]
0it [00:00, ?it/s]
0it [00:00, ?it/s]
0it [00:00, ?it/s]
0it [00:00, ?it/s]
0it [00:00, ?it/s]
0it [00:00, ?it/s]
100%|█████████████████████████████████| 25600/25600 [00:00<00:00, 146318.65it/s]
DONE (t=54.27s).
Accumulating evaluation results...
DONE (t=0.06s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.406
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.695
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.427
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.202
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.521
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.624
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.529
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.845
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.569
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.373
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.606
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.867
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.529
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.529
05/11 17:10:41 - mmengine - [4m[97mINFO[0m - segm_mAP_copypaste: 0.406 0.695 0.427 0.202 0.521 0.624
05/11 17:10:41 - mmengine - [4m[97mINFO[0m - segm_mAR_copypaste: 0.529 0.845 0.569 0.373 0.606 0.867 
05/11 17:10:42 - mmengine - [4m[97mINFO[0m - Iter(test) [400/400]    coco/segm_mAP: 0.4060  coco/segm_mAP_50: 0.6950  coco/segm_mAP_75: 0.4270  coco/segm_mAP_s: 0.2020  coco/segm_mAP_m: 0.5210  coco/segm_mAP_l: 0.6240  data_time: 1.2806  time: 2.0007

