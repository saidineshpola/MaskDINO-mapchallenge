05/08 02:49:06 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.2 (main, Jul 16 2024, 09:34:26) [GCC 11.4.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 770243202
    GPU 0,1,2,3: NVIDIA L40S
    CUDA_HOME: /usr/local/cuda-12.2
    NVCC: Cuda compilation tools, release 12.2, V12.2.140
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.4.1+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.19.1+cu121
    OpenCV: 4.11.0
    MMEngine: 0.10.6

Runtime environment:
    cudnn_benchmark: False
    dist_cfg: {'backend': 'nccl'}
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    seed: 770243202
    Distributed launcher: pytorch
    Distributed training: True
    GPU number: 4
------------------------------------------------------------

05/08 02:49:07 - mmengine - INFO - Config:
_delete_ = True
auto_scale_lr = dict(base_batch_size=32, enable=False)
backbone_embed_multi = dict(decay_mult=0.0, lr_mult=0.1)
backbone_norm_multi = dict(decay_mult=0.0, lr_mult=0.1)
backend_args = None
base_batch_size = 32
batch_augments = [
    dict(
        img_pad_value=0,
        mask_pad_value=0,
        pad_mask=True,
        pad_seg=False,
        size=(
            320,
            320,
        ),
        type='BatchFixedSizePad'),
]
batch_size = 32
custom_keys = dict({
    'absolute_pos_embed':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone':
    dict(decay_mult=1.0, lr_mult=0.1),
    'backbone.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.patch_embed.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.0.blocks.0.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.0.blocks.1.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.0.downsample.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.1.blocks.0.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.1.blocks.1.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.1.downsample.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.0.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.1.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.10.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.11.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.12.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.13.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.14.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.15.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.16.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.17.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.2.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.3.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.4.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.5.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.6.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.7.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.8.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.9.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.downsample.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.3.blocks.0.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.3.blocks.1.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'level_embed':
    dict(decay_mult=0.0, lr_mult=1.0),
    'query_embed':
    dict(decay_mult=0.0, lr_mult=1.0),
    'query_feat':
    dict(decay_mult=0.0, lr_mult=1.0),
    'relative_position_bias_table':
    dict(decay_mult=0.0, lr_mult=0.1)
})
data_root = '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/'
dataset_type = 'SatelliteDataset'
default_hooks = dict(
    checkpoint=dict(
        by_epoch=False,
        interval=1000,
        max_keep_ckpts=3,
        rule='greater',
        save_best='coco/segm_mAP_50',
        save_last=True,
        type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='DetVisualizationHook'))
default_scope = 'mmdet'
depths = [
    2,
    2,
    18,
    2,
]
dynamic_intervals = [
    (
        19200.0,
        20000,
    ),
]
embed_multi = dict(decay_mult=0.0, lr_mult=1.0)
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
image_size = (
    320,
    320,
)
interval = 1000
launcher = 'pytorch'
load_from = 'https://download.openmmlab.com/mmdetection/v3.0/mask2former/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20220504_001756-c9d0c4f2.pth'
log_level = 'INFO'
log_processor = dict(by_epoch=False, type='LogProcessor', window_size=50)
max_iters = 20000
mean = [
    88.03,
    104.33,
    115.77,
]
model = dict(
    backbone=dict(
        attn_drop_rate=0.0,
        convert_weights=True,
        depths=[
            2,
            2,
            18,
            2,
        ],
        drop_path_rate=0.3,
        drop_rate=0.0,
        embed_dims=96,
        frozen_stages=-1,
        init_cfg=dict(
            checkpoint=
            'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth',
            type='Pretrained'),
        mlp_ratio=4,
        num_heads=[
            3,
            6,
            12,
            24,
        ],
        out_indices=(
            0,
            1,
            2,
            3,
        ),
        patch_norm=True,
        qk_scale=None,
        qkv_bias=True,
        type='SwinTransformer',
        window_size=7,
        with_cp=False),
    data_preprocessor=dict(
        batch_augments=[
            dict(
                img_pad_value=0,
                mask_pad_value=0,
                pad_mask=True,
                pad_seg=False,
                size=(
                    320,
                    320,
                ),
                type='BatchFixedSizePad'),
        ],
        bgr_to_rgb=True,
        mask_pad_value=0,
        mean=[
            88.03,
            104.33,
            115.77,
        ],
        pad_mask=True,
        pad_seg=False,
        pad_size_divisor=32,
        seg_pad_value=255,
        std=[
            44.37,
            43.48,
            41.56,
        ],
        type='DetDataPreprocessor'),
    init_cfg=None,
    panoptic_fusion_head=dict(
        init_cfg=None,
        loss_panoptic=None,
        num_stuff_classes=0,
        num_things_classes=1,
        type='MaskFormerFusionHead'),
    panoptic_head=dict(
        enforce_decoder_input_project=False,
        feat_channels=256,
        in_channels=[
            96,
            192,
            384,
            768,
        ],
        loss_cls=dict(
            class_weight=[
                1.0,
                1.0,
            ],
            loss_weight=2.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=False),
        loss_dice=dict(
            activate=True,
            eps=1.0,
            loss_weight=5.0,
            naive_dice=True,
            reduction='mean',
            type='DiceLoss',
            use_sigmoid=True),
        loss_mask=dict(
            loss_weight=5.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=True),
        num_queries=100,
        num_stuff_classes=0,
        num_things_classes=1,
        num_transformer_feat_level=3,
        out_channels=256,
        pixel_decoder=dict(
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                layer_cfg=dict(
                    ffn_cfg=dict(
                        act_cfg=dict(inplace=True, type='ReLU'),
                        embed_dims=256,
                        feedforward_channels=320,
                        ffn_drop=0.0,
                        num_fcs=2),
                    self_attn_cfg=dict(
                        batch_first=True,
                        dropout=0.0,
                        embed_dims=256,
                        num_heads=8,
                        num_levels=3,
                        num_points=4)),
                num_layers=6),
            norm_cfg=dict(num_groups=32, type='GN'),
            num_outs=3,
            positional_encoding=dict(normalize=True, num_feats=128),
            type='MSDeformAttnPixelDecoder'),
        positional_encoding=dict(normalize=True, num_feats=128),
        strides=[
            4,
            8,
            16,
            32,
        ],
        transformer_decoder=dict(
            init_cfg=None,
            layer_cfg=dict(
                cross_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8),
                ffn_cfg=dict(
                    act_cfg=dict(inplace=True, type='ReLU'),
                    embed_dims=256,
                    feedforward_channels=2048,
                    ffn_drop=0.0,
                    num_fcs=2),
                self_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8)),
            num_layers=9,
            return_intermediate=True),
        type='Mask2FormerHead'),
    test_cfg=dict(
        filter_low_score=True,
        instance_on=True,
        iou_thr=0.5,
        max_per_image=100,
        panoptic_on=False,
        semantic_on=False),
    train_cfg=dict(
        assigner=dict(
            match_costs=[
                dict(type='ClassificationCost', weight=2.0),
                dict(
                    type='CrossEntropyLossCost', use_sigmoid=True, weight=5.0),
                dict(eps=1.0, pred_act=True, type='DiceCost', weight=5.0),
            ],
            type='HungarianAssigner'),
        importance_sample_ratio=0.75,
        num_points=12544,
        oversample_ratio=3.0,
        sampler=dict(type='MaskPseudoSampler')),
    type='Mask2Former')
num_classes = 1
num_stuff_classes = 1
num_things_classes = 1
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.01, norm_type=2),
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        eps=1e-08,
        lr=0.0001,
        type='AdamW',
        weight_decay=0.05),
    paramwise_cfg=dict(
        custom_keys=dict({
            'absolute_pos_embed':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone':
            dict(decay_mult=1.0, lr_mult=0.1),
            'backbone.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.patch_embed.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.0.blocks.0.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.0.blocks.1.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.0.downsample.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.1.blocks.0.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.1.blocks.1.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.1.downsample.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.0.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.1.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.10.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.11.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.12.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.13.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.14.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.15.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.16.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.17.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.2.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.3.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.4.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.5.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.6.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.7.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.8.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.9.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.downsample.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.3.blocks.0.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.3.blocks.1.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'level_embed':
            dict(decay_mult=0.0, lr_mult=1.0),
            'query_embed':
            dict(decay_mult=0.0, lr_mult=1.0),
            'query_feat':
            dict(decay_mult=0.0, lr_mult=1.0),
            'relative_position_bias_table':
            dict(decay_mult=0.0, lr_mult=0.1)
        }),
        norm_decay_mult=0.0),
    type='OptimWrapper')
param_scheduler = dict(
    begin=0,
    by_epoch=False,
    end=20000,
    gamma=0.1,
    milestones=[
        16000.0,
        18000.0,
    ],
    type='MultiStepLR')
pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth'
std = [
    44.37,
    43.48,
    41.56,
]
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=32,
    dataset=dict(
        ann_file=
        '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/test/annotations/annotation_non_augmented.json',
        backend_args=None,
        data_prefix=dict(
            img=
            '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/test/images/'
        ),
        data_root='/usr/instance_segmentation/',
        filter_cfg=dict(filter_empty_gt=True, min_size=15),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                320,
                320,
            ), type='Resize'),
            dict(
                pad_val=dict(img=(
                    114,
                    114,
                    114,
                )),
                size=(
                    320,
                    320,
                ),
                type='Pad'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='SatelliteDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    ann_file=
    '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/test/annotations/annotation_non_augmented.json',
    backend_args=None,
    format_only=False,
    metric=[
        'segm',
    ],
    type='CocoMetric')
test_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        320,
        320,
    ), type='Resize'),
    dict(pad_val=dict(img=(
        114,
        114,
        114,
    )), size=(
        320,
        320,
    ), type='Pad'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(
    dynamic_intervals=[
        (
            19200.0,
            20000,
        ),
    ],
    max_iters=20000,
    type='IterBasedTrainLoop',
    val_interval=1000)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=32,
    dataset=dict(
        ann_file=
        '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/train/annotations/annotation_non_augmented.json',
        backend_args=None,
        data_prefix=dict(
            img=
            '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/train/images/'
        ),
        data_root='/usr/instance_segmentation/',
        filter_cfg=dict(filter_empty_gt=True, min_size=15),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=True,
                with_mask=True),
            dict(img_scale=(
                320,
                320,
            ), pad_val=114.0, type='CachedMosaic'),
            dict(
                keep_ratio=True,
                ratio_range=(
                    0.1,
                    2.0,
                ),
                scale=(
                    320,
                    320,
                ),
                type='RandomResize'),
            dict(
                allow_negative_crop=True,
                crop_size=(
                    320,
                    320,
                ),
                recompute_bbox=True,
                type='RandomCrop'),
            dict(type='YOLOXHSVRandomAug'),
            dict(prob=0.5, type='RandomFlip'),
            dict(
                pad_val=dict(img=(
                    114,
                    114,
                    114,
                )),
                size=(
                    320,
                    320,
                ),
                type='Pad'),
            dict(
                img_scale=(
                    320,
                    320,
                ),
                max_cached_images=20,
                pad_val=(
                    114,
                    114,
                    114,
                ),
                ratio_range=(
                    1.0,
                    1.0,
                ),
                type='CachedMixUp'),
            dict(min_gt_bbox_wh=(
                1,
                1,
            ), type='FilterAnnotations'),
            dict(type='PackDetInputs'),
        ],
        type='SatelliteDataset'),
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(
        poly2mask=False,
        type='LoadAnnotations',
        with_bbox=True,
        with_mask=True),
    dict(img_scale=(
        320,
        320,
    ), pad_val=114.0, type='CachedMosaic'),
    dict(
        keep_ratio=True,
        ratio_range=(
            0.1,
            2.0,
        ),
        scale=(
            320,
            320,
        ),
        type='RandomResize'),
    dict(
        allow_negative_crop=True,
        crop_size=(
            320,
            320,
        ),
        recompute_bbox=True,
        type='RandomCrop'),
    dict(type='YOLOXHSVRandomAug'),
    dict(prob=0.5, type='RandomFlip'),
    dict(pad_val=dict(img=(
        114,
        114,
        114,
    )), size=(
        320,
        320,
    ), type='Pad'),
    dict(
        img_scale=(
            320,
            320,
        ),
        max_cached_images=20,
        pad_val=(
            114,
            114,
            114,
        ),
        ratio_range=(
            1.0,
            1.0,
        ),
        type='CachedMixUp'),
    dict(min_gt_bbox_wh=(
        1,
        1,
    ), type='FilterAnnotations'),
    dict(type='PackDetInputs'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=32,
    dataset=dict(
        ann_file=
        '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/val/annotations/annotation_non_augmented.json',
        backend_args=None,
        data_prefix=dict(
            img=
            '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/val/images/'
        ),
        data_root='/usr/instance_segmentation/',
        filter_cfg=dict(filter_empty_gt=True, min_size=15),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                320,
                320,
            ), type='Resize'),
            dict(
                pad_val=dict(img=(
                    114,
                    114,
                    114,
                )),
                size=(
                    320,
                    320,
                ),
                type='Pad'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='SatelliteDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    ann_file=
    '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/val/annotations/annotation_non_augmented.json',
    backend_args=None,
    format_only=False,
    metric=[
        'segm',
    ],
    type='CocoMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = 'work_dirs/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco'

05/08 02:49:09 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
loading annotations into memory...loading annotations into memory...

loading annotations into memory...
loading annotations into memory...
Done (t=2.78s)
creating index...
Done (t=2.82s)
creating index...
Done (t=2.83s)
creating index...
index created!
index created!
index created!
Done (t=2.96s)
creating index...
index created!
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:weight_decay=0.05
Done (t=0.49s)
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:decay_mult=1.0
creating index...
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:weight_decay=0.05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:decay_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm0.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm0.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm0.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm0.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm0.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm0.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm0.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm0.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr=1e-05
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr_mult=0.1
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.0.gn.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.0.gn.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.1.gn.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.1.gn.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.2.gn.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.input_convs.2.gn.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.0.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.0.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.0.norms.1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.0.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.0.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.1.norms.1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.0.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.0.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.2.norms.1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.0.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.0.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.3.norms.1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.0.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.0.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.4.norms.1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.0.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.0.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.encoder.layers.5.norms.1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.lateral_convs.0.gn.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.lateral_convs.0.gn.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.output_convs.0.gn.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.pixel_decoder.output_convs.0.gn.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.0.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.0.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.0.norms.2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.0.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.0.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.1.norms.2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.0.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.0.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.2.norms.2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.0.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.0.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.3.norms.2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.0.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.0.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.4.norms.2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.0.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.0.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.5.norms.2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.0.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.0.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.6.norms.2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.0.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.0.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.7.norms.2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.0.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.0.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.1.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.1.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.2.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.layers.8.norms.2.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.post_norm.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.transformer_decoder.post_norm.bias:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:lr=0.0001
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.query_embed.weight:lr_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:lr=0.0001
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.query_feat.weight:lr_mult=1.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:lr=0.0001
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:weight_decay=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:decay_mult=0.0
05/08 02:49:16 - mmengine - INFO - paramwise_options -- panoptic_head.level_embed.weight:lr_mult=1.0
Done (t=0.49s)
creating index...
index created!
index created!
Done (t=0.49s)
creating index...
index created!
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.47s)
creating index...
index created!
loading annotations into memory...
Done (t=0.40s)
creating index...
Done (t=0.39s)
creating index...
index created!
index created!
Done (t=0.38s)
creating index...
index created!
loading annotations into memory...
Done (t=0.38s)
creating index...
index created!
05/08 02:49:19 - mmengine - INFO - Loads checkpoint by http backend from path: https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth
Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v3.0/mask2former/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20220504_001756-c9d0c4f2.pth
Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v3.0/mask2former/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20220504_001756-c9d0c4f2.pth
Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v3.0/mask2former/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20220504_001756-c9d0c4f2.pth
Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v3.0/mask2former/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20220504_001756-c9d0c4f2.pth
The model and loaded state dict do not match exactly

size mismatch for panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([320, 256]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.0.ffn.layers.1.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([256, 320]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([320, 256]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.1.ffn.layers.1.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([256, 320]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([320, 256]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.2.ffn.layers.1.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([256, 320]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([320, 256]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.3.ffn.layers.1.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([256, 320]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([320, 256]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.4.ffn.layers.1.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([256, 320]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([320, 256]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for panoptic_head.pixel_decoder.encoder.layers.5.ffn.layers.1.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([256, 320]).
size mismatch for panoptic_head.cls_embed.weight: copying a param with shape torch.Size([81, 256]) from checkpoint, the shape in current model is torch.Size([2, 256]).
size mismatch for panoptic_head.cls_embed.bias: copying a param with shape torch.Size([81]) from checkpoint, the shape in current model is torch.Size([2]).
05/08 02:49:20 - mmengine - INFO - Load checkpoint from https://download.openmmlab.com/mmdetection/v3.0/mask2former/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20220504_001756-c9d0c4f2.pth
05/08 02:49:20 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
05/08 02:49:20 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
05/08 02:49:20 - mmengine - INFO - Checkpoints will be saved to /data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco.
05/08 02:50:32 - mmengine - INFO - Iter(train) [   50/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:59:57  time: 1.4435  data_time: 0.0561  memory: 29207  grad_norm: 59.7282  loss: 50.6823  loss_cls: 0.9128  loss_mask: 0.4171  loss_dice: 3.6467  d0.loss_cls: 1.1677  d0.loss_mask: 0.4423  d0.loss_dice: 3.8652  d1.loss_cls: 0.9754  d1.loss_mask: 0.4239  d1.loss_dice: 3.7441  d2.loss_cls: 0.9732  d2.loss_mask: 0.4207  d2.loss_dice: 3.7060  d3.loss_cls: 0.9258  d3.loss_mask: 0.4373  d3.loss_dice: 3.6745  d4.loss_cls: 0.9224  d4.loss_mask: 0.4196  d4.loss_dice: 3.6781  d5.loss_cls: 0.9180  d5.loss_mask: 0.4144  d5.loss_dice: 3.6710  d6.loss_cls: 0.9005  d6.loss_mask: 0.4290  d6.loss_dice: 3.6537  d7.loss_cls: 0.9003  d7.loss_mask: 0.4126  d7.loss_dice: 3.6602  d8.loss_cls: 0.8988  d8.loss_mask: 0.4126  d8.loss_dice: 3.6584
05/08 02:51:44 - mmengine - INFO - Iter(train) [  100/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:59:37  time: 1.4487  data_time: 0.0603  memory: 29078  grad_norm: 43.9293  loss: 40.4477  loss_cls: 0.7397  loss_mask: 0.3173  loss_dice: 2.8102  d0.loss_cls: 1.0860  d0.loss_mask: 0.3453  d0.loss_dice: 3.2582  d1.loss_cls: 0.8042  d1.loss_mask: 0.3408  d1.loss_dice: 3.1035  d2.loss_cls: 0.7856  d2.loss_mask: 0.3358  d2.loss_dice: 2.9995  d3.loss_cls: 0.7547  d3.loss_mask: 0.3288  d3.loss_dice: 2.9064  d4.loss_cls: 0.7327  d4.loss_mask: 0.3273  d4.loss_dice: 2.8997  d5.loss_cls: 0.7349  d5.loss_mask: 0.3234  d5.loss_dice: 2.8742  d6.loss_cls: 0.7250  d6.loss_mask: 0.3240  d6.loss_dice: 2.8308  d7.loss_cls: 0.7344  d7.loss_mask: 0.3193  d7.loss_dice: 2.8278  d8.loss_cls: 0.7382  d8.loss_mask: 0.3202  d8.loss_dice: 2.8196
05/08 02:52:57 - mmengine - INFO - Iter(train) [  150/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:59:50  time: 1.4590  data_time: 0.0565  memory: 28942  grad_norm: 43.3658  loss: 38.6625  loss_cls: 0.7094  loss_mask: 0.2868  loss_dice: 2.6843  d0.loss_cls: 1.0437  d0.loss_mask: 0.3231  d0.loss_dice: 3.1110  d1.loss_cls: 0.7982  d1.loss_mask: 0.3187  d1.loss_dice: 3.0095  d2.loss_cls: 0.7709  d2.loss_mask: 0.3076  d2.loss_dice: 2.8881  d3.loss_cls: 0.7269  d3.loss_mask: 0.2993  d3.loss_dice: 2.7797  d4.loss_cls: 0.6962  d4.loss_mask: 0.2961  d4.loss_dice: 2.7744  d5.loss_cls: 0.7033  d5.loss_mask: 0.2925  d5.loss_dice: 2.7517  d6.loss_cls: 0.6995  d6.loss_mask: 0.2901  d6.loss_dice: 2.7116  d7.loss_cls: 0.7046  d7.loss_mask: 0.2883  d7.loss_dice: 2.7029  d8.loss_cls: 0.7094  d8.loss_mask: 0.2877  d8.loss_dice: 2.6970
05/08 02:54:09 - mmengine - INFO - Iter(train) [  200/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:56:45  time: 1.4276  data_time: 0.0562  memory: 28586  grad_norm: 43.7326  loss: 37.5524  loss_cls: 0.7158  loss_mask: 0.2778  loss_dice: 2.5799  d0.loss_cls: 1.0254  d0.loss_mask: 0.3200  d0.loss_dice: 2.9972  d1.loss_cls: 0.8017  d1.loss_mask: 0.3133  d1.loss_dice: 2.9110  d2.loss_cls: 0.7663  d2.loss_mask: 0.2991  d2.loss_dice: 2.7874  d3.loss_cls: 0.7217  d3.loss_mask: 0.2906  d3.loss_dice: 2.6798  d4.loss_cls: 0.6990  d4.loss_mask: 0.2864  d4.loss_dice: 2.6744  d5.loss_cls: 0.7059  d5.loss_mask: 0.2852  d5.loss_dice: 2.6495  d6.loss_cls: 0.7077  d6.loss_mask: 0.2812  d6.loss_dice: 2.6041  d7.loss_cls: 0.7073  d7.loss_mask: 0.2811  d7.loss_dice: 2.6006  d8.loss_cls: 0.7100  d8.loss_mask: 0.2800  d8.loss_dice: 2.5931
05/08 02:55:22 - mmengine - INFO - Iter(train) [  250/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:57:21  time: 1.4721  data_time: 0.0613  memory: 29061  grad_norm: 43.2151  loss: 38.9083  loss_cls: 0.7338  loss_mask: 0.2850  loss_dice: 2.6847  d0.loss_cls: 1.0449  d0.loss_mask: 0.3243  d0.loss_dice: 3.1211  d1.loss_cls: 0.8229  d1.loss_mask: 0.3219  d1.loss_dice: 3.0193  d2.loss_cls: 0.7760  d2.loss_mask: 0.3067  d2.loss_dice: 2.9061  d3.loss_cls: 0.7334  d3.loss_mask: 0.2981  d3.loss_dice: 2.7923  d4.loss_cls: 0.7164  d4.loss_mask: 0.2945  d4.loss_dice: 2.7826  d5.loss_cls: 0.7225  d5.loss_mask: 0.2918  d5.loss_dice: 2.7591  d6.loss_cls: 0.7259  d6.loss_mask: 0.2874  d6.loss_dice: 2.7153  d7.loss_cls: 0.7240  d7.loss_mask: 0.2876  d7.loss_dice: 2.7113  d8.loss_cls: 0.7306  d8.loss_mask: 0.2846  d8.loss_dice: 2.7039
05/08 02:56:34 - mmengine - INFO - Iter(train) [  300/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:55:21  time: 1.4357  data_time: 0.0586  memory: 28773  grad_norm: 46.4919  loss: 36.5001  loss_cls: 0.7053  loss_mask: 0.2739  loss_dice: 2.4909  d0.loss_cls: 1.0278  d0.loss_mask: 0.3184  d0.loss_dice: 2.8846  d1.loss_cls: 0.7884  d1.loss_mask: 0.3093  d1.loss_dice: 2.8163  d2.loss_cls: 0.7415  d2.loss_mask: 0.2939  d2.loss_dice: 2.7087  d3.loss_cls: 0.7077  d3.loss_mask: 0.2862  d3.loss_dice: 2.5984  d4.loss_cls: 0.6905  d4.loss_mask: 0.2821  d4.loss_dice: 2.5833  d5.loss_cls: 0.6944  d5.loss_mask: 0.2791  d5.loss_dice: 2.5683  d6.loss_cls: 0.6976  d6.loss_mask: 0.2768  d6.loss_dice: 2.5133  d7.loss_cls: 0.6931  d7.loss_mask: 0.2765  d7.loss_dice: 2.5119  d8.loss_cls: 0.6997  d8.loss_mask: 0.2743  d8.loss_dice: 2.5079
05/08 02:57:46 - mmengine - INFO - Iter(train) [  350/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:53:48  time: 1.4407  data_time: 0.0565  memory: 29854  grad_norm: 40.1179  loss: 37.4162  loss_cls: 0.7017  loss_mask: 0.2590  loss_dice: 2.5930  d0.loss_cls: 1.0510  d0.loss_mask: 0.3057  d0.loss_dice: 2.9840  d1.loss_cls: 0.7948  d1.loss_mask: 0.2983  d1.loss_dice: 2.9378  d2.loss_cls: 0.7421  d2.loss_mask: 0.2829  d2.loss_dice: 2.8110  d3.loss_cls: 0.6998  d3.loss_mask: 0.2703  d3.loss_dice: 2.7016  d4.loss_cls: 0.6850  d4.loss_mask: 0.2679  d4.loss_dice: 2.6868  d5.loss_cls: 0.6897  d5.loss_mask: 0.2646  d5.loss_dice: 2.6703  d6.loss_cls: 0.6971  d6.loss_mask: 0.2619  d6.loss_dice: 2.6185  d7.loss_cls: 0.6941  d7.loss_mask: 0.2615  d7.loss_dice: 2.6166  d8.loss_cls: 0.6976  d8.loss_mask: 0.2609  d8.loss_dice: 2.6108
05/08 02:58:59 - mmengine - INFO - Iter(train) [  400/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:53:34  time: 1.4706  data_time: 0.0533  memory: 28318  grad_norm: 42.6950  loss: 34.2538  loss_cls: 0.6733  loss_mask: 0.2462  loss_dice: 2.3352  d0.loss_cls: 1.0214  d0.loss_mask: 0.2920  d0.loss_dice: 2.6738  d1.loss_cls: 0.7706  d1.loss_mask: 0.2785  d1.loss_dice: 2.6444  d2.loss_cls: 0.7104  d2.loss_mask: 0.2662  d2.loss_dice: 2.5342  d3.loss_cls: 0.6676  d3.loss_mask: 0.2603  d3.loss_dice: 2.4297  d4.loss_cls: 0.6586  d4.loss_mask: 0.2553  d4.loss_dice: 2.4129  d5.loss_cls: 0.6619  d5.loss_mask: 0.2510  d5.loss_dice: 2.3982  d6.loss_cls: 0.6707  d6.loss_mask: 0.2495  d6.loss_dice: 2.3551  d7.loss_cls: 0.6683  d7.loss_mask: 0.2484  d7.loss_dice: 2.3563  d8.loss_cls: 0.6695  d8.loss_mask: 0.2471  d8.loss_dice: 2.3473
05/08 02:59:05 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 02:59:05 - mmengine - WARNING - Reach the end of the dataloader, it will be restarted and continue to iterate. It is recommended to use `mmengine.dataset.InfiniteSampler` to enable the dataloader to iterate infinitely.
05/08 03:00:12 - mmengine - INFO - Iter(train) [  450/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:52:37  time: 1.4569  data_time: 0.1019  memory: 28237  grad_norm: 44.5306  loss: 34.4422  loss_cls: 0.6634  loss_mask: 0.2502  loss_dice: 2.3482  d0.loss_cls: 1.0244  d0.loss_mask: 0.2990  d0.loss_dice: 2.7061  d1.loss_cls: 0.7753  d1.loss_mask: 0.2893  d1.loss_dice: 2.6701  d2.loss_cls: 0.7060  d2.loss_mask: 0.2740  d2.loss_dice: 2.5650  d3.loss_cls: 0.6599  d3.loss_mask: 0.2658  d3.loss_dice: 2.4536  d4.loss_cls: 0.6517  d4.loss_mask: 0.2602  d4.loss_dice: 2.4334  d5.loss_cls: 0.6496  d5.loss_mask: 0.2588  d5.loss_dice: 2.4161  d6.loss_cls: 0.6549  d6.loss_mask: 0.2551  d6.loss_dice: 2.3690  d7.loss_cls: 0.6522  d7.loss_mask: 0.2542  d7.loss_dice: 2.3682  d8.loss_cls: 0.6594  d8.loss_mask: 0.2516  d8.loss_dice: 2.3577
05/08 03:01:24 - mmengine - INFO - Iter(train) [  500/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:50:41  time: 1.4281  data_time: 0.0600  memory: 28853  grad_norm: 56.0044  loss: 34.0246  loss_cls: 0.6771  loss_mask: 0.2429  loss_dice: 2.3095  d0.loss_cls: 1.0229  d0.loss_mask: 0.2907  d0.loss_dice: 2.6368  d1.loss_cls: 0.7905  d1.loss_mask: 0.2804  d1.loss_dice: 2.6167  d2.loss_cls: 0.7137  d2.loss_mask: 0.2653  d2.loss_dice: 2.5123  d3.loss_cls: 0.6681  d3.loss_mask: 0.2569  d3.loss_dice: 2.4105  d4.loss_cls: 0.6609  d4.loss_mask: 0.2515  d4.loss_dice: 2.3935  d5.loss_cls: 0.6696  d5.loss_mask: 0.2493  d5.loss_dice: 2.3746  d6.loss_cls: 0.6721  d6.loss_mask: 0.2457  d6.loss_dice: 2.3287  d7.loss_cls: 0.6700  d7.loss_mask: 0.2451  d7.loss_dice: 2.3283  d8.loss_cls: 0.6742  d8.loss_mask: 0.2441  d8.loss_dice: 2.3224
05/08 03:02:36 - mmengine - INFO - Iter(train) [  550/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:49:39  time: 1.4543  data_time: 0.0562  memory: 29576  grad_norm: 39.9831  loss: 35.4590  loss_cls: 0.6897  loss_mask: 0.2538  loss_dice: 2.4282  d0.loss_cls: 1.0439  d0.loss_mask: 0.3030  d0.loss_dice: 2.7603  d1.loss_cls: 0.7956  d1.loss_mask: 0.2932  d1.loss_dice: 2.7399  d2.loss_cls: 0.7285  d2.loss_mask: 0.2769  d2.loss_dice: 2.6336  d3.loss_cls: 0.6869  d3.loss_mask: 0.2674  d3.loss_dice: 2.5271  d4.loss_cls: 0.6792  d4.loss_mask: 0.2632  d4.loss_dice: 2.5067  d5.loss_cls: 0.6783  d5.loss_mask: 0.2599  d5.loss_dice: 2.4919  d6.loss_cls: 0.6847  d6.loss_mask: 0.2566  d6.loss_dice: 2.4453  d7.loss_cls: 0.6840  d7.loss_mask: 0.2561  d7.loss_dice: 2.4445  d8.loss_cls: 0.6856  d8.loss_mask: 0.2540  d8.loss_dice: 2.4413
05/08 03:03:49 - mmengine - INFO - Iter(train) [  600/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:48:22  time: 1.4461  data_time: 0.0538  memory: 29231  grad_norm: 56.4057  loss: 33.2743  loss_cls: 0.6719  loss_mask: 0.2466  loss_dice: 2.2338  d0.loss_cls: 1.0357  d0.loss_mask: 0.2960  d0.loss_dice: 2.5524  d1.loss_cls: 0.7709  d1.loss_mask: 0.2828  d1.loss_dice: 2.5468  d2.loss_cls: 0.7079  d2.loss_mask: 0.2709  d2.loss_dice: 2.4426  d3.loss_cls: 0.6657  d3.loss_mask: 0.2609  d3.loss_dice: 2.3314  d4.loss_cls: 0.6596  d4.loss_mask: 0.2567  d4.loss_dice: 2.3136  d5.loss_cls: 0.6629  d5.loss_mask: 0.2537  d5.loss_dice: 2.2997  d6.loss_cls: 0.6679  d6.loss_mask: 0.2503  d6.loss_dice: 2.2564  d7.loss_cls: 0.6647  d7.loss_mask: 0.2497  d7.loss_dice: 2.2568  d8.loss_cls: 0.6688  d8.loss_mask: 0.2478  d8.loss_dice: 2.2494
05/08 03:05:01 - mmengine - INFO - Iter(train) [  650/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:46:52  time: 1.4367  data_time: 0.0570  memory: 31222  grad_norm: 45.9783  loss: 34.8700  loss_cls: 0.6826  loss_mask: 0.2423  loss_dice: 2.4036  d0.loss_cls: 1.0344  d0.loss_mask: 0.2830  d0.loss_dice: 2.6897  d1.loss_cls: 0.7814  d1.loss_mask: 0.2749  d1.loss_dice: 2.6975  d2.loss_cls: 0.7181  d2.loss_mask: 0.2639  d2.loss_dice: 2.5942  d3.loss_cls: 0.6739  d3.loss_mask: 0.2546  d3.loss_dice: 2.4968  d4.loss_cls: 0.6702  d4.loss_mask: 0.2509  d4.loss_dice: 2.4765  d5.loss_cls: 0.6703  d5.loss_mask: 0.2479  d5.loss_dice: 2.4619  d6.loss_cls: 0.6761  d6.loss_mask: 0.2455  d6.loss_dice: 2.4138  d7.loss_cls: 0.6736  d7.loss_mask: 0.2447  d7.loss_dice: 2.4153  d8.loss_cls: 0.6784  d8.loss_mask: 0.2427  d8.loss_dice: 2.4114
05/08 03:06:13 - mmengine - INFO - Iter(train) [  700/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:45:55  time: 1.4585  data_time: 0.0601  memory: 29100  grad_norm: 44.6382  loss: 35.1703  loss_cls: 0.6813  loss_mask: 0.2436  loss_dice: 2.4106  d0.loss_cls: 1.0552  d0.loss_mask: 0.2890  d0.loss_dice: 2.7380  d1.loss_cls: 0.7930  d1.loss_mask: 0.2800  d1.loss_dice: 2.7415  d2.loss_cls: 0.7238  d2.loss_mask: 0.2650  d2.loss_dice: 2.6359  d3.loss_cls: 0.6755  d3.loss_mask: 0.2559  d3.loss_dice: 2.5179  d4.loss_cls: 0.6635  d4.loss_mask: 0.2529  d4.loss_dice: 2.5029  d5.loss_cls: 0.6693  d5.loss_mask: 0.2502  d5.loss_dice: 2.4793  d6.loss_cls: 0.6769  d6.loss_mask: 0.2471  d6.loss_dice: 2.4316  d7.loss_cls: 0.6721  d7.loss_mask: 0.2456  d7.loss_dice: 2.4312  d8.loss_cls: 0.6758  d8.loss_mask: 0.2440  d8.loss_dice: 2.4218
05/08 03:07:26 - mmengine - INFO - Iter(train) [  750/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:44:35  time: 1.4426  data_time: 0.0572  memory: 29303  grad_norm: 46.4550  loss: 35.2144  loss_cls: 0.6937  loss_mask: 0.2554  loss_dice: 2.3967  d0.loss_cls: 1.0391  d0.loss_mask: 0.3040  d0.loss_dice: 2.7308  d1.loss_cls: 0.7899  d1.loss_mask: 0.2936  d1.loss_dice: 2.7201  d2.loss_cls: 0.7246  d2.loss_mask: 0.2784  d2.loss_dice: 2.6159  d3.loss_cls: 0.6829  d3.loss_mask: 0.2706  d3.loss_dice: 2.4978  d4.loss_cls: 0.6778  d4.loss_mask: 0.2675  d4.loss_dice: 2.4835  d5.loss_cls: 0.6792  d5.loss_mask: 0.2627  d5.loss_dice: 2.4684  d6.loss_cls: 0.6879  d6.loss_mask: 0.2598  d6.loss_dice: 2.4173  d7.loss_cls: 0.6865  d7.loss_mask: 0.2589  d7.loss_dice: 2.4154  d8.loss_cls: 0.6887  d8.loss_mask: 0.2566  d8.loss_dice: 2.4107
05/08 03:08:38 - mmengine - INFO - Iter(train) [  800/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:43:30  time: 1.4540  data_time: 0.0531  memory: 28936  grad_norm: 39.0706  loss: 32.7198  loss_cls: 0.6639  loss_mask: 0.2282  loss_dice: 2.2206  d0.loss_cls: 1.0153  d0.loss_mask: 0.2722  d0.loss_dice: 2.4876  d1.loss_cls: 0.7675  d1.loss_mask: 0.2612  d1.loss_dice: 2.5149  d2.loss_cls: 0.6949  d2.loss_mask: 0.2482  d2.loss_dice: 2.4144  d3.loss_cls: 0.6524  d3.loss_mask: 0.2389  d3.loss_dice: 2.3137  d4.loss_cls: 0.6428  d4.loss_mask: 0.2373  d4.loss_dice: 2.3055  d5.loss_cls: 0.6474  d5.loss_mask: 0.2336  d5.loss_dice: 2.2837  d6.loss_cls: 0.6585  d6.loss_mask: 0.2305  d6.loss_dice: 2.2363  d7.loss_cls: 0.6567  d7.loss_mask: 0.2313  d7.loss_dice: 2.2399  d8.loss_cls: 0.6605  d8.loss_mask: 0.2290  d8.loss_dice: 2.2331
05/08 03:09:51 - mmengine - INFO - Iter(train) [  850/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:42:28  time: 1.4577  data_time: 0.1101  memory: 29160  grad_norm: 48.0586  loss: 35.3197  loss_cls: 0.6842  loss_mask: 0.2486  loss_dice: 2.4234  d0.loss_cls: 1.0393  d0.loss_mask: 0.2944  d0.loss_dice: 2.7369  d1.loss_cls: 0.7979  d1.loss_mask: 0.2844  d1.loss_dice: 2.7472  d2.loss_cls: 0.7200  d2.loss_mask: 0.2690  d2.loss_dice: 2.6436  d3.loss_cls: 0.6791  d3.loss_mask: 0.2600  d3.loss_dice: 2.5221  d4.loss_cls: 0.6678  d4.loss_mask: 0.2567  d4.loss_dice: 2.5122  d5.loss_cls: 0.6708  d5.loss_mask: 0.2536  d5.loss_dice: 2.4959  d6.loss_cls: 0.6766  d6.loss_mask: 0.2530  d6.loss_dice: 2.4483  d7.loss_cls: 0.6738  d7.loss_mask: 0.2518  d7.loss_dice: 2.4434  d8.loss_cls: 0.6795  d8.loss_mask: 0.2494  d8.loss_dice: 2.4368
05/08 03:11:03 - mmengine - INFO - Iter(train) [  900/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:40:59  time: 1.4335  data_time: 0.0626  memory: 28304  grad_norm: 47.9176  loss: 33.6513  loss_cls: 0.6673  loss_mask: 0.2335  loss_dice: 2.2984  d0.loss_cls: 1.0216  d0.loss_mask: 0.2800  d0.loss_dice: 2.5851  d1.loss_cls: 0.7745  d1.loss_mask: 0.2692  d1.loss_dice: 2.6033  d2.loss_cls: 0.7032  d2.loss_mask: 0.2560  d2.loss_dice: 2.5017  d3.loss_cls: 0.6612  d3.loss_mask: 0.2463  d3.loss_dice: 2.3886  d4.loss_cls: 0.6558  d4.loss_mask: 0.2437  d4.loss_dice: 2.3718  d5.loss_cls: 0.6561  d5.loss_mask: 0.2408  d5.loss_dice: 2.3640  d6.loss_cls: 0.6637  d6.loss_mask: 0.2359  d6.loss_dice: 2.3147  d7.loss_cls: 0.6598  d7.loss_mask: 0.2361  d7.loss_dice: 2.3131  d8.loss_cls: 0.6648  d8.loss_mask: 0.2341  d8.loss_dice: 2.3071
05/08 03:12:14 - mmengine - INFO - Iter(train) [  950/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:39:27  time: 1.4294  data_time: 0.0569  memory: 29477  grad_norm: 69.6243  loss: 34.8161  loss_cls: 0.6807  loss_mask: 0.2369  loss_dice: 2.3786  d0.loss_cls: 1.0482  d0.loss_mask: 0.2866  d0.loss_dice: 2.7123  d1.loss_cls: 0.7951  d1.loss_mask: 0.2766  d1.loss_dice: 2.7203  d2.loss_cls: 0.7173  d2.loss_mask: 0.2607  d2.loss_dice: 2.6100  d3.loss_cls: 0.6770  d3.loss_mask: 0.2497  d3.loss_dice: 2.4851  d4.loss_cls: 0.6685  d4.loss_mask: 0.2461  d4.loss_dice: 2.4717  d5.loss_cls: 0.6728  d5.loss_mask: 0.2435  d5.loss_dice: 2.4453  d6.loss_cls: 0.6788  d6.loss_mask: 0.2402  d6.loss_dice: 2.3986  d7.loss_cls: 0.6768  d7.loss_mask: 0.2400  d7.loss_dice: 2.3937  d8.loss_cls: 0.6768  d8.loss_mask: 0.2374  d8.loss_dice: 2.3909
05/08 03:13:26 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 03:13:26 - mmengine - INFO - Iter(train) [ 1000/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:38:10  time: 1.4421  data_time: 0.0565  memory: 29122  grad_norm: 42.4612  loss: 33.7261  loss_cls: 0.6629  loss_mask: 0.2445  loss_dice: 2.2933  d0.loss_cls: 1.0437  d0.loss_mask: 0.2908  d0.loss_dice: 2.5920  d1.loss_cls: 0.7751  d1.loss_mask: 0.2789  d1.loss_dice: 2.6013  d2.loss_cls: 0.6953  d2.loss_mask: 0.2665  d2.loss_dice: 2.4968  d3.loss_cls: 0.6607  d3.loss_mask: 0.2559  d3.loss_dice: 2.3886  d4.loss_cls: 0.6491  d4.loss_mask: 0.2530  d4.loss_dice: 2.3785  d5.loss_cls: 0.6512  d5.loss_mask: 0.2500  d5.loss_dice: 2.3567  d6.loss_cls: 0.6576  d6.loss_mask: 0.2483  d6.loss_dice: 2.3145  d7.loss_cls: 0.6562  d7.loss_mask: 0.2472  d7.loss_dice: 2.3106  d8.loss_cls: 0.6578  d8.loss_mask: 0.2451  d8.loss_dice: 2.3038
05/08 03:13:26 - mmengine - INFO - Saving checkpoint at 1000 iterations
05/08 03:14:18 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9759  data_time: 0.0348  memory: 3258  
05/08 03:14:41 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.34s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 03:14:48 - mmengine - INFO - start multi processing evaluation ...
DONE (t=56.42s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.335
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.653
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.295
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.184
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.411
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.806
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.466
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.867
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.438
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.308
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.551
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.817
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.466
05/08 03:15:45 - mmengine - INFO - segm_mAP_copypaste: 0.335 0.653 0.295 0.184 0.411 0.806
05/08 03:15:46 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.3350  coco/segm_mAP_50: 0.6530  coco/segm_mAP_75: 0.2950  coco/segm_mAP_s: 0.1840  coco/segm_mAP_m: 0.4110  coco/segm_mAP_l: 0.8060  data_time: 0.0348  time: 0.9759
05/08 03:15:47 - mmengine - INFO - The best checkpoint with 0.6530 coco/segm_mAP_50 at 1000 iter is saved to best_coco_segm_mAP_50_iter_1000.pth.
05/08 03:17:02 - mmengine - INFO - Iter(train) [ 1050/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:04:37  time: 3.2853  data_time: 1.9037  memory: 29313  grad_norm: 46.8246  loss: 35.1636  loss_cls: 0.6744  loss_mask: 0.2468  loss_dice: 2.4099  d0.loss_cls: 1.0579  d0.loss_mask: 0.2953  d0.loss_dice: 2.7377  d1.loss_cls: 0.7985  d1.loss_mask: 0.2833  d1.loss_dice: 2.7453  d2.loss_cls: 0.7205  d2.loss_mask: 0.2705  d2.loss_dice: 2.6276  d3.loss_cls: 0.6725  d3.loss_mask: 0.2610  d3.loss_dice: 2.5132  d4.loss_cls: 0.6621  d4.loss_mask: 0.2561  d4.loss_dice: 2.4964  d5.loss_cls: 0.6688  d5.loss_mask: 0.2524  d5.loss_dice: 2.4728  d6.loss_cls: 0.6732  d6.loss_mask: 0.2496  d6.loss_dice: 2.4315  d7.loss_cls: 0.6713  d7.loss_mask: 0.2491  d7.loss_dice: 2.4281  d8.loss_cls: 0.6720  d8.loss_mask: 0.2473  d8.loss_dice: 2.4185
05/08 03:18:13 - mmengine - INFO - Iter(train) [ 1100/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 8:01:45  time: 1.4238  data_time: 0.0582  memory: 29604  grad_norm: 49.9485  loss: 32.9030  loss_cls: 0.6628  loss_mask: 0.2316  loss_dice: 2.2330  d0.loss_cls: 1.0232  d0.loss_mask: 0.2767  d0.loss_dice: 2.4877  d1.loss_cls: 0.7721  d1.loss_mask: 0.2632  d1.loss_dice: 2.5214  d2.loss_cls: 0.7004  d2.loss_mask: 0.2532  d2.loss_dice: 2.4275  d3.loss_cls: 0.6579  d3.loss_mask: 0.2434  d3.loss_dice: 2.3337  d4.loss_cls: 0.6515  d4.loss_mask: 0.2394  d4.loss_dice: 2.3162  d5.loss_cls: 0.6516  d5.loss_mask: 0.2369  d5.loss_dice: 2.2968  d6.loss_cls: 0.6601  d6.loss_mask: 0.2334  d6.loss_dice: 2.2513  d7.loss_cls: 0.6561  d7.loss_mask: 0.2334  d7.loss_dice: 2.2506  d8.loss_cls: 0.6568  d8.loss_mask: 0.2320  d8.loss_dice: 2.2490
05/08 03:19:25 - mmengine - INFO - Iter(train) [ 1150/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:59:06  time: 1.4287  data_time: 0.0569  memory: 28782  grad_norm: 44.3837  loss: 33.9442  loss_cls: 0.6660  loss_mask: 0.2398  loss_dice: 2.3164  d0.loss_cls: 1.0421  d0.loss_mask: 0.2871  d0.loss_dice: 2.6054  d1.loss_cls: 0.7836  d1.loss_mask: 0.2745  d1.loss_dice: 2.6340  d2.loss_cls: 0.7083  d2.loss_mask: 0.2628  d2.loss_dice: 2.5240  d3.loss_cls: 0.6591  d3.loss_mask: 0.2529  d3.loss_dice: 2.4135  d4.loss_cls: 0.6532  d4.loss_mask: 0.2497  d4.loss_dice: 2.4004  d5.loss_cls: 0.6550  d5.loss_mask: 0.2455  d5.loss_dice: 2.3814  d6.loss_cls: 0.6589  d6.loss_mask: 0.2434  d6.loss_dice: 2.3338  d7.loss_cls: 0.6561  d7.loss_mask: 0.2418  d7.loss_dice: 2.3319  d8.loss_cls: 0.6581  d8.loss_mask: 0.2404  d8.loss_dice: 2.3253
05/08 03:20:38 - mmengine - INFO - Iter(train) [ 1200/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:56:54  time: 1.4542  data_time: 0.0606  memory: 30993  grad_norm: 80.2013  loss: 34.9589  loss_cls: 0.6781  loss_mask: 0.2364  loss_dice: 2.4081  d0.loss_cls: 1.0452  d0.loss_mask: 0.2863  d0.loss_dice: 2.6981  d1.loss_cls: 0.7970  d1.loss_mask: 0.2760  d1.loss_dice: 2.7248  d2.loss_cls: 0.7195  d2.loss_mask: 0.2590  d2.loss_dice: 2.6177  d3.loss_cls: 0.6701  d3.loss_mask: 0.2497  d3.loss_dice: 2.5120  d4.loss_cls: 0.6597  d4.loss_mask: 0.2467  d4.loss_dice: 2.4924  d5.loss_cls: 0.6628  d5.loss_mask: 0.2425  d5.loss_dice: 2.4736  d6.loss_cls: 0.6684  d6.loss_mask: 0.2389  d6.loss_dice: 2.4307  d7.loss_cls: 0.6675  d7.loss_mask: 0.2382  d7.loss_dice: 2.4287  d8.loss_cls: 0.6708  d8.loss_mask: 0.2373  d8.loss_dice: 2.4225
05/08 03:21:51 - mmengine - INFO - Iter(train) [ 1250/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:55:03  time: 1.4746  data_time: 0.1028  memory: 28717  grad_norm: 43.4675  loss: 32.9736  loss_cls: 0.6650  loss_mask: 0.2259  loss_dice: 2.2400  d0.loss_cls: 1.0374  d0.loss_mask: 0.2759  d0.loss_dice: 2.4997  d1.loss_cls: 0.7849  d1.loss_mask: 0.2598  d1.loss_dice: 2.5371  d2.loss_cls: 0.7010  d2.loss_mask: 0.2471  d2.loss_dice: 2.4399  d3.loss_cls: 0.6550  d3.loss_mask: 0.2365  d3.loss_dice: 2.3423  d4.loss_cls: 0.6477  d4.loss_mask: 0.2348  d4.loss_dice: 2.3257  d5.loss_cls: 0.6485  d5.loss_mask: 0.2321  d5.loss_dice: 2.3079  d6.loss_cls: 0.6581  d6.loss_mask: 0.2284  d6.loss_dice: 2.2601  d7.loss_cls: 0.6554  d7.loss_mask: 0.2280  d7.loss_dice: 2.2591  d8.loss_cls: 0.6590  d8.loss_mask: 0.2260  d8.loss_dice: 2.2555
05/08 03:23:02 - mmengine - INFO - Iter(train) [ 1300/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:52:34  time: 1.4196  data_time: 0.0564  memory: 27875  grad_norm: 45.2273  loss: 32.0705  loss_cls: 0.6385  loss_mask: 0.2267  loss_dice: 2.1704  d0.loss_cls: 1.0087  d0.loss_mask: 0.2760  d0.loss_dice: 2.4389  d1.loss_cls: 0.7617  d1.loss_mask: 0.2617  d1.loss_dice: 2.4834  d2.loss_cls: 0.6799  d2.loss_mask: 0.2468  d2.loss_dice: 2.3750  d3.loss_cls: 0.6360  d3.loss_mask: 0.2389  d3.loss_dice: 2.2685  d4.loss_cls: 0.6248  d4.loss_mask: 0.2362  d4.loss_dice: 2.2551  d5.loss_cls: 0.6278  d5.loss_mask: 0.2319  d5.loss_dice: 2.2337  d6.loss_cls: 0.6351  d6.loss_mask: 0.2301  d6.loss_dice: 2.1891  d7.loss_cls: 0.6321  d7.loss_mask: 0.2292  d7.loss_dice: 2.1874  d8.loss_cls: 0.6351  d8.loss_mask: 0.2273  d8.loss_dice: 2.1848
05/08 03:24:15 - mmengine - INFO - Iter(train) [ 1350/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:50:39  time: 1.4584  data_time: 0.0514  memory: 29663  grad_norm: 56.3276  loss: 31.9181  loss_cls: 0.6617  loss_mask: 0.2309  loss_dice: 2.1288  d0.loss_cls: 1.0214  d0.loss_mask: 0.2783  d0.loss_dice: 2.4147  d1.loss_cls: 0.7863  d1.loss_mask: 0.2651  d1.loss_dice: 2.4305  d2.loss_cls: 0.7014  d2.loss_mask: 0.2522  d2.loss_dice: 2.3301  d3.loss_cls: 0.6572  d3.loss_mask: 0.2435  d3.loss_dice: 2.2234  d4.loss_cls: 0.6482  d4.loss_mask: 0.2411  d4.loss_dice: 2.2099  d5.loss_cls: 0.6524  d5.loss_mask: 0.2370  d5.loss_dice: 2.1936  d6.loss_cls: 0.6608  d6.loss_mask: 0.2344  d6.loss_dice: 2.1475  d7.loss_cls: 0.6543  d7.loss_mask: 0.2337  d7.loss_dice: 2.1493  d8.loss_cls: 0.6581  d8.loss_mask: 0.2316  d8.loss_dice: 2.1406
05/08 03:25:27 - mmengine - INFO - Iter(train) [ 1400/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:48:34  time: 1.4401  data_time: 0.0582  memory: 29052  grad_norm: 46.1572  loss: 32.8205  loss_cls: 0.6536  loss_mask: 0.2285  loss_dice: 2.2388  d0.loss_cls: 1.0260  d0.loss_mask: 0.2760  d0.loss_dice: 2.4780  d1.loss_cls: 0.7736  d1.loss_mask: 0.2643  d1.loss_dice: 2.5279  d2.loss_cls: 0.6882  d2.loss_mask: 0.2521  d2.loss_dice: 2.4399  d3.loss_cls: 0.6456  d3.loss_mask: 0.2408  d3.loss_dice: 2.3331  d4.loss_cls: 0.6358  d4.loss_mask: 0.2371  d4.loss_dice: 2.3199  d5.loss_cls: 0.6400  d5.loss_mask: 0.2337  d5.loss_dice: 2.2986  d6.loss_cls: 0.6445  d6.loss_mask: 0.2312  d6.loss_dice: 2.2558  d7.loss_cls: 0.6465  d7.loss_mask: 0.2312  d7.loss_dice: 2.2527  d8.loss_cls: 0.6466  d8.loss_mask: 0.2289  d8.loss_dice: 2.2513
05/08 03:26:40 - mmengine - INFO - Iter(train) [ 1450/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:46:42  time: 1.4550  data_time: 0.0570  memory: 29560  grad_norm: 50.4547  loss: 34.7717  loss_cls: 0.6727  loss_mask: 0.2401  loss_dice: 2.3871  d0.loss_cls: 1.0529  d0.loss_mask: 0.2909  d0.loss_dice: 2.6738  d1.loss_cls: 0.7861  d1.loss_mask: 0.2775  d1.loss_dice: 2.7166  d2.loss_cls: 0.7062  d2.loss_mask: 0.2663  d2.loss_dice: 2.6033  d3.loss_cls: 0.6636  d3.loss_mask: 0.2552  d3.loss_dice: 2.4902  d4.loss_cls: 0.6568  d4.loss_mask: 0.2506  d4.loss_dice: 2.4770  d5.loss_cls: 0.6594  d5.loss_mask: 0.2470  d5.loss_dice: 2.4578  d6.loss_cls: 0.6680  d6.loss_mask: 0.2438  d6.loss_dice: 2.4076  d7.loss_cls: 0.6639  d7.loss_mask: 0.2433  d7.loss_dice: 2.4035  d8.loss_cls: 0.6673  d8.loss_mask: 0.2410  d8.loss_dice: 2.4020
05/08 03:27:53 - mmengine - INFO - Iter(train) [ 1500/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:44:51  time: 1.4521  data_time: 0.0445  memory: 28538  grad_norm: 54.8460  loss: 34.4949  loss_cls: 0.6715  loss_mask: 0.2267  loss_dice: 2.3773  d0.loss_cls: 1.0384  d0.loss_mask: 0.2713  d0.loss_dice: 2.6390  d1.loss_cls: 0.8023  d1.loss_mask: 0.2614  d1.loss_dice: 2.6945  d2.loss_cls: 0.7118  d2.loss_mask: 0.2499  d2.loss_dice: 2.5947  d3.loss_cls: 0.6629  d3.loss_mask: 0.2406  d3.loss_dice: 2.4802  d4.loss_cls: 0.6548  d4.loss_mask: 0.2354  d4.loss_dice: 2.4680  d5.loss_cls: 0.6621  d5.loss_mask: 0.2329  d5.loss_dice: 2.4458  d6.loss_cls: 0.6649  d6.loss_mask: 0.2305  d6.loss_dice: 2.3985  d7.loss_cls: 0.6643  d7.loss_mask: 0.2289  d7.loss_dice: 2.3984  d8.loss_cls: 0.6671  d8.loss_mask: 0.2280  d8.loss_dice: 2.3929
05/08 03:29:04 - mmengine - INFO - Iter(train) [ 1550/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:42:48  time: 1.4272  data_time: 0.0453  memory: 29155  grad_norm: 43.5723  loss: 33.9510  loss_cls: 0.6748  loss_mask: 0.2341  loss_dice: 2.3182  d0.loss_cls: 1.0417  d0.loss_mask: 0.2823  d0.loss_dice: 2.5852  d1.loss_cls: 0.7980  d1.loss_mask: 0.2668  d1.loss_dice: 2.6269  d2.loss_cls: 0.7110  d2.loss_mask: 0.2550  d2.loss_dice: 2.5177  d3.loss_cls: 0.6692  d3.loss_mask: 0.2467  d3.loss_dice: 2.4167  d4.loss_cls: 0.6629  d4.loss_mask: 0.2429  d4.loss_dice: 2.4015  d5.loss_cls: 0.6607  d5.loss_mask: 0.2404  d5.loss_dice: 2.3828  d6.loss_cls: 0.6668  d6.loss_mask: 0.2377  d6.loss_dice: 2.3364  d7.loss_cls: 0.6653  d7.loss_mask: 0.2360  d7.loss_dice: 2.3365  d8.loss_cls: 0.6676  d8.loss_mask: 0.2353  d8.loss_dice: 2.3338
05/08 03:30:16 - mmengine - INFO - Iter(train) [ 1600/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:40:56  time: 1.4409  data_time: 0.0486  memory: 29145  grad_norm: 46.3063  loss: 33.4328  loss_cls: 0.6622  loss_mask: 0.2311  loss_dice: 2.2691  d0.loss_cls: 1.0438  d0.loss_mask: 0.2818  d0.loss_dice: 2.5671  d1.loss_cls: 0.7922  d1.loss_mask: 0.2685  d1.loss_dice: 2.6011  d2.loss_cls: 0.7003  d2.loss_mask: 0.2545  d2.loss_dice: 2.4903  d3.loss_cls: 0.6588  d3.loss_mask: 0.2434  d3.loss_dice: 2.3677  d4.loss_cls: 0.6485  d4.loss_mask: 0.2409  d4.loss_dice: 2.3559  d5.loss_cls: 0.6483  d5.loss_mask: 0.2366  d5.loss_dice: 2.3384  d6.loss_cls: 0.6566  d6.loss_mask: 0.2345  d6.loss_dice: 2.2895  d7.loss_cls: 0.6556  d7.loss_mask: 0.2336  d7.loss_dice: 2.2888  d8.loss_cls: 0.6586  d8.loss_mask: 0.2316  d8.loss_dice: 2.2835
05/08 03:31:29 - mmengine - INFO - Iter(train) [ 1650/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:39:20  time: 1.4656  data_time: 0.1078  memory: 29208  grad_norm: 54.8670  loss: 33.2676  loss_cls: 0.6600  loss_mask: 0.2114  loss_dice: 2.2749  d0.loss_cls: 1.0274  d0.loss_mask: 0.2602  d0.loss_dice: 2.5705  d1.loss_cls: 0.7874  d1.loss_mask: 0.2516  d1.loss_dice: 2.6081  d2.loss_cls: 0.7029  d2.loss_mask: 0.2360  d2.loss_dice: 2.4929  d3.loss_cls: 0.6545  d3.loss_mask: 0.2269  d3.loss_dice: 2.3756  d4.loss_cls: 0.6481  d4.loss_mask: 0.2228  d4.loss_dice: 2.3610  d5.loss_cls: 0.6497  d5.loss_mask: 0.2193  d5.loss_dice: 2.3450  d6.loss_cls: 0.6560  d6.loss_mask: 0.2145  d6.loss_dice: 2.2935  d7.loss_cls: 0.6562  d7.loss_mask: 0.2138  d7.loss_dice: 2.2928  d8.loss_cls: 0.6553  d8.loss_mask: 0.2122  d8.loss_dice: 2.2869
05/08 03:32:41 - mmengine - INFO - Iter(train) [ 1700/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:37:27  time: 1.4331  data_time: 0.0533  memory: 29294  grad_norm: 61.2581  loss: 33.5672  loss_cls: 0.6700  loss_mask: 0.2323  loss_dice: 2.2815  d0.loss_cls: 1.0310  d0.loss_mask: 0.2820  d0.loss_dice: 2.5512  d1.loss_cls: 0.7858  d1.loss_mask: 0.2697  d1.loss_dice: 2.5962  d2.loss_cls: 0.7073  d2.loss_mask: 0.2573  d2.loss_dice: 2.4926  d3.loss_cls: 0.6684  d3.loss_mask: 0.2468  d3.loss_dice: 2.3818  d4.loss_cls: 0.6586  d4.loss_mask: 0.2423  d4.loss_dice: 2.3666  d5.loss_cls: 0.6613  d5.loss_mask: 0.2389  d5.loss_dice: 2.3497  d6.loss_cls: 0.6655  d6.loss_mask: 0.2351  d6.loss_dice: 2.3007  d7.loss_cls: 0.6619  d7.loss_mask: 0.2341  d7.loss_dice: 2.3038  d8.loss_cls: 0.6645  d8.loss_mask: 0.2332  d8.loss_dice: 2.2972
05/08 03:33:53 - mmengine - INFO - Iter(train) [ 1750/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:35:42  time: 1.4416  data_time: 0.0585  memory: 28558  grad_norm: 53.1978  loss: 33.3334  loss_cls: 0.6595  loss_mask: 0.2284  loss_dice: 2.2704  d0.loss_cls: 1.0284  d0.loss_mask: 0.2789  d0.loss_dice: 2.5562  d1.loss_cls: 0.7891  d1.loss_mask: 0.2671  d1.loss_dice: 2.5921  d2.loss_cls: 0.6964  d2.loss_mask: 0.2500  d2.loss_dice: 2.4861  d3.loss_cls: 0.6555  d3.loss_mask: 0.2399  d3.loss_dice: 2.3684  d4.loss_cls: 0.6472  d4.loss_mask: 0.2378  d4.loss_dice: 2.3529  d5.loss_cls: 0.6477  d5.loss_mask: 0.2353  d5.loss_dice: 2.3377  d6.loss_cls: 0.6532  d6.loss_mask: 0.2316  d6.loss_dice: 2.2886  d7.loss_cls: 0.6528  d7.loss_mask: 0.2309  d7.loss_dice: 2.2863  d8.loss_cls: 0.6506  d8.loss_mask: 0.2297  d8.loss_dice: 2.2847
05/08 03:35:04 - mmengine - INFO - Iter(train) [ 1800/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:33:53  time: 1.4304  data_time: 0.0519  memory: 28533  grad_norm: 53.7579  loss: 33.4426  loss_cls: 0.6527  loss_mask: 0.2168  loss_dice: 2.2978  d0.loss_cls: 1.0336  d0.loss_mask: 0.2664  d0.loss_dice: 2.5776  d1.loss_cls: 0.7676  d1.loss_mask: 0.2558  d1.loss_dice: 2.6192  d2.loss_cls: 0.6882  d2.loss_mask: 0.2402  d2.loss_dice: 2.5164  d3.loss_cls: 0.6472  d3.loss_mask: 0.2299  d3.loss_dice: 2.3994  d4.loss_cls: 0.6403  d4.loss_mask: 0.2265  d4.loss_dice: 2.3861  d5.loss_cls: 0.6441  d5.loss_mask: 0.2225  d5.loss_dice: 2.3656  d6.loss_cls: 0.6496  d6.loss_mask: 0.2199  d6.loss_dice: 2.3208  d7.loss_cls: 0.6468  d7.loss_mask: 0.2190  d7.loss_dice: 2.3145  d8.loss_cls: 0.6491  d8.loss_mask: 0.2172  d8.loss_dice: 2.3118
05/08 03:36:17 - mmengine - INFO - Iter(train) [ 1850/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:32:14  time: 1.4481  data_time: 0.0519  memory: 28763  grad_norm: 42.6605  loss: 31.7207  loss_cls: 0.6557  loss_mask: 0.2115  loss_dice: 2.1427  d0.loss_cls: 1.0123  d0.loss_mask: 0.2616  d0.loss_dice: 2.3986  d1.loss_cls: 0.7670  d1.loss_mask: 0.2468  d1.loss_dice: 2.4399  d2.loss_cls: 0.6885  d2.loss_mask: 0.2338  d2.loss_dice: 2.3444  d3.loss_cls: 0.6526  d3.loss_mask: 0.2234  d3.loss_dice: 2.2324  d4.loss_cls: 0.6447  d4.loss_mask: 0.2203  d4.loss_dice: 2.2199  d5.loss_cls: 0.6442  d5.loss_mask: 0.2171  d5.loss_dice: 2.2029  d6.loss_cls: 0.6511  d6.loss_mask: 0.2143  d6.loss_dice: 2.1583  d7.loss_cls: 0.6486  d7.loss_mask: 0.2132  d7.loss_dice: 2.1579  d8.loss_cls: 0.6520  d8.loss_mask: 0.2118  d8.loss_dice: 2.1530
05/08 03:37:30 - mmengine - INFO - Iter(train) [ 1900/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:30:43  time: 1.4602  data_time: 0.0581  memory: 30277  grad_norm: 50.0522  loss: 34.5789  loss_cls: 0.6696  loss_mask: 0.2319  loss_dice: 2.3757  d0.loss_cls: 1.0657  d0.loss_mask: 0.2822  d0.loss_dice: 2.6690  d1.loss_cls: 0.7788  d1.loss_mask: 0.2698  d1.loss_dice: 2.7104  d2.loss_cls: 0.7053  d2.loss_mask: 0.2551  d2.loss_dice: 2.5997  d3.loss_cls: 0.6635  d3.loss_mask: 0.2461  d3.loss_dice: 2.4779  d4.loss_cls: 0.6565  d4.loss_mask: 0.2428  d4.loss_dice: 2.4687  d5.loss_cls: 0.6607  d5.loss_mask: 0.2367  d5.loss_dice: 2.4423  d6.loss_cls: 0.6646  d6.loss_mask: 0.2362  d6.loss_dice: 2.3946  d7.loss_cls: 0.6598  d7.loss_mask: 0.2343  d7.loss_dice: 2.3963  d8.loss_cls: 0.6639  d8.loss_mask: 0.2330  d8.loss_dice: 2.3878
05/08 03:38:42 - mmengine - INFO - Iter(train) [ 1950/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:29:07  time: 1.4478  data_time: 0.0536  memory: 29281  grad_norm: 53.3290  loss: 32.6399  loss_cls: 0.6660  loss_mask: 0.2193  loss_dice: 2.2004  d0.loss_cls: 1.0398  d0.loss_mask: 0.2694  d0.loss_dice: 2.4948  d1.loss_cls: 0.7794  d1.loss_mask: 0.2543  d1.loss_dice: 2.5406  d2.loss_cls: 0.6999  d2.loss_mask: 0.2409  d2.loss_dice: 2.4294  d3.loss_cls: 0.6519  d3.loss_mask: 0.2329  d3.loss_dice: 2.3049  d4.loss_cls: 0.6441  d4.loss_mask: 0.2311  d4.loss_dice: 2.2939  d5.loss_cls: 0.6503  d5.loss_mask: 0.2254  d5.loss_dice: 2.2749  d6.loss_cls: 0.6594  d6.loss_mask: 0.2226  d6.loss_dice: 2.2219  d7.loss_cls: 0.6554  d7.loss_mask: 0.2220  d7.loss_dice: 2.2204  d8.loss_cls: 0.6579  d8.loss_mask: 0.2203  d8.loss_dice: 2.2164
05/08 03:39:54 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 03:39:54 - mmengine - INFO - Iter(train) [ 2000/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:27:30  time: 1.4435  data_time: 0.0527  memory: 28536  grad_norm: 57.6487  loss: 32.2897  loss_cls: 0.6733  loss_mask: 0.2219  loss_dice: 2.1665  d0.loss_cls: 1.0261  d0.loss_mask: 0.2740  d0.loss_dice: 2.4422  d1.loss_cls: 0.7727  d1.loss_mask: 0.2601  d1.loss_dice: 2.4780  d2.loss_cls: 0.7024  d2.loss_mask: 0.2435  d2.loss_dice: 2.3736  d3.loss_cls: 0.6696  d3.loss_mask: 0.2340  d3.loss_dice: 2.2608  d4.loss_cls: 0.6646  d4.loss_mask: 0.2296  d4.loss_dice: 2.2492  d5.loss_cls: 0.6641  d5.loss_mask: 0.2273  d5.loss_dice: 2.2313  d6.loss_cls: 0.6701  d6.loss_mask: 0.2248  d6.loss_dice: 2.1845  d7.loss_cls: 0.6685  d7.loss_mask: 0.2239  d7.loss_dice: 2.1818  d8.loss_cls: 0.6678  d8.loss_mask: 0.2235  d8.loss_dice: 2.1801
05/08 03:39:54 - mmengine - INFO - Saving checkpoint at 2000 iterations
05/08 03:40:47 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9887  data_time: 0.0402  memory: 3258  
05/08 03:41:10 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.46s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 03:41:17 - mmengine - INFO - start multi processing evaluation ...
DONE (t=52.86s).
Accumulating evaluation results...
DONE (t=0.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.360
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.686
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.326
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.201
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.431
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.472
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.836
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.445
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.322
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.545
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.883
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.472
05/08 03:42:10 - mmengine - INFO - segm_mAP_copypaste: 0.360 0.686 0.326 0.201 0.431 0.808
05/08 03:42:11 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.3600  coco/segm_mAP_50: 0.6860  coco/segm_mAP_75: 0.3260  coco/segm_mAP_s: 0.2010  coco/segm_mAP_m: 0.4310  coco/segm_mAP_l: 0.8080  data_time: 0.0399  time: 0.9864
05/08 03:42:11 - mmengine - INFO - The previous best checkpoint /data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco/best_coco_segm_mAP_50_iter_1000.pth is removed
05/08 03:42:12 - mmengine - INFO - The best checkpoint with 0.6860 coco/segm_mAP_50 at 2000 iter is saved to best_coco_segm_mAP_50_iter_2000.pth.
05/08 03:43:28 - mmengine - INFO - Iter(train) [ 2050/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:38:58  time: 3.2338  data_time: 1.8798  memory: 29701  grad_norm: 52.4380  loss: 33.1274  loss_cls: 0.6511  loss_mask: 0.2064  loss_dice: 2.2846  d0.loss_cls: 1.0320  d0.loss_mask: 0.2516  d0.loss_dice: 2.5319  d1.loss_cls: 0.7704  d1.loss_mask: 0.2412  d1.loss_dice: 2.5950  d2.loss_cls: 0.6961  d2.loss_mask: 0.2246  d2.loss_dice: 2.4989  d3.loss_cls: 0.6510  d3.loss_mask: 0.2169  d3.loss_dice: 2.3805  d4.loss_cls: 0.6377  d4.loss_mask: 0.2149  d4.loss_dice: 2.3680  d5.loss_cls: 0.6429  d5.loss_mask: 0.2118  d5.loss_dice: 2.3507  d6.loss_cls: 0.6456  d6.loss_mask: 0.2093  d6.loss_dice: 2.3052  d7.loss_cls: 0.6435  d7.loss_mask: 0.2088  d7.loss_dice: 2.3028  d8.loss_cls: 0.6436  d8.loss_mask: 0.2069  d8.loss_dice: 2.3036
05/08 03:44:40 - mmengine - INFO - Iter(train) [ 2100/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:37:03  time: 1.4432  data_time: 0.0598  memory: 29125  grad_norm: 41.7250  loss: 31.2405  loss_cls: 0.6401  loss_mask: 0.2147  loss_dice: 2.1034  d0.loss_cls: 1.0193  d0.loss_mask: 0.2666  d0.loss_dice: 2.3521  d1.loss_cls: 0.7495  d1.loss_mask: 0.2477  d1.loss_dice: 2.4116  d2.loss_cls: 0.6742  d2.loss_mask: 0.2361  d2.loss_dice: 2.3106  d3.loss_cls: 0.6286  d3.loss_mask: 0.2271  d3.loss_dice: 2.2028  d4.loss_cls: 0.6210  d4.loss_mask: 0.2252  d4.loss_dice: 2.1903  d5.loss_cls: 0.6246  d5.loss_mask: 0.2212  d5.loss_dice: 2.1668  d6.loss_cls: 0.6292  d6.loss_mask: 0.2189  d6.loss_dice: 2.1239  d7.loss_cls: 0.6322  d7.loss_mask: 0.2176  d7.loss_dice: 2.1217  d8.loss_cls: 0.6320  d8.loss_mask: 0.2154  d8.loss_dice: 2.1161
05/08 03:45:53 - mmengine - INFO - Iter(train) [ 2150/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:35:11  time: 1.4474  data_time: 0.0566  memory: 29922  grad_norm: 50.0285  loss: 33.0359  loss_cls: 0.6766  loss_mask: 0.2238  loss_dice: 2.2251  d0.loss_cls: 1.0524  d0.loss_mask: 0.2728  d0.loss_dice: 2.5031  d1.loss_cls: 0.8068  d1.loss_mask: 0.2594  d1.loss_dice: 2.5479  d2.loss_cls: 0.7192  d2.loss_mask: 0.2481  d2.loss_dice: 2.4401  d3.loss_cls: 0.6714  d3.loss_mask: 0.2382  d3.loss_dice: 2.3318  d4.loss_cls: 0.6630  d4.loss_mask: 0.2344  d4.loss_dice: 2.3149  d5.loss_cls: 0.6672  d5.loss_mask: 0.2314  d5.loss_dice: 2.2929  d6.loss_cls: 0.6716  d6.loss_mask: 0.2283  d6.loss_dice: 2.2463  d7.loss_cls: 0.6664  d7.loss_mask: 0.2271  d7.loss_dice: 2.2424  d8.loss_cls: 0.6705  d8.loss_mask: 0.2254  d8.loss_dice: 2.2374
05/08 03:47:04 - mmengine - INFO - Iter(train) [ 2200/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:33:10  time: 1.4192  data_time: 0.0605  memory: 29047  grad_norm: 53.9318  loss: 34.3695  loss_cls: 0.6598  loss_mask: 0.2330  loss_dice: 2.3577  d0.loss_cls: 1.0371  d0.loss_mask: 0.2901  d0.loss_dice: 2.6468  d1.loss_cls: 0.7940  d1.loss_mask: 0.2728  d1.loss_dice: 2.6955  d2.loss_cls: 0.7119  d2.loss_mask: 0.2573  d2.loss_dice: 2.5832  d3.loss_cls: 0.6548  d3.loss_mask: 0.2479  d3.loss_dice: 2.4696  d4.loss_cls: 0.6449  d4.loss_mask: 0.2433  d4.loss_dice: 2.4555  d5.loss_cls: 0.6417  d5.loss_mask: 0.2402  d5.loss_dice: 2.4360  d6.loss_cls: 0.6527  d6.loss_mask: 0.2364  d6.loss_dice: 2.3799  d7.loss_cls: 0.6504  d7.loss_mask: 0.2360  d7.loss_dice: 2.3791  d8.loss_cls: 0.6537  d8.loss_mask: 0.2338  d8.loss_dice: 2.3741
05/08 03:48:15 - mmengine - INFO - Iter(train) [ 2250/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:31:12  time: 1.4220  data_time: 0.0555  memory: 28712  grad_norm: 69.5421  loss: 31.9272  loss_cls: 0.6495  loss_mask: 0.2099  loss_dice: 2.1670  d0.loss_cls: 1.0087  d0.loss_mask: 0.2533  d0.loss_dice: 2.4106  d1.loss_cls: 0.7793  d1.loss_mask: 0.2439  d1.loss_dice: 2.4613  d2.loss_cls: 0.7006  d2.loss_mask: 0.2309  d2.loss_dice: 2.3705  d3.loss_cls: 0.6513  d3.loss_mask: 0.2226  d3.loss_dice: 2.2606  d4.loss_cls: 0.6373  d4.loss_mask: 0.2189  d4.loss_dice: 2.2501  d5.loss_cls: 0.6391  d5.loss_mask: 0.2153  d5.loss_dice: 2.2292  d6.loss_cls: 0.6448  d6.loss_mask: 0.2133  d6.loss_dice: 2.1871  d7.loss_cls: 0.6428  d7.loss_mask: 0.2113  d7.loss_dice: 2.1848  d8.loss_cls: 0.6434  d8.loss_mask: 0.2100  d8.loss_dice: 2.1796
05/08 03:49:25 - mmengine - INFO - Iter(train) [ 2300/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:29:11  time: 1.4099  data_time: 0.0611  memory: 28520  grad_norm: 59.8122  loss: 31.2830  loss_cls: 0.6364  loss_mask: 0.1972  loss_dice: 2.1263  d0.loss_cls: 1.0160  d0.loss_mask: 0.2410  d0.loss_dice: 2.3602  d1.loss_cls: 0.7774  d1.loss_mask: 0.2290  d1.loss_dice: 2.4146  d2.loss_cls: 0.6902  d2.loss_mask: 0.2168  d2.loss_dice: 2.3274  d3.loss_cls: 0.6404  d3.loss_mask: 0.2062  d3.loss_dice: 2.2216  d4.loss_cls: 0.6268  d4.loss_mask: 0.2046  d4.loss_dice: 2.2069  d5.loss_cls: 0.6274  d5.loss_mask: 0.2003  d5.loss_dice: 2.1939  d6.loss_cls: 0.6331  d6.loss_mask: 0.1999  d6.loss_dice: 2.1451  d7.loss_cls: 0.6307  d7.loss_mask: 0.1985  d7.loss_dice: 2.1433  d8.loss_cls: 0.6335  d8.loss_mask: 0.1977  d8.loss_dice: 2.1403
05/08 03:50:38 - mmengine - INFO - Iter(train) [ 2350/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:27:28  time: 1.4507  data_time: 0.0667  memory: 28401  grad_norm: 55.7363  loss: 31.4194  loss_cls: 0.6514  loss_mask: 0.2266  loss_dice: 2.0966  d0.loss_cls: 1.0209  d0.loss_mask: 0.2805  d0.loss_dice: 2.3482  d1.loss_cls: 0.7684  d1.loss_mask: 0.2623  d1.loss_dice: 2.4014  d2.loss_cls: 0.6919  d2.loss_mask: 0.2505  d2.loss_dice: 2.2942  d3.loss_cls: 0.6453  d3.loss_mask: 0.2404  d3.loss_dice: 2.1944  d4.loss_cls: 0.6357  d4.loss_mask: 0.2377  d4.loss_dice: 2.1801  d5.loss_cls: 0.6410  d5.loss_mask: 0.2334  d5.loss_dice: 2.1624  d6.loss_cls: 0.6452  d6.loss_mask: 0.2295  d6.loss_dice: 2.1143  d7.loss_cls: 0.6432  d7.loss_mask: 0.2293  d7.loss_dice: 2.1142  d8.loss_cls: 0.6455  d8.loss_mask: 0.2276  d8.loss_dice: 2.1073
05/08 03:51:49 - mmengine - INFO - Iter(train) [ 2400/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:25:37  time: 1.4256  data_time: 0.0566  memory: 29055  grad_norm: 57.6070  loss: 32.0884  loss_cls: 0.6450  loss_mask: 0.2238  loss_dice: 2.1702  d0.loss_cls: 1.0229  d0.loss_mask: 0.2778  d0.loss_dice: 2.4153  d1.loss_cls: 0.7624  d1.loss_mask: 0.2614  d1.loss_dice: 2.4761  d2.loss_cls: 0.6758  d2.loss_mask: 0.2474  d2.loss_dice: 2.3823  d3.loss_cls: 0.6389  d3.loss_mask: 0.2369  d3.loss_dice: 2.2720  d4.loss_cls: 0.6294  d4.loss_mask: 0.2335  d4.loss_dice: 2.2566  d5.loss_cls: 0.6322  d5.loss_mask: 0.2299  d5.loss_dice: 2.2389  d6.loss_cls: 0.6392  d6.loss_mask: 0.2267  d6.loss_dice: 2.1893  d7.loss_cls: 0.6378  d7.loss_mask: 0.2259  d7.loss_dice: 2.1908  d8.loss_cls: 0.6392  d8.loss_mask: 0.2249  d8.loss_dice: 2.1862
05/08 03:53:02 - mmengine - INFO - Iter(train) [ 2450/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:24:00  time: 1.4600  data_time: 0.1028  memory: 29007  grad_norm: 59.6912  loss: 32.8797  loss_cls: 0.6907  loss_mask: 0.2175  loss_dice: 2.2182  d0.loss_cls: 1.0220  d0.loss_mask: 0.2644  d0.loss_dice: 2.4769  d1.loss_cls: 0.8040  d1.loss_mask: 0.2520  d1.loss_dice: 2.5276  d2.loss_cls: 0.7218  d2.loss_mask: 0.2402  d2.loss_dice: 2.4273  d3.loss_cls: 0.6801  d3.loss_mask: 0.2300  d3.loss_dice: 2.3188  d4.loss_cls: 0.6706  d4.loss_mask: 0.2259  d4.loss_dice: 2.3025  d5.loss_cls: 0.6718  d5.loss_mask: 0.2243  d5.loss_dice: 2.2895  d6.loss_cls: 0.6802  d6.loss_mask: 0.2207  d6.loss_dice: 2.2338  d7.loss_cls: 0.6803  d7.loss_mask: 0.2196  d7.loss_dice: 2.2345  d8.loss_cls: 0.6840  d8.loss_mask: 0.2179  d8.loss_dice: 2.2325
05/08 03:54:15 - mmengine - INFO - Iter(train) [ 2500/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:22:25  time: 1.4638  data_time: 0.0633  memory: 29443  grad_norm: 48.1984  loss: 32.3972  loss_cls: 0.6457  loss_mask: 0.2090  loss_dice: 2.2082  d0.loss_cls: 1.0310  d0.loss_mask: 0.2593  d0.loss_dice: 2.4838  d1.loss_cls: 0.7783  d1.loss_mask: 0.2456  d1.loss_dice: 2.5247  d2.loss_cls: 0.6922  d2.loss_mask: 0.2324  d2.loss_dice: 2.4238  d3.loss_cls: 0.6403  d3.loss_mask: 0.2210  d3.loss_dice: 2.3104  d4.loss_cls: 0.6338  d4.loss_mask: 0.2180  d4.loss_dice: 2.2919  d5.loss_cls: 0.6335  d5.loss_mask: 0.2148  d5.loss_dice: 2.2782  d6.loss_cls: 0.6377  d6.loss_mask: 0.2112  d6.loss_dice: 2.2316  d7.loss_cls: 0.6379  d7.loss_mask: 0.2101  d7.loss_dice: 2.2268  d8.loss_cls: 0.6395  d8.loss_mask: 0.2089  d8.loss_dice: 2.2175
05/08 03:55:27 - mmengine - INFO - Iter(train) [ 2550/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:20:37  time: 1.4245  data_time: 0.0627  memory: 29050  grad_norm: 53.0455  loss: 32.7001  loss_cls: 0.6539  loss_mask: 0.2206  loss_dice: 2.2221  d0.loss_cls: 1.0257  d0.loss_mask: 0.2707  d0.loss_dice: 2.4843  d1.loss_cls: 0.7709  d1.loss_mask: 0.2560  d1.loss_dice: 2.5409  d2.loss_cls: 0.6926  d2.loss_mask: 0.2414  d2.loss_dice: 2.4355  d3.loss_cls: 0.6482  d3.loss_mask: 0.2337  d3.loss_dice: 2.3261  d4.loss_cls: 0.6404  d4.loss_mask: 0.2308  d4.loss_dice: 2.3094  d5.loss_cls: 0.6437  d5.loss_mask: 0.2265  d5.loss_dice: 2.2915  d6.loss_cls: 0.6496  d6.loss_mask: 0.2243  d6.loss_dice: 2.2425  d7.loss_cls: 0.6498  d7.loss_mask: 0.2231  d7.loss_dice: 2.2407  d8.loss_cls: 0.6503  d8.loss_mask: 0.2207  d8.loss_dice: 2.2342
05/08 03:56:39 - mmengine - INFO - Iter(train) [ 2600/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:18:58  time: 1.4439  data_time: 0.0656  memory: 29268  grad_norm: 60.6729  loss: 32.8575  loss_cls: 0.6552  loss_mask: 0.2234  loss_dice: 2.2236  d0.loss_cls: 1.0318  d0.loss_mask: 0.2776  d0.loss_dice: 2.5213  d1.loss_cls: 0.7797  d1.loss_mask: 0.2598  d1.loss_dice: 2.5627  d2.loss_cls: 0.6971  d2.loss_mask: 0.2465  d2.loss_dice: 2.4476  d3.loss_cls: 0.6562  d3.loss_mask: 0.2362  d3.loss_dice: 2.3315  d4.loss_cls: 0.6469  d4.loss_mask: 0.2332  d4.loss_dice: 2.3143  d5.loss_cls: 0.6477  d5.loss_mask: 0.2303  d5.loss_dice: 2.2913  d6.loss_cls: 0.6502  d6.loss_mask: 0.2265  d6.loss_dice: 2.2452  d7.loss_cls: 0.6484  d7.loss_mask: 0.2256  d7.loss_dice: 2.2400  d8.loss_cls: 0.6480  d8.loss_mask: 0.2233  d8.loss_dice: 2.2365
05/08 03:57:50 - mmengine - INFO - Iter(train) [ 2650/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:17:15  time: 1.4312  data_time: 0.0613  memory: 28742  grad_norm: 47.0543  loss: 31.8120  loss_cls: 0.6510  loss_mask: 0.2233  loss_dice: 2.1328  d0.loss_cls: 1.0266  d0.loss_mask: 0.2757  d0.loss_dice: 2.4122  d1.loss_cls: 0.7670  d1.loss_mask: 0.2632  d1.loss_dice: 2.4376  d2.loss_cls: 0.6896  d2.loss_mask: 0.2475  d2.loss_dice: 2.3426  d3.loss_cls: 0.6461  d3.loss_mask: 0.2367  d3.loss_dice: 2.2360  d4.loss_cls: 0.6364  d4.loss_mask: 0.2333  d4.loss_dice: 2.2220  d5.loss_cls: 0.6394  d5.loss_mask: 0.2294  d5.loss_dice: 2.1991  d6.loss_cls: 0.6467  d6.loss_mask: 0.2267  d6.loss_dice: 2.1526  d7.loss_cls: 0.6420  d7.loss_mask: 0.2258  d7.loss_dice: 2.1544  d8.loss_cls: 0.6469  d8.loss_mask: 0.2237  d8.loss_dice: 2.1454
05/08 03:59:02 - mmengine - INFO - Iter(train) [ 2700/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:15:34  time: 1.4333  data_time: 0.0555  memory: 28720  grad_norm: 51.5247  loss: 31.8833  loss_cls: 0.6578  loss_mask: 0.2105  loss_dice: 2.1518  d0.loss_cls: 1.0208  d0.loss_mask: 0.2603  d0.loss_dice: 2.4133  d1.loss_cls: 0.7719  d1.loss_mask: 0.2443  d1.loss_dice: 2.4634  d2.loss_cls: 0.6954  d2.loss_mask: 0.2326  d2.loss_dice: 2.3623  d3.loss_cls: 0.6533  d3.loss_mask: 0.2234  d3.loss_dice: 2.2476  d4.loss_cls: 0.6426  d4.loss_mask: 0.2189  d4.loss_dice: 2.2371  d5.loss_cls: 0.6474  d5.loss_mask: 0.2164  d5.loss_dice: 2.2182  d6.loss_cls: 0.6542  d6.loss_mask: 0.2135  d6.loss_dice: 2.1685  d7.loss_cls: 0.6508  d7.loss_mask: 0.2122  d7.loss_dice: 2.1661  d8.loss_cls: 0.6542  d8.loss_mask: 0.2112  d8.loss_dice: 2.1632
05/08 04:00:14 - mmengine - INFO - Iter(train) [ 2750/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:13:56  time: 1.4386  data_time: 0.0510  memory: 29265  grad_norm: 45.1976  loss: 33.0890  loss_cls: 0.6607  loss_mask: 0.2259  loss_dice: 2.2420  d0.loss_cls: 1.0430  d0.loss_mask: 0.2845  d0.loss_dice: 2.5348  d1.loss_cls: 0.7794  d1.loss_mask: 0.2640  d1.loss_dice: 2.5759  d2.loss_cls: 0.7057  d2.loss_mask: 0.2513  d2.loss_dice: 2.4624  d3.loss_cls: 0.6565  d3.loss_mask: 0.2397  d3.loss_dice: 2.3469  d4.loss_cls: 0.6477  d4.loss_mask: 0.2363  d4.loss_dice: 2.3262  d5.loss_cls: 0.6493  d5.loss_mask: 0.2320  d5.loss_dice: 2.3085  d6.loss_cls: 0.6529  d6.loss_mask: 0.2291  d6.loss_dice: 2.2625  d7.loss_cls: 0.6509  d7.loss_mask: 0.2281  d7.loss_dice: 2.2592  d8.loss_cls: 0.6534  d8.loss_mask: 0.2259  d8.loss_dice: 2.2542
05/08 04:01:25 - mmengine - INFO - Iter(train) [ 2800/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:12:15  time: 1.4253  data_time: 0.0632  memory: 29417  grad_norm: 52.5675  loss: 33.0476  loss_cls: 0.6475  loss_mask: 0.2232  loss_dice: 2.2528  d0.loss_cls: 1.0366  d0.loss_mask: 0.2739  d0.loss_dice: 2.5364  d1.loss_cls: 0.7761  d1.loss_mask: 0.2647  d1.loss_dice: 2.5719  d2.loss_cls: 0.6930  d2.loss_mask: 0.2492  d2.loss_dice: 2.4740  d3.loss_cls: 0.6430  d3.loss_mask: 0.2377  d3.loss_dice: 2.3626  d4.loss_cls: 0.6317  d4.loss_mask: 0.2335  d4.loss_dice: 2.3428  d5.loss_cls: 0.6298  d5.loss_mask: 0.2295  d5.loss_dice: 2.3231  d6.loss_cls: 0.6414  d6.loss_mask: 0.2270  d6.loss_dice: 2.2754  d7.loss_cls: 0.6411  d7.loss_mask: 0.2255  d7.loss_dice: 2.2715  d8.loss_cls: 0.6426  d8.loss_mask: 0.2241  d8.loss_dice: 2.2658
05/08 04:02:39 - mmengine - INFO - Iter(train) [ 2850/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:10:52  time: 1.4829  data_time: 0.1031  memory: 29605  grad_norm: 53.5509  loss: 32.2572  loss_cls: 0.6591  loss_mask: 0.2111  loss_dice: 2.1817  d0.loss_cls: 1.0250  d0.loss_mask: 0.2647  d0.loss_dice: 2.4436  d1.loss_cls: 0.7745  d1.loss_mask: 0.2550  d1.loss_dice: 2.4956  d2.loss_cls: 0.6938  d2.loss_mask: 0.2337  d2.loss_dice: 2.3976  d3.loss_cls: 0.6508  d3.loss_mask: 0.2245  d3.loss_dice: 2.2861  d4.loss_cls: 0.6412  d4.loss_mask: 0.2193  d4.loss_dice: 2.2744  d5.loss_cls: 0.6449  d5.loss_mask: 0.2163  d5.loss_dice: 2.2530  d6.loss_cls: 0.6551  d6.loss_mask: 0.2137  d6.loss_dice: 2.2079  d7.loss_cls: 0.6503  d7.loss_mask: 0.2128  d7.loss_dice: 2.2059  d8.loss_cls: 0.6501  d8.loss_mask: 0.2127  d8.loss_dice: 2.2028
05/08 04:03:52 - mmengine - INFO - Iter(train) [ 2900/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:09:21  time: 1.4541  data_time: 0.0501  memory: 29344  grad_norm: 54.0470  loss: 32.8538  loss_cls: 0.6574  loss_mask: 0.2058  loss_dice: 2.2528  d0.loss_cls: 1.0270  d0.loss_mask: 0.2566  d0.loss_dice: 2.5054  d1.loss_cls: 0.7847  d1.loss_mask: 0.2415  d1.loss_dice: 2.5722  d2.loss_cls: 0.6993  d2.loss_mask: 0.2279  d2.loss_dice: 2.4600  d3.loss_cls: 0.6527  d3.loss_mask: 0.2183  d3.loss_dice: 2.3465  d4.loss_cls: 0.6453  d4.loss_mask: 0.2141  d4.loss_dice: 2.3362  d5.loss_cls: 0.6474  d5.loss_mask: 0.2116  d5.loss_dice: 2.3167  d6.loss_cls: 0.6522  d6.loss_mask: 0.2091  d6.loss_dice: 2.2685  d7.loss_cls: 0.6477  d7.loss_mask: 0.2073  d7.loss_dice: 2.2685  d8.loss_cls: 0.6496  d8.loss_mask: 0.2064  d8.loss_dice: 2.2649
05/08 04:05:04 - mmengine - INFO - Iter(train) [ 2950/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:07:47  time: 1.4415  data_time: 0.0520  memory: 29658  grad_norm: 61.5183  loss: 33.4641  loss_cls: 0.6578  loss_mask: 0.2114  loss_dice: 2.3009  d0.loss_cls: 1.0507  d0.loss_mask: 0.2600  d0.loss_dice: 2.5488  d1.loss_cls: 0.7774  d1.loss_mask: 0.2468  d1.loss_dice: 2.6261  d2.loss_cls: 0.6984  d2.loss_mask: 0.2328  d2.loss_dice: 2.5214  d3.loss_cls: 0.6523  d3.loss_mask: 0.2235  d3.loss_dice: 2.4029  d4.loss_cls: 0.6432  d4.loss_mask: 0.2204  d4.loss_dice: 2.3902  d5.loss_cls: 0.6466  d5.loss_mask: 0.2172  d5.loss_dice: 2.3754  d6.loss_cls: 0.6536  d6.loss_mask: 0.2139  d6.loss_dice: 2.3245  d7.loss_cls: 0.6509  d7.loss_mask: 0.2133  d7.loss_dice: 2.3173  d8.loss_cls: 0.6574  d8.loss_mask: 0.2108  d8.loss_dice: 2.3183
05/08 04:06:16 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 04:06:16 - mmengine - INFO - Iter(train) [ 3000/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:06:13  time: 1.4401  data_time: 0.0481  memory: 29648  grad_norm: 43.4897  loss: 33.3200  loss_cls: 0.6634  loss_mask: 0.2155  loss_dice: 2.2777  d0.loss_cls: 1.0343  d0.loss_mask: 0.2643  d0.loss_dice: 2.5459  d1.loss_cls: 0.7867  d1.loss_mask: 0.2513  d1.loss_dice: 2.6087  d2.loss_cls: 0.7011  d2.loss_mask: 0.2366  d2.loss_dice: 2.5008  d3.loss_cls: 0.6591  d3.loss_mask: 0.2265  d3.loss_dice: 2.3808  d4.loss_cls: 0.6449  d4.loss_mask: 0.2240  d4.loss_dice: 2.3704  d5.loss_cls: 0.6474  d5.loss_mask: 0.2212  d5.loss_dice: 2.3512  d6.loss_cls: 0.6571  d6.loss_mask: 0.2192  d6.loss_dice: 2.2997  d7.loss_cls: 0.6545  d7.loss_mask: 0.2182  d7.loss_dice: 2.2967  d8.loss_cls: 0.6563  d8.loss_mask: 0.2160  d8.loss_dice: 2.2907
05/08 04:06:16 - mmengine - INFO - Saving checkpoint at 3000 iterations
05/08 04:07:08 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9765  data_time: 0.0271  memory: 3258  
05/08 04:07:31 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.49s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 04:07:39 - mmengine - INFO - start multi processing evaluation ...
DONE (t=52.86s).
Accumulating evaluation results...
DONE (t=0.06s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.372
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.744
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.300
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.437
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.811
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.498
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.906
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.461
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.371
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.561
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.833
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.498
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.498
05/08 04:08:32 - mmengine - INFO - segm_mAP_copypaste: 0.372 0.744 0.300 0.219 0.437 0.811
05/08 04:08:33 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.3720  coco/segm_mAP_50: 0.7440  coco/segm_mAP_75: 0.3000  coco/segm_mAP_s: 0.2190  coco/segm_mAP_m: 0.4370  coco/segm_mAP_l: 0.8110  data_time: 0.0270  time: 0.9746
05/08 04:08:33 - mmengine - INFO - The previous best checkpoint /data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco/best_coco_segm_mAP_50_iter_2000.pth is removed
05/08 04:08:34 - mmengine - INFO - The best checkpoint with 0.7440 coco/segm_mAP_50 at 3000 iter is saved to best_coco_segm_mAP_50_iter_3000.pth.
05/08 04:09:48 - mmengine - INFO - Iter(train) [ 3050/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:12:49  time: 3.2020  data_time: 1.8458  memory: 28653  grad_norm: 63.3024  loss: 31.9985  loss_cls: 0.6534  loss_mask: 0.2242  loss_dice: 2.1537  d0.loss_cls: 1.0327  d0.loss_mask: 0.2754  d0.loss_dice: 2.3971  d1.loss_cls: 0.7722  d1.loss_mask: 0.2581  d1.loss_dice: 2.4606  d2.loss_cls: 0.6947  d2.loss_mask: 0.2460  d2.loss_dice: 2.3604  d3.loss_cls: 0.6497  d3.loss_mask: 0.2365  d3.loss_dice: 2.2520  d4.loss_cls: 0.6408  d4.loss_mask: 0.2329  d4.loss_dice: 2.2354  d5.loss_cls: 0.6431  d5.loss_mask: 0.2297  d5.loss_dice: 2.2169  d6.loss_cls: 0.6494  d6.loss_mask: 0.2273  d6.loss_dice: 2.1736  d7.loss_cls: 0.6447  d7.loss_mask: 0.2261  d7.loss_dice: 2.1729  d8.loss_cls: 0.6467  d8.loss_mask: 0.2249  d8.loss_dice: 2.1675
05/08 04:10:59 - mmengine - INFO - Iter(train) [ 3100/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:11:04  time: 1.4278  data_time: 0.0500  memory: 29150  grad_norm: 69.4015  loss: 32.4472  loss_cls: 0.6549  loss_mask: 0.2193  loss_dice: 2.1902  d0.loss_cls: 1.0469  d0.loss_mask: 0.2737  d0.loss_dice: 2.4613  d1.loss_cls: 0.7785  d1.loss_mask: 0.2557  d1.loss_dice: 2.5250  d2.loss_cls: 0.6927  d2.loss_mask: 0.2398  d2.loss_dice: 2.4122  d3.loss_cls: 0.6496  d3.loss_mask: 0.2317  d3.loss_dice: 2.2950  d4.loss_cls: 0.6419  d4.loss_mask: 0.2287  d4.loss_dice: 2.2795  d5.loss_cls: 0.6417  d5.loss_mask: 0.2257  d5.loss_dice: 2.2626  d6.loss_cls: 0.6495  d6.loss_mask: 0.2227  d6.loss_dice: 2.2114  d7.loss_cls: 0.6484  d7.loss_mask: 0.2217  d7.loss_dice: 2.2079  d8.loss_cls: 0.6526  d8.loss_mask: 0.2206  d8.loss_dice: 2.2060
05/08 04:12:11 - mmengine - INFO - Iter(train) [ 3150/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:09:25  time: 1.4445  data_time: 0.0553  memory: 28408  grad_norm: 71.6307  loss: 30.9199  loss_cls: 0.6382  loss_mask: 0.2016  loss_dice: 2.0813  d0.loss_cls: 1.0388  d0.loss_mask: 0.2497  d0.loss_dice: 2.3400  d1.loss_cls: 0.7592  d1.loss_mask: 0.2366  d1.loss_dice: 2.3909  d2.loss_cls: 0.6716  d2.loss_mask: 0.2244  d2.loss_dice: 2.2838  d3.loss_cls: 0.6307  d3.loss_mask: 0.2148  d3.loss_dice: 2.1771  d4.loss_cls: 0.6227  d4.loss_mask: 0.2119  d4.loss_dice: 2.1651  d5.loss_cls: 0.6248  d5.loss_mask: 0.2080  d5.loss_dice: 2.1490  d6.loss_cls: 0.6301  d6.loss_mask: 0.2053  d6.loss_dice: 2.1011  d7.loss_cls: 0.6318  d7.loss_mask: 0.2037  d7.loss_dice: 2.0989  d8.loss_cls: 0.6350  d8.loss_mask: 0.2024  d8.loss_dice: 2.0914
05/08 04:13:24 - mmengine - INFO - Iter(train) [ 3200/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:07:49  time: 1.4555  data_time: 0.0563  memory: 28532  grad_norm: 46.3928  loss: 30.3461  loss_cls: 0.6417  loss_mask: 0.2052  loss_dice: 2.0181  d0.loss_cls: 1.0326  d0.loss_mask: 0.2564  d0.loss_dice: 2.2649  d1.loss_cls: 0.7638  d1.loss_mask: 0.2415  d1.loss_dice: 2.3223  d2.loss_cls: 0.6756  d2.loss_mask: 0.2259  d2.loss_dice: 2.2236  d3.loss_cls: 0.6393  d3.loss_mask: 0.2169  d3.loss_dice: 2.1119  d4.loss_cls: 0.6292  d4.loss_mask: 0.2140  d4.loss_dice: 2.1008  d5.loss_cls: 0.6324  d5.loss_mask: 0.2108  d5.loss_dice: 2.0839  d6.loss_cls: 0.6366  d6.loss_mask: 0.2074  d6.loss_dice: 2.0366  d7.loss_cls: 0.6358  d7.loss_mask: 0.2073  d7.loss_dice: 2.0349  d8.loss_cls: 0.6371  d8.loss_mask: 0.2060  d8.loss_dice: 2.0337
05/08 04:14:38 - mmengine - INFO - Iter(train) [ 3250/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:06:19  time: 1.4755  data_time: 0.1110  memory: 28690  grad_norm: 53.6390  loss: 31.4593  loss_cls: 0.6644  loss_mask: 0.2107  loss_dice: 2.1008  d0.loss_cls: 1.0367  d0.loss_mask: 0.2657  d0.loss_dice: 2.3672  d1.loss_cls: 0.7826  d1.loss_mask: 0.2453  d1.loss_dice: 2.4172  d2.loss_cls: 0.6945  d2.loss_mask: 0.2334  d2.loss_dice: 2.3092  d3.loss_cls: 0.6532  d3.loss_mask: 0.2221  d3.loss_dice: 2.2062  d4.loss_cls: 0.6496  d4.loss_mask: 0.2192  d4.loss_dice: 2.1856  d5.loss_cls: 0.6495  d5.loss_mask: 0.2162  d5.loss_dice: 2.1670  d6.loss_cls: 0.6536  d6.loss_mask: 0.2148  d6.loss_dice: 2.1252  d7.loss_cls: 0.6523  d7.loss_mask: 0.2136  d7.loss_dice: 2.1226  d8.loss_cls: 0.6573  d8.loss_mask: 0.2121  d8.loss_dice: 2.1117
05/08 04:15:50 - mmengine - INFO - Iter(train) [ 3300/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:04:40  time: 1.4364  data_time: 0.0570  memory: 29489  grad_norm: 63.4107  loss: 32.4455  loss_cls: 0.6483  loss_mask: 0.2118  loss_dice: 2.2093  d0.loss_cls: 1.0491  d0.loss_mask: 0.2627  d0.loss_dice: 2.4594  d1.loss_cls: 0.7773  d1.loss_mask: 0.2489  d1.loss_dice: 2.5265  d2.loss_cls: 0.6802  d2.loss_mask: 0.2332  d2.loss_dice: 2.4299  d3.loss_cls: 0.6397  d3.loss_mask: 0.2236  d3.loss_dice: 2.3145  d4.loss_cls: 0.6341  d4.loss_mask: 0.2221  d4.loss_dice: 2.2921  d5.loss_cls: 0.6349  d5.loss_mask: 0.2183  d5.loss_dice: 2.2784  d6.loss_cls: 0.6395  d6.loss_mask: 0.2151  d6.loss_dice: 2.2319  d7.loss_cls: 0.6376  d7.loss_mask: 0.2142  d7.loss_dice: 2.2334  d8.loss_cls: 0.6408  d8.loss_mask: 0.2126  d8.loss_dice: 2.2261
05/08 04:17:01 - mmengine - INFO - Iter(train) [ 3350/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:03:01  time: 1.4357  data_time: 0.0679  memory: 28738  grad_norm: 49.9475  loss: 31.8909  loss_cls: 0.6380  loss_mask: 0.2039  loss_dice: 2.1737  d0.loss_cls: 1.0383  d0.loss_mask: 0.2491  d0.loss_dice: 2.4114  d1.loss_cls: 0.7737  d1.loss_mask: 0.2384  d1.loss_dice: 2.4901  d2.loss_cls: 0.6765  d2.loss_mask: 0.2240  d2.loss_dice: 2.3952  d3.loss_cls: 0.6324  d3.loss_mask: 0.2161  d3.loss_dice: 2.2749  d4.loss_cls: 0.6276  d4.loss_mask: 0.2122  d4.loss_dice: 2.2587  d5.loss_cls: 0.6260  d5.loss_mask: 0.2089  d5.loss_dice: 2.2400  d6.loss_cls: 0.6350  d6.loss_mask: 0.2059  d6.loss_dice: 2.1899  d7.loss_cls: 0.6298  d7.loss_mask: 0.2048  d7.loss_dice: 2.1901  d8.loss_cls: 0.6332  d8.loss_mask: 0.2045  d8.loss_dice: 2.1884
05/08 04:18:13 - mmengine - INFO - Iter(train) [ 3400/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 7:01:24  time: 1.4380  data_time: 0.0559  memory: 29272  grad_norm: 66.2250  loss: 30.7889  loss_cls: 0.6334  loss_mask: 0.2130  loss_dice: 2.0644  d0.loss_cls: 1.0297  d0.loss_mask: 0.2651  d0.loss_dice: 2.3044  d1.loss_cls: 0.7646  d1.loss_mask: 0.2487  d1.loss_dice: 2.3644  d2.loss_cls: 0.6701  d2.loss_mask: 0.2348  d2.loss_dice: 2.2650  d3.loss_cls: 0.6275  d3.loss_mask: 0.2262  d3.loss_dice: 2.1586  d4.loss_cls: 0.6216  d4.loss_mask: 0.2211  d4.loss_dice: 2.1409  d5.loss_cls: 0.6255  d5.loss_mask: 0.2181  d5.loss_dice: 2.1224  d6.loss_cls: 0.6307  d6.loss_mask: 0.2164  d6.loss_dice: 2.0798  d7.loss_cls: 0.6271  d7.loss_mask: 0.2146  d7.loss_dice: 2.0795  d8.loss_cls: 0.6293  d8.loss_mask: 0.2135  d8.loss_dice: 2.0783
05/08 04:19:26 - mmengine - INFO - Iter(train) [ 3450/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:59:50  time: 1.4500  data_time: 0.0600  memory: 28810  grad_norm: 68.9645  loss: 31.8807  loss_cls: 0.6630  loss_mask: 0.2136  loss_dice: 2.1440  d0.loss_cls: 1.0289  d0.loss_mask: 0.2643  d0.loss_dice: 2.4052  d1.loss_cls: 0.7785  d1.loss_mask: 0.2510  d1.loss_dice: 2.4584  d2.loss_cls: 0.6979  d2.loss_mask: 0.2362  d2.loss_dice: 2.3516  d3.loss_cls: 0.6562  d3.loss_mask: 0.2241  d3.loss_dice: 2.2427  d4.loss_cls: 0.6494  d4.loss_mask: 0.2216  d4.loss_dice: 2.2248  d5.loss_cls: 0.6494  d5.loss_mask: 0.2180  d5.loss_dice: 2.2080  d6.loss_cls: 0.6578  d6.loss_mask: 0.2160  d6.loss_dice: 2.1608  d7.loss_cls: 0.6554  d7.loss_mask: 0.2158  d7.loss_dice: 2.1594  d8.loss_cls: 0.6613  d8.loss_mask: 0.2139  d8.loss_dice: 2.1536
05/08 04:20:37 - mmengine - INFO - Iter(train) [ 3500/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:58:12  time: 1.4284  data_time: 0.0634  memory: 29150  grad_norm: 51.8858  loss: 32.2849  loss_cls: 0.6616  loss_mask: 0.2181  loss_dice: 2.1790  d0.loss_cls: 1.0429  d0.loss_mask: 0.2685  d0.loss_dice: 2.4298  d1.loss_cls: 0.7759  d1.loss_mask: 0.2559  d1.loss_dice: 2.4973  d2.loss_cls: 0.6932  d2.loss_mask: 0.2396  d2.loss_dice: 2.3869  d3.loss_cls: 0.6506  d3.loss_mask: 0.2294  d3.loss_dice: 2.2824  d4.loss_cls: 0.6457  d4.loss_mask: 0.2272  d4.loss_dice: 2.2659  d5.loss_cls: 0.6475  d5.loss_mask: 0.2238  d5.loss_dice: 2.2466  d6.loss_cls: 0.6539  d6.loss_mask: 0.2204  d6.loss_dice: 2.2030  d7.loss_cls: 0.6552  d7.loss_mask: 0.2196  d7.loss_dice: 2.1967  d8.loss_cls: 0.6570  d8.loss_mask: 0.2189  d8.loss_dice: 2.1922
05/08 04:21:49 - mmengine - INFO - Iter(train) [ 3550/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:56:37  time: 1.4381  data_time: 0.0623  memory: 29218  grad_norm: 65.7706  loss: 31.2773  loss_cls: 0.6555  loss_mask: 0.2091  loss_dice: 2.0991  d0.loss_cls: 1.0452  d0.loss_mask: 0.2641  d0.loss_dice: 2.3391  d1.loss_cls: 0.7652  d1.loss_mask: 0.2502  d1.loss_dice: 2.3947  d2.loss_cls: 0.6792  d2.loss_mask: 0.2333  d2.loss_dice: 2.3010  d3.loss_cls: 0.6456  d3.loss_mask: 0.2223  d3.loss_dice: 2.1909  d4.loss_cls: 0.6363  d4.loss_mask: 0.2198  d4.loss_dice: 2.1794  d5.loss_cls: 0.6429  d5.loss_mask: 0.2147  d5.loss_dice: 2.1626  d6.loss_cls: 0.6496  d6.loss_mask: 0.2118  d6.loss_dice: 2.1193  d7.loss_cls: 0.6460  d7.loss_mask: 0.2112  d7.loss_dice: 2.1181  d8.loss_cls: 0.6487  d8.loss_mask: 0.2099  d8.loss_dice: 2.1125
05/08 04:23:02 - mmengine - INFO - Iter(train) [ 3600/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:55:06  time: 1.4552  data_time: 0.0613  memory: 29312  grad_norm: 69.9242  loss: 31.5841  loss_cls: 0.6572  loss_mask: 0.2137  loss_dice: 2.1162  d0.loss_cls: 1.0535  d0.loss_mask: 0.2645  d0.loss_dice: 2.3818  d1.loss_cls: 0.7698  d1.loss_mask: 0.2487  d1.loss_dice: 2.4401  d2.loss_cls: 0.6882  d2.loss_mask: 0.2367  d2.loss_dice: 2.3243  d3.loss_cls: 0.6541  d3.loss_mask: 0.2248  d3.loss_dice: 2.2082  d4.loss_cls: 0.6429  d4.loss_mask: 0.2228  d4.loss_dice: 2.1958  d5.loss_cls: 0.6410  d5.loss_mask: 0.2192  d5.loss_dice: 2.1786  d6.loss_cls: 0.6501  d6.loss_mask: 0.2167  d6.loss_dice: 2.1350  d7.loss_cls: 0.6500  d7.loss_mask: 0.2163  d7.loss_dice: 2.1352  d8.loss_cls: 0.6530  d8.loss_mask: 0.2147  d8.loss_dice: 2.1308
05/08 04:24:16 - mmengine - INFO - Iter(train) [ 3650/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:53:42  time: 1.4832  data_time: 0.1086  memory: 30864  grad_norm: 62.8043  loss: 30.9039  loss_cls: 0.6405  loss_mask: 0.1982  loss_dice: 2.0842  d0.loss_cls: 1.0566  d0.loss_mask: 0.2477  d0.loss_dice: 2.3171  d1.loss_cls: 0.7629  d1.loss_mask: 0.2356  d1.loss_dice: 2.3917  d2.loss_cls: 0.6737  d2.loss_mask: 0.2201  d2.loss_dice: 2.2867  d3.loss_cls: 0.6340  d3.loss_mask: 0.2110  d3.loss_dice: 2.1773  d4.loss_cls: 0.6241  d4.loss_mask: 0.2074  d4.loss_dice: 2.1667  d5.loss_cls: 0.6268  d5.loss_mask: 0.2036  d5.loss_dice: 2.1474  d6.loss_cls: 0.6321  d6.loss_mask: 0.2008  d6.loss_dice: 2.1007  d7.loss_cls: 0.6288  d7.loss_mask: 0.2009  d7.loss_dice: 2.0982  d8.loss_cls: 0.6338  d8.loss_mask: 0.1985  d8.loss_dice: 2.0969
05/08 04:25:28 - mmengine - INFO - Iter(train) [ 3700/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:52:07  time: 1.4312  data_time: 0.0568  memory: 29085  grad_norm: 48.0034  loss: 31.8483  loss_cls: 0.6501  loss_mask: 0.2095  loss_dice: 2.1493  d0.loss_cls: 1.0397  d0.loss_mask: 0.2583  d0.loss_dice: 2.4076  d1.loss_cls: 0.7804  d1.loss_mask: 0.2410  d1.loss_dice: 2.4869  d2.loss_cls: 0.6917  d2.loss_mask: 0.2280  d2.loss_dice: 2.3723  d3.loss_cls: 0.6481  d3.loss_mask: 0.2200  d3.loss_dice: 2.2448  d4.loss_cls: 0.6433  d4.loss_mask: 0.2174  d4.loss_dice: 2.2273  d5.loss_cls: 0.6411  d5.loss_mask: 0.2142  d5.loss_dice: 2.2139  d6.loss_cls: 0.6466  d6.loss_mask: 0.2118  d6.loss_dice: 2.1643  d7.loss_cls: 0.6461  d7.loss_mask: 0.2111  d7.loss_dice: 2.1642  d8.loss_cls: 0.6472  d8.loss_mask: 0.2103  d8.loss_dice: 2.1617
05/08 04:26:39 - mmengine - INFO - Iter(train) [ 3750/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:50:34  time: 1.4375  data_time: 0.0563  memory: 27996  grad_norm: 73.6639  loss: 29.6923  loss_cls: 0.6349  loss_mask: 0.2070  loss_dice: 1.9673  d0.loss_cls: 1.0297  d0.loss_mask: 0.2600  d0.loss_dice: 2.1943  d1.loss_cls: 0.7655  d1.loss_mask: 0.2446  d1.loss_dice: 2.2463  d2.loss_cls: 0.6676  d2.loss_mask: 0.2270  d2.loss_dice: 2.1571  d3.loss_cls: 0.6328  d3.loss_mask: 0.2166  d3.loss_dice: 2.0505  d4.loss_cls: 0.6215  d4.loss_mask: 0.2153  d4.loss_dice: 2.0360  d5.loss_cls: 0.6228  d5.loss_mask: 0.2122  d5.loss_dice: 2.0251  d6.loss_cls: 0.6289  d6.loss_mask: 0.2096  d6.loss_dice: 1.9854  d7.loss_cls: 0.6284  d7.loss_mask: 0.2091  d7.loss_dice: 1.9801  d8.loss_cls: 0.6303  d8.loss_mask: 0.2078  d8.loss_dice: 1.9784
05/08 04:27:52 - mmengine - INFO - Iter(train) [ 3800/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:49:05  time: 1.4529  data_time: 0.0614  memory: 30393  grad_norm: 51.6811  loss: 32.5201  loss_cls: 0.6503  loss_mask: 0.2050  loss_dice: 2.2275  d0.loss_cls: 1.0571  d0.loss_mask: 0.2552  d0.loss_dice: 2.4579  d1.loss_cls: 0.7728  d1.loss_mask: 0.2454  d1.loss_dice: 2.5267  d2.loss_cls: 0.6831  d2.loss_mask: 0.2240  d2.loss_dice: 2.4317  d3.loss_cls: 0.6480  d3.loss_mask: 0.2154  d3.loss_dice: 2.3227  d4.loss_cls: 0.6364  d4.loss_mask: 0.2140  d4.loss_dice: 2.3052  d5.loss_cls: 0.6426  d5.loss_mask: 0.2103  d5.loss_dice: 2.2899  d6.loss_cls: 0.6495  d6.loss_mask: 0.2075  d6.loss_dice: 2.2437  d7.loss_cls: 0.6487  d7.loss_mask: 0.2069  d7.loss_dice: 2.2467  d8.loss_cls: 0.6501  d8.loss_mask: 0.2058  d8.loss_dice: 2.2402
05/08 04:29:05 - mmengine - INFO - Iter(train) [ 3850/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:47:37  time: 1.4562  data_time: 0.0593  memory: 29959  grad_norm: 40.9512  loss: 31.8017  loss_cls: 0.6507  loss_mask: 0.2083  loss_dice: 2.1500  d0.loss_cls: 1.0545  d0.loss_mask: 0.2581  d0.loss_dice: 2.4067  d1.loss_cls: 0.7761  d1.loss_mask: 0.2419  d1.loss_dice: 2.4552  d2.loss_cls: 0.6828  d2.loss_mask: 0.2273  d2.loss_dice: 2.3564  d3.loss_cls: 0.6414  d3.loss_mask: 0.2198  d3.loss_dice: 2.2485  d4.loss_cls: 0.6321  d4.loss_mask: 0.2166  d4.loss_dice: 2.2340  d5.loss_cls: 0.6368  d5.loss_mask: 0.2151  d5.loss_dice: 2.2165  d6.loss_cls: 0.6446  d6.loss_mask: 0.2117  d6.loss_dice: 2.1713  d7.loss_cls: 0.6405  d7.loss_mask: 0.2106  d7.loss_dice: 2.1724  d8.loss_cls: 0.6457  d8.loss_mask: 0.2093  d8.loss_dice: 2.1670
05/08 04:30:18 - mmengine - INFO - Iter(train) [ 3900/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:46:11  time: 1.4657  data_time: 0.0544  memory: 29229  grad_norm: 41.4553  loss: 31.3564  loss_cls: 0.6644  loss_mask: 0.2006  loss_dice: 2.1037  d0.loss_cls: 1.0521  d0.loss_mask: 0.2500  d0.loss_dice: 2.3565  d1.loss_cls: 0.7767  d1.loss_mask: 0.2376  d1.loss_dice: 2.4142  d2.loss_cls: 0.6955  d2.loss_mask: 0.2220  d2.loss_dice: 2.3050  d3.loss_cls: 0.6559  d3.loss_mask: 0.2127  d3.loss_dice: 2.1960  d4.loss_cls: 0.6490  d4.loss_mask: 0.2084  d4.loss_dice: 2.1840  d5.loss_cls: 0.6515  d5.loss_mask: 0.2070  d5.loss_dice: 2.1679  d6.loss_cls: 0.6570  d6.loss_mask: 0.2038  d6.loss_dice: 2.1238  d7.loss_cls: 0.6601  d7.loss_mask: 0.2017  d7.loss_dice: 2.1197  d8.loss_cls: 0.6627  d8.loss_mask: 0.2014  d8.loss_dice: 2.1157
05/08 04:31:30 - mmengine - INFO - Iter(train) [ 3950/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:44:41  time: 1.4432  data_time: 0.0488  memory: 29203  grad_norm: 51.9941  loss: 31.6941  loss_cls: 0.6631  loss_mask: 0.2127  loss_dice: 2.1240  d0.loss_cls: 1.0547  d0.loss_mask: 0.2660  d0.loss_dice: 2.3788  d1.loss_cls: 0.7569  d1.loss_mask: 0.2486  d1.loss_dice: 2.4482  d2.loss_cls: 0.6856  d2.loss_mask: 0.2354  d2.loss_dice: 2.3343  d3.loss_cls: 0.6500  d3.loss_mask: 0.2250  d3.loss_dice: 2.2253  d4.loss_cls: 0.6418  d4.loss_mask: 0.2223  d4.loss_dice: 2.2042  d5.loss_cls: 0.6480  d5.loss_mask: 0.2186  d5.loss_dice: 2.1913  d6.loss_cls: 0.6596  d6.loss_mask: 0.2157  d6.loss_dice: 2.1445  d7.loss_cls: 0.6615  d7.loss_mask: 0.2157  d7.loss_dice: 2.1426  d8.loss_cls: 0.6635  d8.loss_mask: 0.2139  d8.loss_dice: 2.1421
05/08 04:32:43 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 04:32:43 - mmengine - INFO - Iter(train) [ 4000/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:43:13  time: 1.4496  data_time: 0.0617  memory: 28751  grad_norm: 65.5818  loss: 30.9947  loss_cls: 0.6591  loss_mask: 0.2041  loss_dice: 2.0749  d0.loss_cls: 1.0563  d0.loss_mask: 0.2512  d0.loss_dice: 2.2904  d1.loss_cls: 0.7688  d1.loss_mask: 0.2389  d1.loss_dice: 2.3772  d2.loss_cls: 0.6830  d2.loss_mask: 0.2245  d2.loss_dice: 2.2739  d3.loss_cls: 0.6471  d3.loss_mask: 0.2148  d3.loss_dice: 2.1660  d4.loss_cls: 0.6405  d4.loss_mask: 0.2125  d4.loss_dice: 2.1472  d5.loss_cls: 0.6462  d5.loss_mask: 0.2094  d5.loss_dice: 2.1314  d6.loss_cls: 0.6666  d6.loss_mask: 0.2059  d6.loss_dice: 2.0854  d7.loss_cls: 0.6678  d7.loss_mask: 0.2064  d7.loss_dice: 2.0886  d8.loss_cls: 0.6646  d8.loss_mask: 0.2050  d8.loss_dice: 2.0869
05/08 04:32:43 - mmengine - INFO - Saving checkpoint at 4000 iterations
05/08 04:33:35 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9801  data_time: 0.0285  memory: 3258  
05/08 04:33:57 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.43s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 04:34:05 - mmengine - INFO - start multi processing evaluation ...
DONE (t=53.32s).
Accumulating evaluation results...
DONE (t=0.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.378
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.740
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.303
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.207
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.463
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.869
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.502
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.891
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.445
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.583
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.502
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.502
05/08 04:34:59 - mmengine - INFO - segm_mAP_copypaste: 0.378 0.740 0.303 0.207 0.463 0.869
05/08 04:35:00 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.3780  coco/segm_mAP_50: 0.7400  coco/segm_mAP_75: 0.3030  coco/segm_mAP_s: 0.2070  coco/segm_mAP_m: 0.4630  coco/segm_mAP_l: 0.8690  data_time: 0.0284  time: 0.9779
05/08 04:36:09 - mmengine - INFO - Iter(train) [ 4050/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:47:09  time: 3.0944  data_time: 1.7447  memory: 28992  grad_norm: 45.9876  loss: 30.6623  loss_cls: 0.6527  loss_mask: 0.2087  loss_dice: 2.0488  d0.loss_cls: 1.0455  d0.loss_mask: 0.2561  d0.loss_dice: 2.2549  d1.loss_cls: 0.7668  d1.loss_mask: 0.2406  d1.loss_dice: 2.3346  d2.loss_cls: 0.6730  d2.loss_mask: 0.2272  d2.loss_dice: 2.2360  d3.loss_cls: 0.6434  d3.loss_mask: 0.2169  d3.loss_dice: 2.1259  d4.loss_cls: 0.6344  d4.loss_mask: 0.2140  d4.loss_dice: 2.1116  d5.loss_cls: 0.6374  d5.loss_mask: 0.2130  d5.loss_dice: 2.1018  d6.loss_cls: 0.6767  d6.loss_mask: 0.2124  d6.loss_dice: 2.0582  d7.loss_cls: 0.6675  d7.loss_mask: 0.2118  d7.loss_dice: 2.0601  d8.loss_cls: 0.6618  d8.loss_mask: 0.2104  d8.loss_dice: 2.0602
05/08 04:37:24 - mmengine - INFO - Iter(train) [ 4100/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:45:46  time: 1.5014  data_time: 0.1025  memory: 28930  grad_norm: 52.1700  loss: 30.5454  loss_cls: 0.6462  loss_mask: 0.2056  loss_dice: 2.0428  d0.loss_cls: 1.0532  d0.loss_mask: 0.2557  d0.loss_dice: 2.2490  d1.loss_cls: 0.7536  d1.loss_mask: 0.2398  d1.loss_dice: 2.3281  d2.loss_cls: 0.6680  d2.loss_mask: 0.2248  d2.loss_dice: 2.2267  d3.loss_cls: 0.6408  d3.loss_mask: 0.2153  d3.loss_dice: 2.1242  d4.loss_cls: 0.6333  d4.loss_mask: 0.2128  d4.loss_dice: 2.1134  d5.loss_cls: 0.6351  d5.loss_mask: 0.2097  d5.loss_dice: 2.1009  d6.loss_cls: 0.6582  d6.loss_mask: 0.2095  d6.loss_dice: 2.0606  d7.loss_cls: 0.6534  d7.loss_mask: 0.2089  d7.loss_dice: 2.0606  d8.loss_cls: 0.6515  d8.loss_mask: 0.2072  d8.loss_dice: 2.0565
05/08 04:38:36 - mmengine - INFO - Iter(train) [ 4150/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:44:09  time: 1.4225  data_time: 0.0578  memory: 30923  grad_norm: 51.7761  loss: 32.0451  loss_cls: 0.6578  loss_mask: 0.2167  loss_dice: 2.1548  d0.loss_cls: 1.0767  d0.loss_mask: 0.2692  d0.loss_dice: 2.4099  d1.loss_cls: 0.7817  d1.loss_mask: 0.2538  d1.loss_dice: 2.4715  d2.loss_cls: 0.6855  d2.loss_mask: 0.2385  d2.loss_dice: 2.3623  d3.loss_cls: 0.6500  d3.loss_mask: 0.2284  d3.loss_dice: 2.2460  d4.loss_cls: 0.6418  d4.loss_mask: 0.2256  d4.loss_dice: 2.2347  d5.loss_cls: 0.6463  d5.loss_mask: 0.2217  d5.loss_dice: 2.2151  d6.loss_cls: 0.6657  d6.loss_mask: 0.2211  d6.loss_dice: 2.1733  d7.loss_cls: 0.6604  d7.loss_mask: 0.2201  d7.loss_dice: 2.1720  d8.loss_cls: 0.6580  d8.loss_mask: 0.2180  d8.loss_dice: 2.1684
05/08 04:39:48 - mmengine - INFO - Iter(train) [ 4200/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:42:36  time: 1.4453  data_time: 0.0456  memory: 29717  grad_norm: 72.6091  loss: 32.3197  loss_cls: 0.6456  loss_mask: 0.2201  loss_dice: 2.1931  d0.loss_cls: 1.0637  d0.loss_mask: 0.2699  d0.loss_dice: 2.4275  d1.loss_cls: 0.7654  d1.loss_mask: 0.2530  d1.loss_dice: 2.5094  d2.loss_cls: 0.6801  d2.loss_mask: 0.2417  d2.loss_dice: 2.4009  d3.loss_cls: 0.6425  d3.loss_mask: 0.2319  d3.loss_dice: 2.2808  d4.loss_cls: 0.6337  d4.loss_mask: 0.2282  d4.loss_dice: 2.2689  d5.loss_cls: 0.6351  d5.loss_mask: 0.2264  d5.loss_dice: 2.2509  d6.loss_cls: 0.6561  d6.loss_mask: 0.2236  d6.loss_dice: 2.2127  d7.loss_cls: 0.6491  d7.loss_mask: 0.2230  d7.loss_dice: 2.2109  d8.loss_cls: 0.6459  d8.loss_mask: 0.2210  d8.loss_dice: 2.2089
05/08 04:41:00 - mmengine - INFO - Iter(train) [ 4250/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:41:04  time: 1.4411  data_time: 0.0470  memory: 29275  grad_norm: 59.2450  loss: 31.8878  loss_cls: 0.6471  loss_mask: 0.2181  loss_dice: 2.1505  d0.loss_cls: 1.0618  d0.loss_mask: 0.2711  d0.loss_dice: 2.3913  d1.loss_cls: 0.7694  d1.loss_mask: 0.2550  d1.loss_dice: 2.4660  d2.loss_cls: 0.6794  d2.loss_mask: 0.2404  d2.loss_dice: 2.3586  d3.loss_cls: 0.6421  d3.loss_mask: 0.2279  d3.loss_dice: 2.2407  d4.loss_cls: 0.6302  d4.loss_mask: 0.2251  d4.loss_dice: 2.2283  d5.loss_cls: 0.6339  d5.loss_mask: 0.2223  d5.loss_dice: 2.2141  d6.loss_cls: 0.6568  d6.loss_mask: 0.2206  d6.loss_dice: 2.1684  d7.loss_cls: 0.6491  d7.loss_mask: 0.2203  d7.loss_dice: 2.1690  d8.loss_cls: 0.6489  d8.loss_mask: 0.2192  d8.loss_dice: 2.1623
05/08 04:42:12 - mmengine - INFO - Iter(train) [ 4300/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:39:30  time: 1.4340  data_time: 0.0622  memory: 29597  grad_norm: 74.0169  loss: 31.2003  loss_cls: 0.6358  loss_mask: 0.2072  loss_dice: 2.1089  d0.loss_cls: 1.0443  d0.loss_mask: 0.2569  d0.loss_dice: 2.3357  d1.loss_cls: 0.7597  d1.loss_mask: 0.2467  d1.loss_dice: 2.4175  d2.loss_cls: 0.6688  d2.loss_mask: 0.2269  d2.loss_dice: 2.3151  d3.loss_cls: 0.6288  d3.loss_mask: 0.2199  d3.loss_dice: 2.1966  d4.loss_cls: 0.6196  d4.loss_mask: 0.2167  d4.loss_dice: 2.1833  d5.loss_cls: 0.6179  d5.loss_mask: 0.2134  d5.loss_dice: 2.1694  d6.loss_cls: 0.6391  d6.loss_mask: 0.2110  d6.loss_dice: 2.1286  d7.loss_cls: 0.6337  d7.loss_mask: 0.2101  d7.loss_dice: 2.1244  d8.loss_cls: 0.6332  d8.loss_mask: 0.2087  d8.loss_dice: 2.1224
05/08 04:43:24 - mmengine - INFO - Iter(train) [ 4350/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:38:00  time: 1.4520  data_time: 0.0480  memory: 29178  grad_norm: 57.5752  loss: 32.2103  loss_cls: 0.6417  loss_mask: 0.2100  loss_dice: 2.1873  d0.loss_cls: 1.0800  d0.loss_mask: 0.2587  d0.loss_dice: 2.4377  d1.loss_cls: 0.7642  d1.loss_mask: 0.2457  d1.loss_dice: 2.5320  d2.loss_cls: 0.6808  d2.loss_mask: 0.2315  d2.loss_dice: 2.4055  d3.loss_cls: 0.6411  d3.loss_mask: 0.2206  d3.loss_dice: 2.2829  d4.loss_cls: 0.6306  d4.loss_mask: 0.2173  d4.loss_dice: 2.2687  d5.loss_cls: 0.6331  d5.loss_mask: 0.2147  d5.loss_dice: 2.2530  d6.loss_cls: 0.6498  d6.loss_mask: 0.2130  d6.loss_dice: 2.1995  d7.loss_cls: 0.6425  d7.loss_mask: 0.2123  d7.loss_dice: 2.2044  d8.loss_cls: 0.6441  d8.loss_mask: 0.2106  d8.loss_dice: 2.1969
05/08 04:44:36 - mmengine - INFO - Iter(train) [ 4400/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:36:29  time: 1.4416  data_time: 0.0507  memory: 29393  grad_norm: 46.8495  loss: 30.9404  loss_cls: 0.6408  loss_mask: 0.1935  loss_dice: 2.0901  d0.loss_cls: 1.0658  d0.loss_mask: 0.2374  d0.loss_dice: 2.3043  d1.loss_cls: 0.7547  d1.loss_mask: 0.2266  d1.loss_dice: 2.4034  d2.loss_cls: 0.6708  d2.loss_mask: 0.2136  d2.loss_dice: 2.2979  d3.loss_cls: 0.6364  d3.loss_mask: 0.2052  d3.loss_dice: 2.1798  d4.loss_cls: 0.6292  d4.loss_mask: 0.2021  d4.loss_dice: 2.1610  d5.loss_cls: 0.6278  d5.loss_mask: 0.1998  d5.loss_dice: 2.1513  d6.loss_cls: 0.6534  d6.loss_mask: 0.1970  d6.loss_dice: 2.1092  d7.loss_cls: 0.6456  d7.loss_mask: 0.1960  d7.loss_dice: 2.1073  d8.loss_cls: 0.6431  d8.loss_mask: 0.1943  d8.loss_dice: 2.1030
05/08 04:45:48 - mmengine - INFO - Iter(train) [ 4450/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:34:57  time: 1.4315  data_time: 0.0461  memory: 29137  grad_norm: 63.6525  loss: 31.6020  loss_cls: 0.6450  loss_mask: 0.2043  loss_dice: 2.1393  d0.loss_cls: 1.0693  d0.loss_mask: 0.2531  d0.loss_dice: 2.3566  d1.loss_cls: 0.7582  d1.loss_mask: 0.2407  d1.loss_dice: 2.4620  d2.loss_cls: 0.6750  d2.loss_mask: 0.2270  d2.loss_dice: 2.3488  d3.loss_cls: 0.6422  d3.loss_mask: 0.2133  d3.loss_dice: 2.2287  d4.loss_cls: 0.6333  d4.loss_mask: 0.2113  d4.loss_dice: 2.2117  d5.loss_cls: 0.6362  d5.loss_mask: 0.2082  d5.loss_dice: 2.1979  d6.loss_cls: 0.6573  d6.loss_mask: 0.2077  d6.loss_dice: 2.1594  d7.loss_cls: 0.6510  d7.loss_mask: 0.2069  d7.loss_dice: 2.1541  d8.loss_cls: 0.6466  d8.loss_mask: 0.2053  d8.loss_dice: 2.1514
05/08 04:47:00 - mmengine - INFO - Iter(train) [ 4500/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:33:28  time: 1.4508  data_time: 0.0903  memory: 29463  grad_norm: 72.8228  loss: 31.9775  loss_cls: 0.6327  loss_mask: 0.2075  loss_dice: 2.1766  d0.loss_cls: 1.0773  d0.loss_mask: 0.2614  d0.loss_dice: 2.4169  d1.loss_cls: 0.7630  d1.loss_mask: 0.2469  d1.loss_dice: 2.5086  d2.loss_cls: 0.6691  d2.loss_mask: 0.2291  d2.loss_dice: 2.3971  d3.loss_cls: 0.6321  d3.loss_mask: 0.2201  d3.loss_dice: 2.2639  d4.loss_cls: 0.6238  d4.loss_mask: 0.2176  d4.loss_dice: 2.2479  d5.loss_cls: 0.6261  d5.loss_mask: 0.2130  d5.loss_dice: 2.2364  d6.loss_cls: 0.6353  d6.loss_mask: 0.2107  d6.loss_dice: 2.1959  d7.loss_cls: 0.6374  d7.loss_mask: 0.2102  d7.loss_dice: 2.1923  d8.loss_cls: 0.6346  d8.loss_mask: 0.2082  d8.loss_dice: 2.1860
05/08 04:48:13 - mmengine - INFO - Iter(train) [ 4550/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:31:59  time: 1.4467  data_time: 0.0462  memory: 28785  grad_norm: 52.8603  loss: 31.3834  loss_cls: 0.6362  loss_mask: 0.2092  loss_dice: 2.1077  d0.loss_cls: 1.0758  d0.loss_mask: 0.2599  d0.loss_dice: 2.3571  d1.loss_cls: 0.7733  d1.loss_mask: 0.2475  d1.loss_dice: 2.4432  d2.loss_cls: 0.6783  d2.loss_mask: 0.2311  d2.loss_dice: 2.3220  d3.loss_cls: 0.6380  d3.loss_mask: 0.2214  d3.loss_dice: 2.2001  d4.loss_cls: 0.6323  d4.loss_mask: 0.2177  d4.loss_dice: 2.1794  d5.loss_cls: 0.6324  d5.loss_mask: 0.2145  d5.loss_dice: 2.1714  d6.loss_cls: 0.6472  d6.loss_mask: 0.2118  d6.loss_dice: 2.1265  d7.loss_cls: 0.6412  d7.loss_mask: 0.2114  d7.loss_dice: 2.1234  d8.loss_cls: 0.6432  d8.loss_mask: 0.2102  d8.loss_dice: 2.1201
05/08 04:49:25 - mmengine - INFO - Iter(train) [ 4600/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:30:30  time: 1.4454  data_time: 0.0459  memory: 29951  grad_norm: 65.0559  loss: 30.3014  loss_cls: 0.6321  loss_mask: 0.2006  loss_dice: 2.0304  d0.loss_cls: 1.0493  d0.loss_mask: 0.2478  d0.loss_dice: 2.2413  d1.loss_cls: 0.7552  d1.loss_mask: 0.2349  d1.loss_dice: 2.3327  d2.loss_cls: 0.6670  d2.loss_mask: 0.2192  d2.loss_dice: 2.2295  d3.loss_cls: 0.6312  d3.loss_mask: 0.2112  d3.loss_dice: 2.1117  d4.loss_cls: 0.6210  d4.loss_mask: 0.2086  d4.loss_dice: 2.0969  d5.loss_cls: 0.6233  d5.loss_mask: 0.2055  d5.loss_dice: 2.0871  d6.loss_cls: 0.6436  d6.loss_mask: 0.2033  d6.loss_dice: 2.0469  d7.loss_cls: 0.6398  d7.loss_mask: 0.2026  d7.loss_dice: 2.0475  d8.loss_cls: 0.6388  d8.loss_mask: 0.2015  d8.loss_dice: 2.0406
05/08 04:50:37 - mmengine - INFO - Iter(train) [ 4650/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:29:01  time: 1.4437  data_time: 0.0505  memory: 29114  grad_norm: 61.7340  loss: 31.6217  loss_cls: 0.6584  loss_mask: 0.2056  loss_dice: 2.1302  d0.loss_cls: 1.0754  d0.loss_mask: 0.2511  d0.loss_dice: 2.3407  d1.loss_cls: 0.7780  d1.loss_mask: 0.2413  d1.loss_dice: 2.4469  d2.loss_cls: 0.6893  d2.loss_mask: 0.2255  d2.loss_dice: 2.3374  d3.loss_cls: 0.6575  d3.loss_mask: 0.2168  d3.loss_dice: 2.2130  d4.loss_cls: 0.6506  d4.loss_mask: 0.2129  d4.loss_dice: 2.1982  d5.loss_cls: 0.6514  d5.loss_mask: 0.2103  d5.loss_dice: 2.1828  d6.loss_cls: 0.6662  d6.loss_mask: 0.2090  d6.loss_dice: 2.1462  d7.loss_cls: 0.6610  d7.loss_mask: 0.2079  d7.loss_dice: 2.1497  d8.loss_cls: 0.6626  d8.loss_mask: 0.2063  d8.loss_dice: 2.1396
05/08 04:51:48 - mmengine - INFO - Iter(train) [ 4700/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:27:29  time: 1.4228  data_time: 0.0521  memory: 28344  grad_norm: 61.0011  loss: 29.8068  loss_cls: 0.6213  loss_mask: 0.1913  loss_dice: 1.9980  d0.loss_cls: 1.0513  d0.loss_mask: 0.2357  d0.loss_dice: 2.2142  d1.loss_cls: 0.7442  d1.loss_mask: 0.2285  d1.loss_dice: 2.2975  d2.loss_cls: 0.6638  d2.loss_mask: 0.2135  d2.loss_dice: 2.1915  d3.loss_cls: 0.6211  d3.loss_mask: 0.2028  d3.loss_dice: 2.0837  d4.loss_cls: 0.6130  d4.loss_mask: 0.2002  d4.loss_dice: 2.0692  d5.loss_cls: 0.6148  d5.loss_mask: 0.1952  d5.loss_dice: 2.0580  d6.loss_cls: 0.6284  d6.loss_mask: 0.1944  d6.loss_dice: 2.0159  d7.loss_cls: 0.6218  d7.loss_mask: 0.1936  d7.loss_dice: 2.0171  d8.loss_cls: 0.6266  d8.loss_mask: 0.1925  d8.loss_dice: 2.0076
05/08 04:53:00 - mmengine - INFO - Iter(train) [ 4750/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:26:00  time: 1.4401  data_time: 0.0486  memory: 29902  grad_norm: 124.2339  loss: 31.9885  loss_cls: 0.6469  loss_mask: 0.2092  loss_dice: 2.1667  d0.loss_cls: 1.0808  d0.loss_mask: 0.2577  d0.loss_dice: 2.4151  d1.loss_cls: 0.7636  d1.loss_mask: 0.2448  d1.loss_dice: 2.4997  d2.loss_cls: 0.6808  d2.loss_mask: 0.2269  d2.loss_dice: 2.3820  d3.loss_cls: 0.6398  d3.loss_mask: 0.2196  d3.loss_dice: 2.2644  d4.loss_cls: 0.6317  d4.loss_mask: 0.2170  d4.loss_dice: 2.2465  d5.loss_cls: 0.6333  d5.loss_mask: 0.2139  d5.loss_dice: 2.2314  d6.loss_cls: 0.6460  d6.loss_mask: 0.2122  d6.loss_dice: 2.1862  d7.loss_cls: 0.6406  d7.loss_mask: 0.2113  d7.loss_dice: 2.1855  d8.loss_cls: 0.6451  d8.loss_mask: 0.2103  d8.loss_dice: 2.1795
05/08 04:54:13 - mmengine - INFO - Iter(train) [ 4800/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:24:33  time: 1.4455  data_time: 0.0515  memory: 29506  grad_norm: 54.7397  loss: 31.9342  loss_cls: 0.6511  loss_mask: 0.2078  loss_dice: 2.1641  d0.loss_cls: 1.0739  d0.loss_mask: 0.2536  d0.loss_dice: 2.3739  d1.loss_cls: 0.7687  d1.loss_mask: 0.2396  d1.loss_dice: 2.4842  d2.loss_cls: 0.6873  d2.loss_mask: 0.2290  d2.loss_dice: 2.3698  d3.loss_cls: 0.6484  d3.loss_mask: 0.2192  d3.loss_dice: 2.2528  d4.loss_cls: 0.6433  d4.loss_mask: 0.2164  d4.loss_dice: 2.2327  d5.loss_cls: 0.6462  d5.loss_mask: 0.2143  d5.loss_dice: 2.2175  d6.loss_cls: 0.6605  d6.loss_mask: 0.2112  d6.loss_dice: 2.1810  d7.loss_cls: 0.6527  d7.loss_mask: 0.2098  d7.loss_dice: 2.1833  d8.loss_cls: 0.6592  d8.loss_mask: 0.2081  d8.loss_dice: 2.1748
05/08 04:55:24 - mmengine - INFO - Iter(train) [ 4850/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:23:04  time: 1.4371  data_time: 0.0480  memory: 28692  grad_norm: 74.8360  loss: 31.4573  loss_cls: 0.6443  loss_mask: 0.1963  loss_dice: 2.1254  d0.loss_cls: 1.0695  d0.loss_mask: 0.2430  d0.loss_dice: 2.3507  d1.loss_cls: 0.7845  d1.loss_mask: 0.2328  d1.loss_dice: 2.4537  d2.loss_cls: 0.6925  d2.loss_mask: 0.2177  d2.loss_dice: 2.3369  d3.loss_cls: 0.6461  d3.loss_mask: 0.2080  d3.loss_dice: 2.2188  d4.loss_cls: 0.6349  d4.loss_mask: 0.2044  d4.loss_dice: 2.2012  d5.loss_cls: 0.6317  d5.loss_mask: 0.2028  d5.loss_dice: 2.1894  d6.loss_cls: 0.6520  d6.loss_mask: 0.2002  d6.loss_dice: 2.1438  d7.loss_cls: 0.6448  d7.loss_mask: 0.1991  d7.loss_dice: 2.1471  d8.loss_cls: 0.6467  d8.loss_mask: 0.1970  d8.loss_dice: 2.1417
05/08 04:56:38 - mmengine - INFO - Iter(train) [ 4900/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:21:40  time: 1.4658  data_time: 0.1052  memory: 29483  grad_norm: 57.1714  loss: 31.4828  loss_cls: 0.6568  loss_mask: 0.2114  loss_dice: 2.1061  d0.loss_cls: 1.0676  d0.loss_mask: 0.2622  d0.loss_dice: 2.3377  d1.loss_cls: 0.7713  d1.loss_mask: 0.2490  d1.loss_dice: 2.4272  d2.loss_cls: 0.6904  d2.loss_mask: 0.2335  d2.loss_dice: 2.3081  d3.loss_cls: 0.6561  d3.loss_mask: 0.2236  d3.loss_dice: 2.1997  d4.loss_cls: 0.6523  d4.loss_mask: 0.2213  d4.loss_dice: 2.1789  d5.loss_cls: 0.6565  d5.loss_mask: 0.2178  d5.loss_dice: 2.1660  d6.loss_cls: 0.6618  d6.loss_mask: 0.2145  d6.loss_dice: 2.1277  d7.loss_cls: 0.6570  d7.loss_mask: 0.2136  d7.loss_dice: 2.1259  d8.loss_cls: 0.6590  d8.loss_mask: 0.2125  d8.loss_dice: 2.1173
05/08 04:57:49 - mmengine - INFO - Iter(train) [ 4950/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:20:12  time: 1.4352  data_time: 0.0581  memory: 28476  grad_norm: 61.5289  loss: 30.4922  loss_cls: 0.6423  loss_mask: 0.2007  loss_dice: 2.0370  d0.loss_cls: 1.0570  d0.loss_mask: 0.2459  d0.loss_dice: 2.2376  d1.loss_cls: 0.7645  d1.loss_mask: 0.2355  d1.loss_dice: 2.3353  d2.loss_cls: 0.6725  d2.loss_mask: 0.2203  d2.loss_dice: 2.2339  d3.loss_cls: 0.6517  d3.loss_mask: 0.2123  d3.loss_dice: 2.1203  d4.loss_cls: 0.6451  d4.loss_mask: 0.2084  d4.loss_dice: 2.1106  d5.loss_cls: 0.6440  d5.loss_mask: 0.2051  d5.loss_dice: 2.0981  d6.loss_cls: 0.6539  d6.loss_mask: 0.2028  d6.loss_dice: 2.0564  d7.loss_cls: 0.6434  d7.loss_mask: 0.2031  d7.loss_dice: 2.0589  d8.loss_cls: 0.6432  d8.loss_mask: 0.2016  d8.loss_dice: 2.0506
05/08 04:59:02 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 04:59:02 - mmengine - INFO - Iter(train) [ 5000/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:18:47  time: 1.4570  data_time: 0.0670  memory: 28946  grad_norm: 47.8820  loss: 31.4921  loss_cls: 0.6507  loss_mask: 0.2123  loss_dice: 2.1158  d0.loss_cls: 1.0742  d0.loss_mask: 0.2583  d0.loss_dice: 2.3129  d1.loss_cls: 0.7769  d1.loss_mask: 0.2455  d1.loss_dice: 2.4177  d2.loss_cls: 0.6823  d2.loss_mask: 0.2339  d2.loss_dice: 2.3153  d3.loss_cls: 0.6579  d3.loss_mask: 0.2251  d3.loss_dice: 2.1954  d4.loss_cls: 0.6587  d4.loss_mask: 0.2206  d4.loss_dice: 2.1833  d5.loss_cls: 0.6507  d5.loss_mask: 0.2166  d5.loss_dice: 2.1757  d6.loss_cls: 0.6640  d6.loss_mask: 0.2164  d6.loss_dice: 2.1370  d7.loss_cls: 0.6540  d7.loss_mask: 0.2153  d7.loss_dice: 2.1340  d8.loss_cls: 0.6521  d8.loss_mask: 0.2127  d8.loss_dice: 2.1270
05/08 04:59:02 - mmengine - INFO - Saving checkpoint at 5000 iterations
05/08 04:59:54 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9832  data_time: 0.0253  memory: 3258  
05/08 05:00:18 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.45s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 05:00:25 - mmengine - INFO - start multi processing evaluation ...
DONE (t=53.38s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.372
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.722
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.341
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.204
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.450
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.841
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.498
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.898
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.477
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.349
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.570
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.917
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.498
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.498
05/08 05:01:19 - mmengine - INFO - segm_mAP_copypaste: 0.372 0.722 0.341 0.204 0.450 0.841
05/08 05:01:20 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.3720  coco/segm_mAP_50: 0.7220  coco/segm_mAP_75: 0.3410  coco/segm_mAP_s: 0.2040  coco/segm_mAP_m: 0.4500  coco/segm_mAP_l: 0.8410  data_time: 0.0252  time: 0.9809
05/08 05:02:30 - mmengine - INFO - Iter(train) [ 5050/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:21:27  time: 3.1037  data_time: 1.7579  memory: 28122  grad_norm: 48.7927  loss: 30.7436  loss_cls: 0.6222  loss_mask: 0.2131  loss_dice: 2.0698  d0.loss_cls: 1.0669  d0.loss_mask: 0.2601  d0.loss_dice: 2.2665  d1.loss_cls: 0.7520  d1.loss_mask: 0.2493  d1.loss_dice: 2.3584  d2.loss_cls: 0.6568  d2.loss_mask: 0.2334  d2.loss_dice: 2.2629  d3.loss_cls: 0.6311  d3.loss_mask: 0.2256  d3.loss_dice: 2.1464  d4.loss_cls: 0.6260  d4.loss_mask: 0.2222  d4.loss_dice: 2.1346  d5.loss_cls: 0.6245  d5.loss_mask: 0.2186  d5.loss_dice: 2.1237  d6.loss_cls: 0.6379  d6.loss_mask: 0.2168  d6.loss_dice: 2.0829  d7.loss_cls: 0.6264  d7.loss_mask: 0.2155  d7.loss_dice: 2.0832  d8.loss_cls: 0.6234  d8.loss_mask: 0.2139  d8.loss_dice: 2.0794
05/08 05:03:41 - mmengine - INFO - Iter(train) [ 5100/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:19:57  time: 1.4391  data_time: 0.0528  memory: 28393  grad_norm: 53.6940  loss: 31.6698  loss_cls: 0.6471  loss_mask: 0.2115  loss_dice: 2.1297  d0.loss_cls: 1.0827  d0.loss_mask: 0.2572  d0.loss_dice: 2.3524  d1.loss_cls: 0.7779  d1.loss_mask: 0.2515  d1.loss_dice: 2.4437  d2.loss_cls: 0.6831  d2.loss_mask: 0.2307  d2.loss_dice: 2.3360  d3.loss_cls: 0.6556  d3.loss_mask: 0.2229  d3.loss_dice: 2.2225  d4.loss_cls: 0.6448  d4.loss_mask: 0.2192  d4.loss_dice: 2.2094  d5.loss_cls: 0.6422  d5.loss_mask: 0.2173  d5.loss_dice: 2.1931  d6.loss_cls: 0.6593  d6.loss_mask: 0.2157  d6.loss_dice: 2.1464  d7.loss_cls: 0.6518  d7.loss_mask: 0.2136  d7.loss_dice: 2.1486  d8.loss_cls: 0.6499  d8.loss_mask: 0.2122  d8.loss_dice: 2.1418
05/08 05:04:52 - mmengine - INFO - Iter(train) [ 5150/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:18:24  time: 1.4161  data_time: 0.0576  memory: 29623  grad_norm: 42.8549  loss: 31.7819  loss_cls: 0.6356  loss_mask: 0.2069  loss_dice: 2.1608  d0.loss_cls: 1.0690  d0.loss_mask: 0.2541  d0.loss_dice: 2.3691  d1.loss_cls: 0.7595  d1.loss_mask: 0.2419  d1.loss_dice: 2.4765  d2.loss_cls: 0.6737  d2.loss_mask: 0.2295  d2.loss_dice: 2.3644  d3.loss_cls: 0.6423  d3.loss_mask: 0.2183  d3.loss_dice: 2.2460  d4.loss_cls: 0.6356  d4.loss_mask: 0.2162  d4.loss_dice: 2.2377  d5.loss_cls: 0.6357  d5.loss_mask: 0.2131  d5.loss_dice: 2.2220  d6.loss_cls: 0.6464  d6.loss_mask: 0.2096  d6.loss_dice: 2.1737  d7.loss_cls: 0.6357  d7.loss_mask: 0.2088  d7.loss_dice: 2.1795  d8.loss_cls: 0.6369  d8.loss_mask: 0.2080  d8.loss_dice: 2.1753
05/08 05:06:04 - mmengine - INFO - Iter(train) [ 5200/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:16:55  time: 1.4399  data_time: 0.0590  memory: 28830  grad_norm: 52.4837  loss: 30.2853  loss_cls: 0.6401  loss_mask: 0.2002  loss_dice: 2.0255  d0.loss_cls: 1.0625  d0.loss_mask: 0.2491  d0.loss_dice: 2.2277  d1.loss_cls: 0.7580  d1.loss_mask: 0.2402  d1.loss_dice: 2.3198  d2.loss_cls: 0.6687  d2.loss_mask: 0.2217  d2.loss_dice: 2.2105  d3.loss_cls: 0.6400  d3.loss_mask: 0.2125  d3.loss_dice: 2.0968  d4.loss_cls: 0.6314  d4.loss_mask: 0.2101  d4.loss_dice: 2.0924  d5.loss_cls: 0.6372  d5.loss_mask: 0.2058  d5.loss_dice: 2.0758  d6.loss_cls: 0.6541  d6.loss_mask: 0.2043  d6.loss_dice: 2.0360  d7.loss_cls: 0.6437  d7.loss_mask: 0.2029  d7.loss_dice: 2.0391  d8.loss_cls: 0.6415  d8.loss_mask: 0.2017  d8.loss_dice: 2.0356
05/08 05:07:15 - mmengine - INFO - Iter(train) [ 5250/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:15:24  time: 1.4229  data_time: 0.0575  memory: 29575  grad_norm: 55.9198  loss: 32.7711  loss_cls: 0.6609  loss_mask: 0.2196  loss_dice: 2.2210  d0.loss_cls: 1.0888  d0.loss_mask: 0.2655  d0.loss_dice: 2.4438  d1.loss_cls: 0.7747  d1.loss_mask: 0.2589  d1.loss_dice: 2.5409  d2.loss_cls: 0.6863  d2.loss_mask: 0.2381  d2.loss_dice: 2.4357  d3.loss_cls: 0.6648  d3.loss_mask: 0.2309  d3.loss_dice: 2.3108  d4.loss_cls: 0.6542  d4.loss_mask: 0.2273  d4.loss_dice: 2.3000  d5.loss_cls: 0.6645  d5.loss_mask: 0.2239  d5.loss_dice: 2.2776  d6.loss_cls: 0.6736  d6.loss_mask: 0.2227  d6.loss_dice: 2.2386  d7.loss_cls: 0.6649  d7.loss_mask: 0.2221  d7.loss_dice: 2.2406  d8.loss_cls: 0.6662  d8.loss_mask: 0.2214  d8.loss_dice: 2.2330
05/08 05:08:29 - mmengine - INFO - Iter(train) [ 5300/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:13:59  time: 1.4677  data_time: 0.0994  memory: 29737  grad_norm: 55.5825  loss: 33.3040  loss_cls: 0.6486  loss_mask: 0.2115  loss_dice: 2.2887  d0.loss_cls: 1.0992  d0.loss_mask: 0.2591  d0.loss_dice: 2.5269  d1.loss_cls: 0.7731  d1.loss_mask: 0.2480  d1.loss_dice: 2.6251  d2.loss_cls: 0.6860  d2.loss_mask: 0.2336  d2.loss_dice: 2.5064  d3.loss_cls: 0.6536  d3.loss_mask: 0.2215  d3.loss_dice: 2.3747  d4.loss_cls: 0.6443  d4.loss_mask: 0.2189  d4.loss_dice: 2.3615  d5.loss_cls: 0.6470  d5.loss_mask: 0.2185  d5.loss_dice: 2.3487  d6.loss_cls: 0.6610  d6.loss_mask: 0.2133  d6.loss_dice: 2.3047  d7.loss_cls: 0.6516  d7.loss_mask: 0.2127  d7.loss_dice: 2.3069  d8.loss_cls: 0.6493  d8.loss_mask: 0.2118  d8.loss_dice: 2.2976
05/08 05:09:41 - mmengine - INFO - Iter(train) [ 5350/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:12:33  time: 1.4530  data_time: 0.0553  memory: 28665  grad_norm: 58.4598  loss: 29.9426  loss_cls: 0.6436  loss_mask: 0.2022  loss_dice: 1.9775  d0.loss_cls: 1.0747  d0.loss_mask: 0.2522  d0.loss_dice: 2.1968  d1.loss_cls: 0.7596  d1.loss_mask: 0.2372  d1.loss_dice: 2.2911  d2.loss_cls: 0.6765  d2.loss_mask: 0.2242  d2.loss_dice: 2.1731  d3.loss_cls: 0.6449  d3.loss_mask: 0.2126  d3.loss_dice: 2.0597  d4.loss_cls: 0.6380  d4.loss_mask: 0.2110  d4.loss_dice: 2.0463  d5.loss_cls: 0.6442  d5.loss_mask: 0.2087  d5.loss_dice: 2.0276  d6.loss_cls: 0.6597  d6.loss_mask: 0.2059  d6.loss_dice: 1.9929  d7.loss_cls: 0.6514  d7.loss_mask: 0.2042  d7.loss_dice: 1.9894  d8.loss_cls: 0.6478  d8.loss_mask: 0.2032  d8.loss_dice: 1.9865
05/08 05:10:54 - mmengine - INFO - Iter(train) [ 5400/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:11:07  time: 1.4531  data_time: 0.0629  memory: 28928  grad_norm: 67.0235  loss: 31.1536  loss_cls: 0.6373  loss_mask: 0.1983  loss_dice: 2.1144  d0.loss_cls: 1.0695  d0.loss_mask: 0.2450  d0.loss_dice: 2.3135  d1.loss_cls: 0.7496  d1.loss_mask: 0.2333  d1.loss_dice: 2.4168  d2.loss_cls: 0.6691  d2.loss_mask: 0.2206  d2.loss_dice: 2.3070  d3.loss_cls: 0.6332  d3.loss_mask: 0.2099  d3.loss_dice: 2.1995  d4.loss_cls: 0.6276  d4.loss_mask: 0.2078  d4.loss_dice: 2.1855  d5.loss_cls: 0.6338  d5.loss_mask: 0.2036  d5.loss_dice: 2.1702  d6.loss_cls: 0.6491  d6.loss_mask: 0.2016  d6.loss_dice: 2.1285  d7.loss_cls: 0.6391  d7.loss_mask: 0.2003  d7.loss_dice: 2.1277  d8.loss_cls: 0.6399  d8.loss_mask: 0.1991  d8.loss_dice: 2.1227
05/08 05:12:07 - mmengine - INFO - Iter(train) [ 5450/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:09:41  time: 1.4520  data_time: 0.0612  memory: 29071  grad_norm: 59.6293  loss: 31.9603  loss_cls: 0.6500  loss_mask: 0.2042  loss_dice: 2.1607  d0.loss_cls: 1.0924  d0.loss_mask: 0.2527  d0.loss_dice: 2.3995  d1.loss_cls: 0.7794  d1.loss_mask: 0.2414  d1.loss_dice: 2.4853  d2.loss_cls: 0.6826  d2.loss_mask: 0.2278  d2.loss_dice: 2.3702  d3.loss_cls: 0.6465  d3.loss_mask: 0.2167  d3.loss_dice: 2.2457  d4.loss_cls: 0.6421  d4.loss_mask: 0.2139  d4.loss_dice: 2.2367  d5.loss_cls: 0.6482  d5.loss_mask: 0.2107  d5.loss_dice: 2.2232  d6.loss_cls: 0.6667  d6.loss_mask: 0.2076  d6.loss_dice: 2.1796  d7.loss_cls: 0.6535  d7.loss_mask: 0.2066  d7.loss_dice: 2.1834  d8.loss_cls: 0.6543  d8.loss_mask: 0.2048  d8.loss_dice: 2.1739
05/08 05:13:19 - mmengine - INFO - Iter(train) [ 5500/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:08:15  time: 1.4504  data_time: 0.0585  memory: 28952  grad_norm: 52.4745  loss: 30.8164  loss_cls: 0.6344  loss_mask: 0.2066  loss_dice: 2.0666  d0.loss_cls: 1.0769  d0.loss_mask: 0.2552  d0.loss_dice: 2.2737  d1.loss_cls: 0.7602  d1.loss_mask: 0.2423  d1.loss_dice: 2.3839  d2.loss_cls: 0.6708  d2.loss_mask: 0.2240  d2.loss_dice: 2.2711  d3.loss_cls: 0.6305  d3.loss_mask: 0.2174  d3.loss_dice: 2.1558  d4.loss_cls: 0.6212  d4.loss_mask: 0.2151  d4.loss_dice: 2.1432  d5.loss_cls: 0.6281  d5.loss_mask: 0.2116  d5.loss_dice: 2.1269  d6.loss_cls: 0.6475  d6.loss_mask: 0.2106  d6.loss_dice: 2.0851  d7.loss_cls: 0.6378  d7.loss_mask: 0.2090  d7.loss_dice: 2.0852  d8.loss_cls: 0.6356  d8.loss_mask: 0.2081  d8.loss_dice: 2.0819
05/08 05:14:31 - mmengine - INFO - Iter(train) [ 5550/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:06:47  time: 1.4329  data_time: 0.0556  memory: 28712  grad_norm: 61.2873  loss: 31.6458  loss_cls: 0.6403  loss_mask: 0.2007  loss_dice: 2.1536  d0.loss_cls: 1.0826  d0.loss_mask: 0.2440  d0.loss_dice: 2.3520  d1.loss_cls: 0.7711  d1.loss_mask: 0.2380  d1.loss_dice: 2.4582  d2.loss_cls: 0.6736  d2.loss_mask: 0.2207  d2.loss_dice: 2.3500  d3.loss_cls: 0.6305  d3.loss_mask: 0.2116  d3.loss_dice: 2.2413  d4.loss_cls: 0.6289  d4.loss_mask: 0.2077  d4.loss_dice: 2.2222  d5.loss_cls: 0.6325  d5.loss_mask: 0.2045  d5.loss_dice: 2.2159  d6.loss_cls: 0.6528  d6.loss_mask: 0.2039  d6.loss_dice: 2.1755  d7.loss_cls: 0.6414  d7.loss_mask: 0.2032  d7.loss_dice: 2.1759  d8.loss_cls: 0.6398  d8.loss_mask: 0.2016  d8.loss_dice: 2.1718
05/08 05:15:44 - mmengine - INFO - Iter(train) [ 5600/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:05:22  time: 1.4611  data_time: 0.0544  memory: 29903  grad_norm: 59.5599  loss: 30.5827  loss_cls: 0.6468  loss_mask: 0.2017  loss_dice: 2.0471  d0.loss_cls: 1.0778  d0.loss_mask: 0.2437  d0.loss_dice: 2.2537  d1.loss_cls: 0.7656  d1.loss_mask: 0.2350  d1.loss_dice: 2.3287  d2.loss_cls: 0.6713  d2.loss_mask: 0.2186  d2.loss_dice: 2.2314  d3.loss_cls: 0.6403  d3.loss_mask: 0.2083  d3.loss_dice: 2.1286  d4.loss_cls: 0.6366  d4.loss_mask: 0.2079  d4.loss_dice: 2.1161  d5.loss_cls: 0.6459  d5.loss_mask: 0.2067  d5.loss_dice: 2.1053  d6.loss_cls: 0.6613  d6.loss_mask: 0.2045  d6.loss_dice: 2.0693  d7.loss_cls: 0.6493  d7.loss_mask: 0.2035  d7.loss_dice: 2.0662  d8.loss_cls: 0.6407  d8.loss_mask: 0.2020  d8.loss_dice: 2.0686
05/08 05:16:57 - mmengine - INFO - Iter(train) [ 5650/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:03:58  time: 1.4545  data_time: 0.0617  memory: 28673  grad_norm: 58.5042  loss: 32.2585  loss_cls: 0.6635  loss_mask: 0.2123  loss_dice: 2.1815  d0.loss_cls: 1.0710  d0.loss_mask: 0.2554  d0.loss_dice: 2.3992  d1.loss_cls: 0.7751  d1.loss_mask: 0.2486  d1.loss_dice: 2.4942  d2.loss_cls: 0.6769  d2.loss_mask: 0.2303  d2.loss_dice: 2.3812  d3.loss_cls: 0.6431  d3.loss_mask: 0.2228  d3.loss_dice: 2.2733  d4.loss_cls: 0.6401  d4.loss_mask: 0.2201  d4.loss_dice: 2.2591  d5.loss_cls: 0.6508  d5.loss_mask: 0.2195  d5.loss_dice: 2.2492  d6.loss_cls: 0.6837  d6.loss_mask: 0.2186  d6.loss_dice: 2.2150  d7.loss_cls: 0.6704  d7.loss_mask: 0.2159  d7.loss_dice: 2.2123  d8.loss_cls: 0.6561  d8.loss_mask: 0.2138  d8.loss_dice: 2.2055
05/08 05:18:10 - mmengine - INFO - Iter(train) [ 5700/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:02:34  time: 1.4598  data_time: 0.1018  memory: 28637  grad_norm: 63.3059  loss: 31.9512  loss_cls: 0.6503  loss_mask: 0.2077  loss_dice: 2.1590  d0.loss_cls: 1.0846  d0.loss_mask: 0.2531  d0.loss_dice: 2.3749  d1.loss_cls: 0.7657  d1.loss_mask: 0.2460  d1.loss_dice: 2.4734  d2.loss_cls: 0.6734  d2.loss_mask: 0.2286  d2.loss_dice: 2.3674  d3.loss_cls: 0.6377  d3.loss_mask: 0.2188  d3.loss_dice: 2.2582  d4.loss_cls: 0.6360  d4.loss_mask: 0.2168  d4.loss_dice: 2.2411  d5.loss_cls: 0.6403  d5.loss_mask: 0.2164  d5.loss_dice: 2.2345  d6.loss_cls: 0.6703  d6.loss_mask: 0.2136  d6.loss_dice: 2.1891  d7.loss_cls: 0.6550  d7.loss_mask: 0.2116  d7.loss_dice: 2.1905  d8.loss_cls: 0.6462  d8.loss_mask: 0.2100  d8.loss_dice: 2.1812
05/08 05:19:22 - mmengine - INFO - Iter(train) [ 5750/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 6:01:10  time: 1.4564  data_time: 0.0672  memory: 28383  grad_norm: 62.5344  loss: 31.2674  loss_cls: 0.6481  loss_mask: 0.2026  loss_dice: 2.1041  d0.loss_cls: 1.0705  d0.loss_mask: 0.2502  d0.loss_dice: 2.3078  d1.loss_cls: 0.7596  d1.loss_mask: 0.2409  d1.loss_dice: 2.4107  d2.loss_cls: 0.6741  d2.loss_mask: 0.2226  d2.loss_dice: 2.3001  d3.loss_cls: 0.6486  d3.loss_mask: 0.2133  d3.loss_dice: 2.1879  d4.loss_cls: 0.6483  d4.loss_mask: 0.2092  d4.loss_dice: 2.1770  d5.loss_cls: 0.6439  d5.loss_mask: 0.2087  d5.loss_dice: 2.1741  d6.loss_cls: 0.6642  d6.loss_mask: 0.2063  d6.loss_dice: 2.1333  d7.loss_cls: 0.6518  d7.loss_mask: 0.2048  d7.loss_dice: 2.1311  d8.loss_cls: 0.6410  d8.loss_mask: 0.2043  d8.loss_dice: 2.1283
05/08 05:20:36 - mmengine - INFO - Iter(train) [ 5800/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:59:48  time: 1.4707  data_time: 0.0564  memory: 29168  grad_norm: 70.9538  loss: 29.6551  loss_cls: 0.6440  loss_mask: 0.1983  loss_dice: 1.9527  d0.loss_cls: 1.0718  d0.loss_mask: 0.2440  d0.loss_dice: 2.1651  d1.loss_cls: 0.7595  d1.loss_mask: 0.2333  d1.loss_dice: 2.2633  d2.loss_cls: 0.6740  d2.loss_mask: 0.2190  d2.loss_dice: 2.1442  d3.loss_cls: 0.6414  d3.loss_mask: 0.2093  d3.loss_dice: 2.0335  d4.loss_cls: 0.6452  d4.loss_mask: 0.2045  d4.loss_dice: 2.0206  d5.loss_cls: 0.6398  d5.loss_mask: 0.2020  d5.loss_dice: 2.0130  d6.loss_cls: 0.6594  d6.loss_mask: 0.2007  d6.loss_dice: 1.9791  d7.loss_cls: 0.6449  d7.loss_mask: 0.2002  d7.loss_dice: 1.9815  d8.loss_cls: 0.6366  d8.loss_mask: 0.1991  d8.loss_dice: 1.9752
05/08 05:21:47 - mmengine - INFO - Iter(train) [ 5850/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:58:20  time: 1.4290  data_time: 0.0567  memory: 29071  grad_norm: 52.5639  loss: 30.6561  loss_cls: 0.6373  loss_mask: 0.2063  loss_dice: 2.0472  d0.loss_cls: 1.0777  d0.loss_mask: 0.2529  d0.loss_dice: 2.2650  d1.loss_cls: 0.7620  d1.loss_mask: 0.2462  d1.loss_dice: 2.3507  d2.loss_cls: 0.6623  d2.loss_mask: 0.2304  d2.loss_dice: 2.2413  d3.loss_cls: 0.6466  d3.loss_mask: 0.2197  d3.loss_dice: 2.1243  d4.loss_cls: 0.6471  d4.loss_mask: 0.2164  d4.loss_dice: 2.1119  d5.loss_cls: 0.6386  d5.loss_mask: 0.2120  d5.loss_dice: 2.1122  d6.loss_cls: 0.6515  d6.loss_mask: 0.2105  d6.loss_dice: 2.0691  d7.loss_cls: 0.6342  d7.loss_mask: 0.2096  d7.loss_dice: 2.0703  d8.loss_cls: 0.6283  d8.loss_mask: 0.2079  d8.loss_dice: 2.0670
05/08 05:23:00 - mmengine - INFO - Iter(train) [ 5900/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:56:56  time: 1.4456  data_time: 0.0623  memory: 28610  grad_norm: 54.1343  loss: 29.6341  loss_cls: 0.6341  loss_mask: 0.2000  loss_dice: 1.9636  d0.loss_cls: 1.0683  d0.loss_mask: 0.2435  d0.loss_dice: 2.1688  d1.loss_cls: 0.7574  d1.loss_mask: 0.2325  d1.loss_dice: 2.2609  d2.loss_cls: 0.6559  d2.loss_mask: 0.2158  d2.loss_dice: 2.1544  d3.loss_cls: 0.6346  d3.loss_mask: 0.2089  d3.loss_dice: 2.0394  d4.loss_cls: 0.6361  d4.loss_mask: 0.2066  d4.loss_dice: 2.0321  d5.loss_cls: 0.6325  d5.loss_mask: 0.2051  d5.loss_dice: 2.0222  d6.loss_cls: 0.6421  d6.loss_mask: 0.2029  d6.loss_dice: 1.9872  d7.loss_cls: 0.6321  d7.loss_mask: 0.2027  d7.loss_dice: 1.9835  d8.loss_cls: 0.6264  d8.loss_mask: 0.2007  d8.loss_dice: 1.9838
05/08 05:24:12 - mmengine - INFO - Iter(train) [ 5950/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:55:32  time: 1.4529  data_time: 0.0626  memory: 29305  grad_norm: 48.6168  loss: 30.3060  loss_cls: 0.6521  loss_mask: 0.1927  loss_dice: 2.0198  d0.loss_cls: 1.0804  d0.loss_mask: 0.2372  d0.loss_dice: 2.2150  d1.loss_cls: 0.7634  d1.loss_mask: 0.2254  d1.loss_dice: 2.3327  d2.loss_cls: 0.6853  d2.loss_mask: 0.2106  d2.loss_dice: 2.2108  d3.loss_cls: 0.6561  d3.loss_mask: 0.2023  d3.loss_dice: 2.0993  d4.loss_cls: 0.6546  d4.loss_mask: 0.2001  d4.loss_dice: 2.0875  d5.loss_cls: 0.6463  d5.loss_mask: 0.1965  d5.loss_dice: 2.0846  d6.loss_cls: 0.6603  d6.loss_mask: 0.1954  d6.loss_dice: 2.0407  d7.loss_cls: 0.6465  d7.loss_mask: 0.1949  d7.loss_dice: 2.0420  d8.loss_cls: 0.6485  d8.loss_mask: 0.1931  d8.loss_dice: 2.0317
05/08 05:25:24 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 05:25:24 - mmengine - INFO - Iter(train) [ 6000/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:54:06  time: 1.4312  data_time: 0.0571  memory: 29237  grad_norm: 60.9957  loss: 31.0790  loss_cls: 0.6533  loss_mask: 0.2113  loss_dice: 2.0824  d0.loss_cls: 1.0747  d0.loss_mask: 0.2599  d0.loss_dice: 2.2903  d1.loss_cls: 0.7563  d1.loss_mask: 0.2433  d1.loss_dice: 2.3912  d2.loss_cls: 0.6711  d2.loss_mask: 0.2310  d2.loss_dice: 2.2806  d3.loss_cls: 0.6413  d3.loss_mask: 0.2236  d3.loss_dice: 2.1677  d4.loss_cls: 0.6362  d4.loss_mask: 0.2198  d4.loss_dice: 2.1486  d5.loss_cls: 0.6360  d5.loss_mask: 0.2170  d5.loss_dice: 2.1471  d6.loss_cls: 0.6562  d6.loss_mask: 0.2160  d6.loss_dice: 2.1059  d7.loss_cls: 0.6465  d7.loss_mask: 0.2133  d7.loss_dice: 2.1034  d8.loss_cls: 0.6468  d8.loss_mask: 0.2118  d8.loss_dice: 2.0964
05/08 05:25:24 - mmengine - INFO - Saving checkpoint at 6000 iterations
05/08 05:26:16 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9880  data_time: 0.0261  memory: 3258  
05/08 05:26:39 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.56s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 05:26:47 - mmengine - INFO - start multi processing evaluation ...
DONE (t=54.25s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.391
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.766
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.357
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.465
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.831
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.503
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.914
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.477
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.361
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.570
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.917
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.503
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.503
05/08 05:27:41 - mmengine - INFO - segm_mAP_copypaste: 0.391 0.766 0.357 0.227 0.465 0.831
05/08 05:27:42 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.3910  coco/segm_mAP_50: 0.7660  coco/segm_mAP_75: 0.3570  coco/segm_mAP_s: 0.2270  coco/segm_mAP_m: 0.4650  coco/segm_mAP_l: 0.8310  data_time: 0.0260  time: 0.9856
05/08 05:27:42 - mmengine - INFO - The previous best checkpoint /data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco/best_coco_segm_mAP_50_iter_3000.pth is removed
05/08 05:27:44 - mmengine - INFO - The best checkpoint with 0.7660 coco/segm_mAP_50 at 6000 iter is saved to best_coco_segm_mAP_50_iter_6000.pth.
05/08 05:28:59 - mmengine - INFO - Iter(train) [ 6050/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:56:09  time: 3.2461  data_time: 1.8585  memory: 29141  grad_norm: 45.8512  loss: 29.7629  loss_cls: 0.6515  loss_mask: 0.1912  loss_dice: 1.9741  d0.loss_cls: 1.0653  d0.loss_mask: 0.2348  d0.loss_dice: 2.1812  d1.loss_cls: 0.7490  d1.loss_mask: 0.2245  d1.loss_dice: 2.2688  d2.loss_cls: 0.6710  d2.loss_mask: 0.2107  d2.loss_dice: 2.1593  d3.loss_cls: 0.6409  d3.loss_mask: 0.2036  d3.loss_dice: 2.0517  d4.loss_cls: 0.6470  d4.loss_mask: 0.2004  d4.loss_dice: 2.0364  d5.loss_cls: 0.6374  d5.loss_mask: 0.1975  d5.loss_dice: 2.0320  d6.loss_cls: 0.6610  d6.loss_mask: 0.1966  d6.loss_dice: 1.9957  d7.loss_cls: 0.6556  d7.loss_mask: 0.1939  d7.loss_dice: 1.9965  d8.loss_cls: 0.6556  d8.loss_mask: 0.1921  d8.loss_dice: 1.9877
05/08 05:30:11 - mmengine - INFO - Iter(train) [ 6100/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:54:43  time: 1.4462  data_time: 0.1037  memory: 29040  grad_norm: 73.4191  loss: 30.4694  loss_cls: 0.6364  loss_mask: 0.2033  loss_dice: 2.0351  d0.loss_cls: 1.0675  d0.loss_mask: 0.2484  d0.loss_dice: 2.2630  d1.loss_cls: 0.7632  d1.loss_mask: 0.2321  d1.loss_dice: 2.3513  d2.loss_cls: 0.6592  d2.loss_mask: 0.2214  d2.loss_dice: 2.2355  d3.loss_cls: 0.6314  d3.loss_mask: 0.2107  d3.loss_dice: 2.1206  d4.loss_cls: 0.6370  d4.loss_mask: 0.2095  d4.loss_dice: 2.1001  d5.loss_cls: 0.6268  d5.loss_mask: 0.2075  d5.loss_dice: 2.0977  d6.loss_cls: 0.6538  d6.loss_mask: 0.2056  d6.loss_dice: 2.0524  d7.loss_cls: 0.6437  d7.loss_mask: 0.2055  d7.loss_dice: 2.0560  d8.loss_cls: 0.6413  d8.loss_mask: 0.2043  d8.loss_dice: 2.0492
05/08 05:31:23 - mmengine - INFO - Iter(train) [ 6150/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:53:16  time: 1.4371  data_time: 0.0577  memory: 29165  grad_norm: 50.9622  loss: 30.8323  loss_cls: 0.6547  loss_mask: 0.2011  loss_dice: 2.0661  d0.loss_cls: 1.0731  d0.loss_mask: 0.2470  d0.loss_dice: 2.2588  d1.loss_cls: 0.7692  d1.loss_mask: 0.2364  d1.loss_dice: 2.3649  d2.loss_cls: 0.6730  d2.loss_mask: 0.2203  d2.loss_dice: 2.2613  d3.loss_cls: 0.6543  d3.loss_mask: 0.2119  d3.loss_dice: 2.1409  d4.loss_cls: 0.6526  d4.loss_mask: 0.2096  d4.loss_dice: 2.1288  d5.loss_cls: 0.6425  d5.loss_mask: 0.2061  d5.loss_dice: 2.1353  d6.loss_cls: 0.6585  d6.loss_mask: 0.2046  d6.loss_dice: 2.0853  d7.loss_cls: 0.6532  d7.loss_mask: 0.2040  d7.loss_dice: 2.0847  d8.loss_cls: 0.6523  d8.loss_mask: 0.2017  d8.loss_dice: 2.0799
05/08 05:32:35 - mmengine - INFO - Iter(train) [ 6200/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:51:49  time: 1.4407  data_time: 0.0640  memory: 28026  grad_norm: 61.6151  loss: 29.0174  loss_cls: 0.6358  loss_mask: 0.1887  loss_dice: 1.9189  d0.loss_cls: 1.0577  d0.loss_mask: 0.2369  d0.loss_dice: 2.1106  d1.loss_cls: 0.7447  d1.loss_mask: 0.2181  d1.loss_dice: 2.2024  d2.loss_cls: 0.6580  d2.loss_mask: 0.2089  d2.loss_dice: 2.0981  d3.loss_cls: 0.6449  d3.loss_mask: 0.1998  d3.loss_dice: 1.9910  d4.loss_cls: 0.6343  d4.loss_mask: 0.1979  d4.loss_dice: 1.9839  d5.loss_cls: 0.6248  d5.loss_mask: 0.1944  d5.loss_dice: 1.9821  d6.loss_cls: 0.6355  d6.loss_mask: 0.1916  d6.loss_dice: 1.9441  d7.loss_cls: 0.6282  d7.loss_mask: 0.1904  d7.loss_dice: 1.9402  d8.loss_cls: 0.6310  d8.loss_mask: 0.1891  d8.loss_dice: 1.9353
05/08 05:33:47 - mmengine - INFO - Iter(train) [ 6250/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:50:23  time: 1.4381  data_time: 0.0622  memory: 29512  grad_norm: 55.1949  loss: 32.5867  loss_cls: 0.6539  loss_mask: 0.2094  loss_dice: 2.2203  d0.loss_cls: 1.1046  d0.loss_mask: 0.2543  d0.loss_dice: 2.4362  d1.loss_cls: 0.7740  d1.loss_mask: 0.2443  d1.loss_dice: 2.5318  d2.loss_cls: 0.6865  d2.loss_mask: 0.2296  d2.loss_dice: 2.4173  d3.loss_cls: 0.6765  d3.loss_mask: 0.2206  d3.loss_dice: 2.2973  d4.loss_cls: 0.6626  d4.loss_mask: 0.2183  d4.loss_dice: 2.2925  d5.loss_cls: 0.6482  d5.loss_mask: 0.2148  d5.loss_dice: 2.2909  d6.loss_cls: 0.6542  d6.loss_mask: 0.2126  d6.loss_dice: 2.2448  d7.loss_cls: 0.6456  d7.loss_mask: 0.2125  d7.loss_dice: 2.2420  d8.loss_cls: 0.6520  d8.loss_mask: 0.2104  d8.loss_dice: 2.2288
05/08 05:34:59 - mmengine - INFO - Iter(train) [ 6300/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:48:58  time: 1.4522  data_time: 0.0634  memory: 29017  grad_norm: 47.8319  loss: 31.2591  loss_cls: 0.6532  loss_mask: 0.1934  loss_dice: 2.1077  d0.loss_cls: 1.0899  d0.loss_mask: 0.2388  d0.loss_dice: 2.3294  d1.loss_cls: 0.7718  d1.loss_mask: 0.2258  d1.loss_dice: 2.4152  d2.loss_cls: 0.6798  d2.loss_mask: 0.2120  d2.loss_dice: 2.3083  d3.loss_cls: 0.6703  d3.loss_mask: 0.2034  d3.loss_dice: 2.1855  d4.loss_cls: 0.6571  d4.loss_mask: 0.2016  d4.loss_dice: 2.1773  d5.loss_cls: 0.6457  d5.loss_mask: 0.1991  d5.loss_dice: 2.1750  d6.loss_cls: 0.6472  d6.loss_mask: 0.1964  d6.loss_dice: 2.1348  d7.loss_cls: 0.6434  d7.loss_mask: 0.1955  d7.loss_dice: 2.1321  d8.loss_cls: 0.6489  d8.loss_mask: 0.1943  d8.loss_dice: 2.1262
05/08 05:36:11 - mmengine - INFO - Iter(train) [ 6350/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:47:32  time: 1.4396  data_time: 0.0582  memory: 29143  grad_norm: 49.4602  loss: 30.1165  loss_cls: 0.6346  loss_mask: 0.2029  loss_dice: 2.0056  d0.loss_cls: 1.0784  d0.loss_mask: 0.2532  d0.loss_dice: 2.2172  d1.loss_cls: 0.7440  d1.loss_mask: 0.2369  d1.loss_dice: 2.2972  d2.loss_cls: 0.6662  d2.loss_mask: 0.2224  d2.loss_dice: 2.1885  d3.loss_cls: 0.6646  d3.loss_mask: 0.2134  d3.loss_dice: 2.0774  d4.loss_cls: 0.6422  d4.loss_mask: 0.2122  d4.loss_dice: 2.0759  d5.loss_cls: 0.6273  d5.loss_mask: 0.2079  d5.loss_dice: 2.0706  d6.loss_cls: 0.6283  d6.loss_mask: 0.2056  d6.loss_dice: 2.0276  d7.loss_cls: 0.6303  d7.loss_mask: 0.2043  d7.loss_dice: 2.0277  d8.loss_cls: 0.6317  d8.loss_mask: 0.2029  d8.loss_dice: 2.0195
05/08 05:37:22 - mmengine - INFO - Iter(train) [ 6400/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:46:04  time: 1.4224  data_time: 0.0484  memory: 28839  grad_norm: 57.3132  loss: 30.0229  loss_cls: 0.6386  loss_mask: 0.1943  loss_dice: 2.0016  d0.loss_cls: 1.0692  d0.loss_mask: 0.2404  d0.loss_dice: 2.2134  d1.loss_cls: 0.7494  d1.loss_mask: 0.2301  d1.loss_dice: 2.2995  d2.loss_cls: 0.6685  d2.loss_mask: 0.2133  d2.loss_dice: 2.1922  d3.loss_cls: 0.6620  d3.loss_mask: 0.2051  d3.loss_dice: 2.0831  d4.loss_cls: 0.6391  d4.loss_mask: 0.2020  d4.loss_dice: 2.0761  d5.loss_cls: 0.6319  d5.loss_mask: 0.2004  d5.loss_dice: 2.0655  d6.loss_cls: 0.6331  d6.loss_mask: 0.1972  d6.loss_dice: 2.0237  d7.loss_cls: 0.6318  d7.loss_mask: 0.1962  d7.loss_dice: 2.0225  d8.loss_cls: 0.6352  d8.loss_mask: 0.1948  d8.loss_dice: 2.0129
05/08 05:38:34 - mmengine - INFO - Iter(train) [ 6450/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:44:39  time: 1.4426  data_time: 0.0509  memory: 30083  grad_norm: 50.7221  loss: 31.9340  loss_cls: 0.6430  loss_mask: 0.2010  loss_dice: 2.1760  d0.loss_cls: 1.0965  d0.loss_mask: 0.2455  d0.loss_dice: 2.3882  d1.loss_cls: 0.7568  d1.loss_mask: 0.2387  d1.loss_dice: 2.4875  d2.loss_cls: 0.6773  d2.loss_mask: 0.2217  d2.loss_dice: 2.3701  d3.loss_cls: 0.6644  d3.loss_mask: 0.2099  d3.loss_dice: 2.2608  d4.loss_cls: 0.6433  d4.loss_mask: 0.2104  d4.loss_dice: 2.2509  d5.loss_cls: 0.6345  d5.loss_mask: 0.2060  d5.loss_dice: 2.2394  d6.loss_cls: 0.6414  d6.loss_mask: 0.2028  d6.loss_dice: 2.1971  d7.loss_cls: 0.6374  d7.loss_mask: 0.2033  d7.loss_dice: 2.1967  d8.loss_cls: 0.6414  d8.loss_mask: 0.2017  d8.loss_dice: 2.1904
05/08 05:39:48 - mmengine - INFO - Iter(train) [ 6500/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:43:18  time: 1.4758  data_time: 0.1126  memory: 29039  grad_norm: 53.4213  loss: 30.6116  loss_cls: 0.6444  loss_mask: 0.1915  loss_dice: 2.0542  d0.loss_cls: 1.0861  d0.loss_mask: 0.2356  d0.loss_dice: 2.2646  d1.loss_cls: 0.7699  d1.loss_mask: 0.2306  d1.loss_dice: 2.3462  d2.loss_cls: 0.6813  d2.loss_mask: 0.2103  d2.loss_dice: 2.2497  d3.loss_cls: 0.6675  d3.loss_mask: 0.2037  d3.loss_dice: 2.1315  d4.loss_cls: 0.6502  d4.loss_mask: 0.2012  d4.loss_dice: 2.1213  d5.loss_cls: 0.6392  d5.loss_mask: 0.1964  d5.loss_dice: 2.1192  d6.loss_cls: 0.6397  d6.loss_mask: 0.1933  d6.loss_dice: 2.0755  d7.loss_cls: 0.6409  d7.loss_mask: 0.1934  d7.loss_dice: 2.0738  d8.loss_cls: 0.6401  d8.loss_mask: 0.1923  d8.loss_dice: 2.0682
05/08 05:41:00 - mmengine - INFO - Iter(train) [ 6550/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:41:53  time: 1.4443  data_time: 0.0576  memory: 28303  grad_norm: 63.5147  loss: 30.7808  loss_cls: 0.6574  loss_mask: 0.2006  loss_dice: 2.0569  d0.loss_cls: 1.0807  d0.loss_mask: 0.2462  d0.loss_dice: 2.2678  d1.loss_cls: 0.7693  d1.loss_mask: 0.2390  d1.loss_dice: 2.3472  d2.loss_cls: 0.6821  d2.loss_mask: 0.2206  d2.loss_dice: 2.2424  d3.loss_cls: 0.6800  d3.loss_mask: 0.2112  d3.loss_dice: 2.1314  d4.loss_cls: 0.6613  d4.loss_mask: 0.2081  d4.loss_dice: 2.1209  d5.loss_cls: 0.6532  d5.loss_mask: 0.2048  d5.loss_dice: 2.1166  d6.loss_cls: 0.6504  d6.loss_mask: 0.2033  d6.loss_dice: 2.0786  d7.loss_cls: 0.6492  d7.loss_mask: 0.2028  d7.loss_dice: 2.0764  d8.loss_cls: 0.6529  d8.loss_mask: 0.2012  d8.loss_dice: 2.0683
05/08 05:42:13 - mmengine - INFO - Iter(train) [ 6600/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:40:30  time: 1.4547  data_time: 0.0533  memory: 28392  grad_norm: 55.3045  loss: 30.0942  loss_cls: 0.6390  loss_mask: 0.2062  loss_dice: 2.0049  d0.loss_cls: 1.0786  d0.loss_mask: 0.2532  d0.loss_dice: 2.2051  d1.loss_cls: 0.7476  d1.loss_mask: 0.2347  d1.loss_dice: 2.2814  d2.loss_cls: 0.6687  d2.loss_mask: 0.2240  d2.loss_dice: 2.1726  d3.loss_cls: 0.6700  d3.loss_mask: 0.2168  d3.loss_dice: 2.0698  d4.loss_cls: 0.6461  d4.loss_mask: 0.2144  d4.loss_dice: 2.0638  d5.loss_cls: 0.6356  d5.loss_mask: 0.2101  d5.loss_dice: 2.0588  d6.loss_cls: 0.6319  d6.loss_mask: 0.2082  d6.loss_dice: 2.0258  d7.loss_cls: 0.6378  d7.loss_mask: 0.2065  d7.loss_dice: 2.0193  d8.loss_cls: 0.6413  d8.loss_mask: 0.2057  d8.loss_dice: 2.0162
05/08 05:43:26 - mmengine - INFO - Iter(train) [ 6650/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:39:07  time: 1.4570  data_time: 0.0660  memory: 29581  grad_norm: 46.3785  loss: 30.4567  loss_cls: 0.6407  loss_mask: 0.2042  loss_dice: 2.0354  d0.loss_cls: 1.0759  d0.loss_mask: 0.2485  d0.loss_dice: 2.2410  d1.loss_cls: 0.7337  d1.loss_mask: 0.2372  d1.loss_dice: 2.3225  d2.loss_cls: 0.6585  d2.loss_mask: 0.2230  d2.loss_dice: 2.2190  d3.loss_cls: 0.6622  d3.loss_mask: 0.2170  d3.loss_dice: 2.1189  d4.loss_cls: 0.6548  d4.loss_mask: 0.2142  d4.loss_dice: 2.1122  d5.loss_cls: 0.6411  d5.loss_mask: 0.2092  d5.loss_dice: 2.1005  d6.loss_cls: 0.6344  d6.loss_mask: 0.2061  d6.loss_dice: 2.0548  d7.loss_cls: 0.6362  d7.loss_mask: 0.2059  d7.loss_dice: 2.0571  d8.loss_cls: 0.6393  d8.loss_mask: 0.2051  d8.loss_dice: 2.0483
05/08 05:44:39 - mmengine - INFO - Iter(train) [ 6700/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:37:43  time: 1.4524  data_time: 0.0566  memory: 29956  grad_norm: 57.1505  loss: 31.0834  loss_cls: 0.6591  loss_mask: 0.2009  loss_dice: 2.0824  d0.loss_cls: 1.0941  d0.loss_mask: 0.2494  d0.loss_dice: 2.2962  d1.loss_cls: 0.7595  d1.loss_mask: 0.2404  d1.loss_dice: 2.3844  d2.loss_cls: 0.6814  d2.loss_mask: 0.2215  d2.loss_dice: 2.2748  d3.loss_cls: 0.6733  d3.loss_mask: 0.2134  d3.loss_dice: 2.1648  d4.loss_cls: 0.6624  d4.loss_mask: 0.2095  d4.loss_dice: 2.1493  d5.loss_cls: 0.6485  d5.loss_mask: 0.2073  d5.loss_dice: 2.1414  d6.loss_cls: 0.6586  d6.loss_mask: 0.2032  d6.loss_dice: 2.1003  d7.loss_cls: 0.6569  d7.loss_mask: 0.2022  d7.loss_dice: 2.0996  d8.loss_cls: 0.6553  d8.loss_mask: 0.2011  d8.loss_dice: 2.0921
05/08 05:45:51 - mmengine - INFO - Iter(train) [ 6750/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:36:19  time: 1.4405  data_time: 0.0566  memory: 31170  grad_norm: 51.9199  loss: 30.9397  loss_cls: 0.6366  loss_mask: 0.1952  loss_dice: 2.0946  d0.loss_cls: 1.0836  d0.loss_mask: 0.2409  d0.loss_dice: 2.2816  d1.loss_cls: 0.7340  d1.loss_mask: 0.2282  d1.loss_dice: 2.3943  d2.loss_cls: 0.6588  d2.loss_mask: 0.2120  d2.loss_dice: 2.2841  d3.loss_cls: 0.6594  d3.loss_mask: 0.2051  d3.loss_dice: 2.1784  d4.loss_cls: 0.6482  d4.loss_mask: 0.2040  d4.loss_dice: 2.1662  d5.loss_cls: 0.6384  d5.loss_mask: 0.2012  d5.loss_dice: 2.1559  d6.loss_cls: 0.6364  d6.loss_mask: 0.1968  d6.loss_dice: 2.1171  d7.loss_cls: 0.6362  d7.loss_mask: 0.1979  d7.loss_dice: 2.1169  d8.loss_cls: 0.6370  d8.loss_mask: 0.1958  d8.loss_dice: 2.1050
05/08 05:47:03 - mmengine - INFO - Iter(train) [ 6800/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:34:56  time: 1.4545  data_time: 0.0665  memory: 28791  grad_norm: 49.8427  loss: 30.9860  loss_cls: 0.6442  loss_mask: 0.2074  loss_dice: 2.0753  d0.loss_cls: 1.0900  d0.loss_mask: 0.2553  d0.loss_dice: 2.2795  d1.loss_cls: 0.7583  d1.loss_mask: 0.2445  d1.loss_dice: 2.3757  d2.loss_cls: 0.6744  d2.loss_mask: 0.2275  d2.loss_dice: 2.2633  d3.loss_cls: 0.6677  d3.loss_mask: 0.2204  d3.loss_dice: 2.1573  d4.loss_cls: 0.6538  d4.loss_mask: 0.2170  d4.loss_dice: 2.1433  d5.loss_cls: 0.6464  d5.loss_mask: 0.2132  d5.loss_dice: 2.1292  d6.loss_cls: 0.6525  d6.loss_mask: 0.2100  d6.loss_dice: 2.0900  d7.loss_cls: 0.6465  d7.loss_mask: 0.2092  d7.loss_dice: 2.0923  d8.loss_cls: 0.6458  d8.loss_mask: 0.2081  d8.loss_dice: 2.0877
05/08 05:48:16 - mmengine - INFO - Iter(train) [ 6850/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:33:33  time: 1.4450  data_time: 0.0531  memory: 29232  grad_norm: 59.5237  loss: 30.1032  loss_cls: 0.6330  loss_mask: 0.1991  loss_dice: 2.0119  d0.loss_cls: 1.0736  d0.loss_mask: 0.2425  d0.loss_dice: 2.2013  d1.loss_cls: 0.7431  d1.loss_mask: 0.2329  d1.loss_dice: 2.2824  d2.loss_cls: 0.6724  d2.loss_mask: 0.2187  d2.loss_dice: 2.1778  d3.loss_cls: 0.6679  d3.loss_mask: 0.2118  d3.loss_dice: 2.0807  d4.loss_cls: 0.6511  d4.loss_mask: 0.2091  d4.loss_dice: 2.0764  d5.loss_cls: 0.6418  d5.loss_mask: 0.2053  d5.loss_dice: 2.0626  d6.loss_cls: 0.6512  d6.loss_mask: 0.2029  d6.loss_dice: 2.0264  d7.loss_cls: 0.6410  d7.loss_mask: 0.2026  d7.loss_dice: 2.0240  d8.loss_cls: 0.6380  d8.loss_mask: 0.2003  d8.loss_dice: 2.0215
05/08 05:49:28 - mmengine - INFO - Iter(train) [ 6900/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:32:09  time: 1.4458  data_time: 0.0967  memory: 28679  grad_norm: 63.0634  loss: 31.1048  loss_cls: 0.6318  loss_mask: 0.2116  loss_dice: 2.0772  d0.loss_cls: 1.0829  d0.loss_mask: 0.2651  d0.loss_dice: 2.2950  d1.loss_cls: 0.7557  d1.loss_mask: 0.2456  d1.loss_dice: 2.3986  d2.loss_cls: 0.6817  d2.loss_mask: 0.2342  d2.loss_dice: 2.2785  d3.loss_cls: 0.6652  d3.loss_mask: 0.2239  d3.loss_dice: 2.1731  d4.loss_cls: 0.6458  d4.loss_mask: 0.2206  d4.loss_dice: 2.1575  d5.loss_cls: 0.6395  d5.loss_mask: 0.2198  d5.loss_dice: 2.1408  d6.loss_cls: 0.6528  d6.loss_mask: 0.2174  d6.loss_dice: 2.1011  d7.loss_cls: 0.6377  d7.loss_mask: 0.2150  d7.loss_dice: 2.0979  d8.loss_cls: 0.6332  d8.loss_mask: 0.2127  d8.loss_dice: 2.0928
05/08 05:50:40 - mmengine - INFO - Iter(train) [ 6950/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:30:46  time: 1.4493  data_time: 0.0615  memory: 28519  grad_norm: 73.1308  loss: 29.6159  loss_cls: 0.6115  loss_mask: 0.1953  loss_dice: 1.9798  d0.loss_cls: 1.0749  d0.loss_mask: 0.2404  d0.loss_dice: 2.1819  d1.loss_cls: 0.7334  d1.loss_mask: 0.2330  d1.loss_dice: 2.2672  d2.loss_cls: 0.6549  d2.loss_mask: 0.2170  d2.loss_dice: 2.1522  d3.loss_cls: 0.6457  d3.loss_mask: 0.2071  d3.loss_dice: 2.0541  d4.loss_cls: 0.6308  d4.loss_mask: 0.2045  d4.loss_dice: 2.0367  d5.loss_cls: 0.6239  d5.loss_mask: 0.2005  d5.loss_dice: 2.0269  d6.loss_cls: 0.6335  d6.loss_mask: 0.2003  d6.loss_dice: 1.9967  d7.loss_cls: 0.6222  d7.loss_mask: 0.1988  d7.loss_dice: 1.9900  d8.loss_cls: 0.6162  d8.loss_mask: 0.1961  d8.loss_dice: 1.9905
05/08 05:51:52 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 05:51:52 - mmengine - INFO - Iter(train) [ 7000/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:29:21  time: 1.4227  data_time: 0.0513  memory: 28179  grad_norm: 68.0653  loss: 29.3836  loss_cls: 0.6382  loss_mask: 0.2034  loss_dice: 1.9368  d0.loss_cls: 1.0631  d0.loss_mask: 0.2513  d0.loss_dice: 2.1246  d1.loss_cls: 0.7388  d1.loss_mask: 0.2361  d1.loss_dice: 2.2145  d2.loss_cls: 0.6647  d2.loss_mask: 0.2231  d2.loss_dice: 2.1051  d3.loss_cls: 0.6548  d3.loss_mask: 0.2136  d3.loss_dice: 2.0145  d4.loss_cls: 0.6386  d4.loss_mask: 0.2117  d4.loss_dice: 1.9988  d5.loss_cls: 0.6437  d5.loss_mask: 0.2097  d5.loss_dice: 1.9864  d6.loss_cls: 0.6517  d6.loss_mask: 0.2073  d6.loss_dice: 1.9546  d7.loss_cls: 0.6433  d7.loss_mask: 0.2066  d7.loss_dice: 1.9507  d8.loss_cls: 0.6429  d8.loss_mask: 0.2053  d8.loss_dice: 1.9499
05/08 05:51:52 - mmengine - INFO - Saving checkpoint at 7000 iterations
05/08 05:52:44 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9915  data_time: 0.0407  memory: 3258  
05/08 05:53:07 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.37s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 05:53:14 - mmengine - INFO - start multi processing evaluation ...
DONE (t=54.02s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.386
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.753
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.331
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.454
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.859
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.505
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.867
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.484
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.382
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.559
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.900
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.505
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.505
05/08 05:54:08 - mmengine - INFO - segm_mAP_copypaste: 0.386 0.753 0.331 0.227 0.454 0.859
05/08 05:54:09 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.3860  coco/segm_mAP_50: 0.7530  coco/segm_mAP_75: 0.3310  coco/segm_mAP_s: 0.2270  coco/segm_mAP_m: 0.4540  coco/segm_mAP_l: 0.8590  data_time: 0.0404  time: 0.9892
05/08 05:55:21 - mmengine - INFO - Iter(train) [ 7050/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:30:34  time: 3.1420  data_time: 1.7647  memory: 28399  grad_norm: 52.6441  loss: 30.7709  loss_cls: 0.6629  loss_mask: 0.1994  loss_dice: 2.0584  d0.loss_cls: 1.0809  d0.loss_mask: 0.2420  d0.loss_dice: 2.2448  d1.loss_cls: 0.7613  d1.loss_mask: 0.2313  d1.loss_dice: 2.3534  d2.loss_cls: 0.6797  d2.loss_mask: 0.2172  d2.loss_dice: 2.2444  d3.loss_cls: 0.6699  d3.loss_mask: 0.2093  d3.loss_dice: 2.1384  d4.loss_cls: 0.6581  d4.loss_mask: 0.2085  d4.loss_dice: 2.1234  d5.loss_cls: 0.6559  d5.loss_mask: 0.2046  d5.loss_dice: 2.1124  d6.loss_cls: 0.6661  d6.loss_mask: 0.2041  d6.loss_dice: 2.0751  d7.loss_cls: 0.6583  d7.loss_mask: 0.2019  d7.loss_dice: 2.0757  d8.loss_cls: 0.6585  d8.loss_mask: 0.2010  d8.loss_dice: 2.0737
05/08 05:56:32 - mmengine - INFO - Iter(train) [ 7100/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:29:08  time: 1.4295  data_time: 0.0558  memory: 29709  grad_norm: 57.6924  loss: 30.6435  loss_cls: 0.6420  loss_mask: 0.2005  loss_dice: 2.0543  d0.loss_cls: 1.0756  d0.loss_mask: 0.2482  d0.loss_dice: 2.2524  d1.loss_cls: 0.7421  d1.loss_mask: 0.2387  d1.loss_dice: 2.3642  d2.loss_cls: 0.6694  d2.loss_mask: 0.2202  d2.loss_dice: 2.2482  d3.loss_cls: 0.6554  d3.loss_mask: 0.2104  d3.loss_dice: 2.1401  d4.loss_cls: 0.6445  d4.loss_mask: 0.2085  d4.loss_dice: 2.1229  d5.loss_cls: 0.6381  d5.loss_mask: 0.2069  d5.loss_dice: 2.1141  d6.loss_cls: 0.6480  d6.loss_mask: 0.2042  d6.loss_dice: 2.0740  d7.loss_cls: 0.6385  d7.loss_mask: 0.2022  d7.loss_dice: 2.0728  d8.loss_cls: 0.6379  d8.loss_mask: 0.2013  d8.loss_dice: 2.0678
05/08 05:57:44 - mmengine - INFO - Iter(train) [ 7150/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:27:44  time: 1.4395  data_time: 0.0447  memory: 29668  grad_norm: 50.6891  loss: 31.3823  loss_cls: 0.6485  loss_mask: 0.1983  loss_dice: 2.1207  d0.loss_cls: 1.0966  d0.loss_mask: 0.2441  d0.loss_dice: 2.3235  d1.loss_cls: 0.7690  d1.loss_mask: 0.2329  d1.loss_dice: 2.4191  d2.loss_cls: 0.6925  d2.loss_mask: 0.2170  d2.loss_dice: 2.2985  d3.loss_cls: 0.6711  d3.loss_mask: 0.2091  d3.loss_dice: 2.1910  d4.loss_cls: 0.6557  d4.loss_mask: 0.2061  d4.loss_dice: 2.1868  d5.loss_cls: 0.6540  d5.loss_mask: 0.2040  d5.loss_dice: 2.1739  d6.loss_cls: 0.6606  d6.loss_mask: 0.2010  d6.loss_dice: 2.1359  d7.loss_cls: 0.6536  d7.loss_mask: 0.2003  d7.loss_dice: 2.1344  d8.loss_cls: 0.6533  d8.loss_mask: 0.1994  d8.loss_dice: 2.1313
05/08 05:58:58 - mmengine - INFO - Iter(train) [ 7200/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:26:21  time: 1.4627  data_time: 0.0441  memory: 29714  grad_norm: 76.4650  loss: 29.3070  loss_cls: 0.6383  loss_mask: 0.1926  loss_dice: 1.9505  d0.loss_cls: 1.0749  d0.loss_mask: 0.2370  d0.loss_dice: 2.1212  d1.loss_cls: 0.7527  d1.loss_mask: 0.2206  d1.loss_dice: 2.2042  d2.loss_cls: 0.6630  d2.loss_mask: 0.2087  d2.loss_dice: 2.1102  d3.loss_cls: 0.6548  d3.loss_mask: 0.2013  d3.loss_dice: 2.0103  d4.loss_cls: 0.6442  d4.loss_mask: 0.1999  d4.loss_dice: 1.9990  d5.loss_cls: 0.6345  d5.loss_mask: 0.1982  d5.loss_dice: 1.9946  d6.loss_cls: 0.6507  d6.loss_mask: 0.1957  d6.loss_dice: 1.9605  d7.loss_cls: 0.6448  d7.loss_mask: 0.1950  d7.loss_dice: 1.9586  d8.loss_cls: 0.6469  d8.loss_mask: 0.1934  d8.loss_dice: 1.9509
05/08 06:00:08 - mmengine - INFO - Iter(train) [ 7250/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:24:54  time: 1.4143  data_time: 0.0504  memory: 28166  grad_norm: 56.9150  loss: 30.5524  loss_cls: 0.6579  loss_mask: 0.2000  loss_dice: 2.0455  d0.loss_cls: 1.0697  d0.loss_mask: 0.2488  d0.loss_dice: 2.2213  d1.loss_cls: 0.7544  d1.loss_mask: 0.2333  d1.loss_dice: 2.3166  d2.loss_cls: 0.6845  d2.loss_mask: 0.2192  d2.loss_dice: 2.2205  d3.loss_cls: 0.6665  d3.loss_mask: 0.2113  d3.loss_dice: 2.1195  d4.loss_cls: 0.6553  d4.loss_mask: 0.2080  d4.loss_dice: 2.1078  d5.loss_cls: 0.6525  d5.loss_mask: 0.2041  d5.loss_dice: 2.0936  d6.loss_cls: 0.6609  d6.loss_mask: 0.2040  d6.loss_dice: 2.0620  d7.loss_cls: 0.6516  d7.loss_mask: 0.2027  d7.loss_dice: 2.0653  d8.loss_cls: 0.6556  d8.loss_mask: 0.2011  d8.loss_dice: 2.0589
05/08 06:01:21 - mmengine - INFO - Iter(train) [ 7300/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:23:31  time: 1.4534  data_time: 0.0983  memory: 29984  grad_norm: 77.0125  loss: 31.6435  loss_cls: 0.6753  loss_mask: 0.2088  loss_dice: 2.1287  d0.loss_cls: 1.1033  d0.loss_mask: 0.2526  d0.loss_dice: 2.3339  d1.loss_cls: 0.7638  d1.loss_mask: 0.2424  d1.loss_dice: 2.4303  d2.loss_cls: 0.6815  d2.loss_mask: 0.2274  d2.loss_dice: 2.3066  d3.loss_cls: 0.6767  d3.loss_mask: 0.2183  d3.loss_dice: 2.2051  d4.loss_cls: 0.6686  d4.loss_mask: 0.2167  d4.loss_dice: 2.1998  d5.loss_cls: 0.6635  d5.loss_mask: 0.2114  d5.loss_dice: 2.1836  d6.loss_cls: 0.6722  d6.loss_mask: 0.2100  d6.loss_dice: 2.1419  d7.loss_cls: 0.6605  d7.loss_mask: 0.2098  d7.loss_dice: 2.1430  d8.loss_cls: 0.6698  d8.loss_mask: 0.2080  d8.loss_dice: 2.1299
05/08 06:02:34 - mmengine - INFO - Iter(train) [ 7350/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:22:09  time: 1.4640  data_time: 0.0568  memory: 29069  grad_norm: 67.8842  loss: 31.0949  loss_cls: 0.6757  loss_mask: 0.2052  loss_dice: 2.0702  d0.loss_cls: 1.0944  d0.loss_mask: 0.2512  d0.loss_dice: 2.2773  d1.loss_cls: 0.7611  d1.loss_mask: 0.2395  d1.loss_dice: 2.3619  d2.loss_cls: 0.6847  d2.loss_mask: 0.2235  d2.loss_dice: 2.2531  d3.loss_cls: 0.6934  d3.loss_mask: 0.2181  d3.loss_dice: 2.1549  d4.loss_cls: 0.6895  d4.loss_mask: 0.2154  d4.loss_dice: 2.1484  d5.loss_cls: 0.6771  d5.loss_mask: 0.2091  d5.loss_dice: 2.1302  d6.loss_cls: 0.6728  d6.loss_mask: 0.2073  d6.loss_dice: 2.0861  d7.loss_cls: 0.6601  d7.loss_mask: 0.2059  d7.loss_dice: 2.0860  d8.loss_cls: 0.6599  d8.loss_mask: 0.2045  d8.loss_dice: 2.0786
05/08 06:03:47 - mmengine - INFO - Iter(train) [ 7400/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:20:47  time: 1.4524  data_time: 0.0571  memory: 30074  grad_norm: 60.1800  loss: 31.8842  loss_cls: 0.6767  loss_mask: 0.2057  loss_dice: 2.1455  d0.loss_cls: 1.1055  d0.loss_mask: 0.2468  d0.loss_dice: 2.3437  d1.loss_cls: 0.7651  d1.loss_mask: 0.2390  d1.loss_dice: 2.4430  d2.loss_cls: 0.6873  d2.loss_mask: 0.2266  d2.loss_dice: 2.3405  d3.loss_cls: 0.6921  d3.loss_mask: 0.2176  d3.loss_dice: 2.2319  d4.loss_cls: 0.6768  d4.loss_mask: 0.2151  d4.loss_dice: 2.2299  d5.loss_cls: 0.6683  d5.loss_mask: 0.2117  d5.loss_dice: 2.2139  d6.loss_cls: 0.6715  d6.loss_mask: 0.2096  d6.loss_dice: 2.1677  d7.loss_cls: 0.6594  d7.loss_mask: 0.2069  d7.loss_dice: 2.1613  d8.loss_cls: 0.6659  d8.loss_mask: 0.2052  d8.loss_dice: 2.1540
05/08 06:04:59 - mmengine - INFO - Iter(train) [ 7450/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:19:23  time: 1.4429  data_time: 0.0642  memory: 28644  grad_norm: 55.8662  loss: 31.0806  loss_cls: 0.6752  loss_mask: 0.1930  loss_dice: 2.0926  d0.loss_cls: 1.0739  d0.loss_mask: 0.2321  d0.loss_dice: 2.2596  d1.loss_cls: 0.7459  d1.loss_mask: 0.2212  d1.loss_dice: 2.3773  d2.loss_cls: 0.6732  d2.loss_mask: 0.2105  d2.loss_dice: 2.2735  d3.loss_cls: 0.6920  d3.loss_mask: 0.2025  d3.loss_dice: 2.1725  d4.loss_cls: 0.6828  d4.loss_mask: 0.2030  d4.loss_dice: 2.1651  d5.loss_cls: 0.6789  d5.loss_mask: 0.1972  d5.loss_dice: 2.1484  d6.loss_cls: 0.6722  d6.loss_mask: 0.1955  d6.loss_dice: 2.1088  d7.loss_cls: 0.6640  d7.loss_mask: 0.1947  d7.loss_dice: 2.1081  d8.loss_cls: 0.6685  d8.loss_mask: 0.1938  d8.loss_dice: 2.1045
05/08 06:06:12 - mmengine - INFO - Iter(train) [ 7500/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:18:01  time: 1.4613  data_time: 0.0606  memory: 31226  grad_norm: 57.1132  loss: 33.2924  loss_cls: 0.6814  loss_mask: 0.2037  loss_dice: 2.2880  d0.loss_cls: 1.1124  d0.loss_mask: 0.2451  d0.loss_dice: 2.4953  d1.loss_cls: 0.7726  d1.loss_mask: 0.2395  d1.loss_dice: 2.5822  d2.loss_cls: 0.6947  d2.loss_mask: 0.2222  d2.loss_dice: 2.4739  d3.loss_cls: 0.6982  d3.loss_mask: 0.2162  d3.loss_dice: 2.3637  d4.loss_cls: 0.6933  d4.loss_mask: 0.2113  d4.loss_dice: 2.3519  d5.loss_cls: 0.6823  d5.loss_mask: 0.2070  d5.loss_dice: 2.3370  d6.loss_cls: 0.6817  d6.loss_mask: 0.2043  d6.loss_dice: 2.2965  d7.loss_cls: 0.6673  d7.loss_mask: 0.2038  d7.loss_dice: 2.2921  d8.loss_cls: 0.6815  d8.loss_mask: 0.2029  d8.loss_dice: 2.2905
05/08 06:07:24 - mmengine - INFO - Iter(train) [ 7550/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:16:37  time: 1.4367  data_time: 0.0574  memory: 30516  grad_norm: 49.0055  loss: 32.0567  loss_cls: 0.6640  loss_mask: 0.2084  loss_dice: 2.1654  d0.loss_cls: 1.1049  d0.loss_mask: 0.2560  d0.loss_dice: 2.3665  d1.loss_cls: 0.7619  d1.loss_mask: 0.2445  d1.loss_dice: 2.4679  d2.loss_cls: 0.6783  d2.loss_mask: 0.2263  d2.loss_dice: 2.3548  d3.loss_cls: 0.6895  d3.loss_mask: 0.2213  d3.loss_dice: 2.2512  d4.loss_cls: 0.6742  d4.loss_mask: 0.2203  d4.loss_dice: 2.2424  d5.loss_cls: 0.6616  d5.loss_mask: 0.2146  d5.loss_dice: 2.2256  d6.loss_cls: 0.6638  d6.loss_mask: 0.2109  d6.loss_dice: 2.1849  d7.loss_cls: 0.6559  d7.loss_mask: 0.2109  d7.loss_dice: 2.1805  d8.loss_cls: 0.6610  d8.loss_mask: 0.2104  d8.loss_dice: 2.1785
05/08 06:08:36 - mmengine - INFO - Iter(train) [ 7600/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:15:14  time: 1.4416  data_time: 0.0519  memory: 29713  grad_norm: 58.6054  loss: 30.6463  loss_cls: 0.6393  loss_mask: 0.1965  loss_dice: 2.0637  d0.loss_cls: 1.0845  d0.loss_mask: 0.2423  d0.loss_dice: 2.2600  d1.loss_cls: 0.7468  d1.loss_mask: 0.2304  d1.loss_dice: 2.3556  d2.loss_cls: 0.6689  d2.loss_mask: 0.2137  d2.loss_dice: 2.2526  d3.loss_cls: 0.6717  d3.loss_mask: 0.2065  d3.loss_dice: 2.1420  d4.loss_cls: 0.6408  d4.loss_mask: 0.2041  d4.loss_dice: 2.1363  d5.loss_cls: 0.6355  d5.loss_mask: 0.2021  d5.loss_dice: 2.1212  d6.loss_cls: 0.6472  d6.loss_mask: 0.1986  d6.loss_dice: 2.0723  d7.loss_cls: 0.6400  d7.loss_mask: 0.1969  d7.loss_dice: 2.0732  d8.loss_cls: 0.6371  d8.loss_mask: 0.1961  d8.loss_dice: 2.0702
05/08 06:09:47 - mmengine - INFO - Iter(train) [ 7650/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:13:50  time: 1.4241  data_time: 0.0580  memory: 29192  grad_norm: 56.6002  loss: 31.8467  loss_cls: 0.6585  loss_mask: 0.2159  loss_dice: 2.1431  d0.loss_cls: 1.0851  d0.loss_mask: 0.2710  d0.loss_dice: 2.3540  d1.loss_cls: 0.7568  d1.loss_mask: 0.2528  d1.loss_dice: 2.4406  d2.loss_cls: 0.6789  d2.loss_mask: 0.2363  d2.loss_dice: 2.3351  d3.loss_cls: 0.6804  d3.loss_mask: 0.2270  d3.loss_dice: 2.2331  d4.loss_cls: 0.6588  d4.loss_mask: 0.2254  d4.loss_dice: 2.2256  d5.loss_cls: 0.6472  d5.loss_mask: 0.2216  d5.loss_dice: 2.2127  d6.loss_cls: 0.6631  d6.loss_mask: 0.2179  d6.loss_dice: 2.1540  d7.loss_cls: 0.6586  d7.loss_mask: 0.2184  d7.loss_dice: 2.1500  d8.loss_cls: 0.6551  d8.loss_mask: 0.2171  d8.loss_dice: 2.1526
05/08 06:11:00 - mmengine - INFO - Iter(train) [ 7700/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:12:29  time: 1.4636  data_time: 0.1004  memory: 28597  grad_norm: 48.8675  loss: 30.4358  loss_cls: 0.6492  loss_mask: 0.1933  loss_dice: 2.0298  d0.loss_cls: 1.0782  d0.loss_mask: 0.2379  d0.loss_dice: 2.2169  d1.loss_cls: 0.7543  d1.loss_mask: 0.2251  d1.loss_dice: 2.3227  d2.loss_cls: 0.6676  d2.loss_mask: 0.2126  d2.loss_dice: 2.2278  d3.loss_cls: 0.6850  d3.loss_mask: 0.2055  d3.loss_dice: 2.1247  d4.loss_cls: 0.6623  d4.loss_mask: 0.2018  d4.loss_dice: 2.1150  d5.loss_cls: 0.6415  d5.loss_mask: 0.1975  d5.loss_dice: 2.0997  d6.loss_cls: 0.6549  d6.loss_mask: 0.1957  d6.loss_dice: 2.0499  d7.loss_cls: 0.6516  d7.loss_mask: 0.1962  d7.loss_dice: 2.0467  d8.loss_cls: 0.6457  d8.loss_mask: 0.1950  d8.loss_dice: 2.0516
05/08 06:12:12 - mmengine - INFO - Iter(train) [ 7750/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:11:05  time: 1.4342  data_time: 0.0536  memory: 29747  grad_norm: 60.9994  loss: 31.1990  loss_cls: 0.6373  loss_mask: 0.1911  loss_dice: 2.1109  d0.loss_cls: 1.0932  d0.loss_mask: 0.2377  d0.loss_dice: 2.3064  d1.loss_cls: 0.7584  d1.loss_mask: 0.2288  d1.loss_dice: 2.4083  d2.loss_cls: 0.6725  d2.loss_mask: 0.2112  d2.loss_dice: 2.3035  d3.loss_cls: 0.6744  d3.loss_mask: 0.2049  d3.loss_dice: 2.2076  d4.loss_cls: 0.6607  d4.loss_mask: 0.2017  d4.loss_dice: 2.1874  d5.loss_cls: 0.6415  d5.loss_mask: 0.1974  d5.loss_dice: 2.1754  d6.loss_cls: 0.6472  d6.loss_mask: 0.1936  d6.loss_dice: 2.1313  d7.loss_cls: 0.6416  d7.loss_mask: 0.1940  d7.loss_dice: 2.1280  d8.loss_cls: 0.6349  d8.loss_mask: 0.1919  d8.loss_dice: 2.1263
05/08 06:13:24 - mmengine - INFO - Iter(train) [ 7800/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:09:43  time: 1.4483  data_time: 0.0558  memory: 29212  grad_norm: 59.5857  loss: 29.3669  loss_cls: 0.6202  loss_mask: 0.1932  loss_dice: 1.9437  d0.loss_cls: 1.0716  d0.loss_mask: 0.2379  d0.loss_dice: 2.1446  d1.loss_cls: 0.7441  d1.loss_mask: 0.2293  d1.loss_dice: 2.2413  d2.loss_cls: 0.6629  d2.loss_mask: 0.2128  d2.loss_dice: 2.1399  d3.loss_cls: 0.6607  d3.loss_mask: 0.2031  d3.loss_dice: 2.0361  d4.loss_cls: 0.6368  d4.loss_mask: 0.2018  d4.loss_dice: 2.0193  d5.loss_cls: 0.6282  d5.loss_mask: 0.1984  d5.loss_dice: 2.0022  d6.loss_cls: 0.6328  d6.loss_mask: 0.1962  d6.loss_dice: 1.9587  d7.loss_cls: 0.6265  d7.loss_mask: 0.1957  d7.loss_dice: 1.9593  d8.loss_cls: 0.6194  d8.loss_mask: 0.1946  d8.loss_dice: 1.9556
05/08 06:14:36 - mmengine - INFO - Iter(train) [ 7850/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:08:19  time: 1.4219  data_time: 0.0508  memory: 29586  grad_norm: 47.1687  loss: 30.1090  loss_cls: 0.6322  loss_mask: 0.1872  loss_dice: 2.0310  d0.loss_cls: 1.0799  d0.loss_mask: 0.2290  d0.loss_dice: 2.2089  d1.loss_cls: 0.7471  d1.loss_mask: 0.2193  d1.loss_dice: 2.3141  d2.loss_cls: 0.6709  d2.loss_mask: 0.2050  d2.loss_dice: 2.2066  d3.loss_cls: 0.6477  d3.loss_mask: 0.1976  d3.loss_dice: 2.1109  d4.loss_cls: 0.6300  d4.loss_mask: 0.1966  d4.loss_dice: 2.0968  d5.loss_cls: 0.6293  d5.loss_mask: 0.1915  d5.loss_dice: 2.0874  d6.loss_cls: 0.6403  d6.loss_mask: 0.1898  d6.loss_dice: 2.0409  d7.loss_cls: 0.6288  d7.loss_mask: 0.1892  d7.loss_dice: 2.0410  d8.loss_cls: 0.6345  d8.loss_mask: 0.1882  d8.loss_dice: 2.0372
05/08 06:15:47 - mmengine - INFO - Iter(train) [ 7900/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:06:56  time: 1.4259  data_time: 0.0514  memory: 29835  grad_norm: 49.1135  loss: 30.9185  loss_cls: 0.6308  loss_mask: 0.1984  loss_dice: 2.0856  d0.loss_cls: 1.0912  d0.loss_mask: 0.2484  d0.loss_dice: 2.2937  d1.loss_cls: 0.7536  d1.loss_mask: 0.2292  d1.loss_dice: 2.3896  d2.loss_cls: 0.6683  d2.loss_mask: 0.2180  d2.loss_dice: 2.2836  d3.loss_cls: 0.6523  d3.loss_mask: 0.2090  d3.loss_dice: 2.1802  d4.loss_cls: 0.6373  d4.loss_mask: 0.2083  d4.loss_dice: 2.1628  d5.loss_cls: 0.6324  d5.loss_mask: 0.2046  d5.loss_dice: 2.1478  d6.loss_cls: 0.6371  d6.loss_mask: 0.2022  d6.loss_dice: 2.0974  d7.loss_cls: 0.6259  d7.loss_mask: 0.2012  d7.loss_dice: 2.1028  d8.loss_cls: 0.6246  d8.loss_mask: 0.2004  d8.loss_dice: 2.1017
05/08 06:16:58 - mmengine - INFO - Iter(train) [ 7950/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:05:32  time: 1.4274  data_time: 0.0501  memory: 29193  grad_norm: 54.7700  loss: 31.1249  loss_cls: 0.6363  loss_mask: 0.1974  loss_dice: 2.1013  d0.loss_cls: 1.0848  d0.loss_mask: 0.2443  d0.loss_dice: 2.3174  d1.loss_cls: 0.7706  d1.loss_mask: 0.2331  d1.loss_dice: 2.3991  d2.loss_cls: 0.6848  d2.loss_mask: 0.2176  d2.loss_dice: 2.2953  d3.loss_cls: 0.6614  d3.loss_mask: 0.2062  d3.loss_dice: 2.1915  d4.loss_cls: 0.6382  d4.loss_mask: 0.2053  d4.loss_dice: 2.1790  d5.loss_cls: 0.6336  d5.loss_mask: 0.1998  d5.loss_dice: 2.1678  d6.loss_cls: 0.6403  d6.loss_mask: 0.1991  d6.loss_dice: 2.1205  d7.loss_cls: 0.6384  d7.loss_mask: 0.1987  d7.loss_dice: 2.1149  d8.loss_cls: 0.6329  d8.loss_mask: 0.1982  d8.loss_dice: 2.1172
05/08 06:18:09 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 06:18:09 - mmengine - INFO - Iter(train) [ 8000/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:04:08  time: 1.4094  data_time: 0.0596  memory: 28325  grad_norm: 66.2085  loss: 28.6646  loss_cls: 0.6206  loss_mask: 0.1971  loss_dice: 1.8891  d0.loss_cls: 1.0566  d0.loss_mask: 0.2483  d0.loss_dice: 2.0814  d1.loss_cls: 0.7418  d1.loss_mask: 0.2338  d1.loss_dice: 2.1464  d2.loss_cls: 0.6544  d2.loss_mask: 0.2165  d2.loss_dice: 2.0619  d3.loss_cls: 0.6466  d3.loss_mask: 0.2104  d3.loss_dice: 1.9601  d4.loss_cls: 0.6202  d4.loss_mask: 0.2070  d4.loss_dice: 1.9535  d5.loss_cls: 0.6189  d5.loss_mask: 0.2021  d5.loss_dice: 1.9372  d6.loss_cls: 0.6286  d6.loss_mask: 0.2010  d6.loss_dice: 1.8997  d7.loss_cls: 0.6205  d7.loss_mask: 0.1989  d7.loss_dice: 1.8988  d8.loss_cls: 0.6145  d8.loss_mask: 0.1969  d8.loss_dice: 1.9017
05/08 06:18:09 - mmengine - INFO - Saving checkpoint at 8000 iterations
05/08 06:19:01 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9859  data_time: 0.0274  memory: 3258  
05/08 06:19:25 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.38s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 06:19:32 - mmengine - INFO - start multi processing evaluation ...
DONE (t=51.99s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.420
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.779
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.354
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.244
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.509
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.840
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.512
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.914
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.453
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.357
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.589
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.917
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.512
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.512
05/08 06:20:24 - mmengine - INFO - segm_mAP_copypaste: 0.420 0.779 0.354 0.244 0.509 0.840
05/08 06:20:25 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.4200  coco/segm_mAP_50: 0.7790  coco/segm_mAP_75: 0.3540  coco/segm_mAP_s: 0.2440  coco/segm_mAP_m: 0.5090  coco/segm_mAP_l: 0.8400  data_time: 0.0273  time: 0.9838
05/08 06:20:25 - mmengine - INFO - The previous best checkpoint /data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco/best_coco_segm_mAP_50_iter_6000.pth is removed
05/08 06:20:26 - mmengine - INFO - The best checkpoint with 0.7790 coco/segm_mAP_50 at 8000 iter is saved to best_coco_segm_mAP_50_iter_8000.pth.
05/08 06:21:40 - mmengine - INFO - Iter(train) [ 8050/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:04:55  time: 3.1824  data_time: 1.8188  memory: 28846  grad_norm: 65.9962  loss: 29.4259  loss_cls: 0.6342  loss_mask: 0.1883  loss_dice: 1.9566  d0.loss_cls: 1.0614  d0.loss_mask: 0.2347  d0.loss_dice: 2.1465  d1.loss_cls: 0.7429  d1.loss_mask: 0.2256  d1.loss_dice: 2.2241  d2.loss_cls: 0.6582  d2.loss_mask: 0.2056  d2.loss_dice: 2.1293  d3.loss_cls: 0.6671  d3.loss_mask: 0.1997  d3.loss_dice: 2.0264  d4.loss_cls: 0.6495  d4.loss_mask: 0.1976  d4.loss_dice: 2.0152  d5.loss_cls: 0.6411  d5.loss_mask: 0.1936  d5.loss_dice: 2.0118  d6.loss_cls: 0.6508  d6.loss_mask: 0.1917  d6.loss_dice: 1.9762  d7.loss_cls: 0.6340  d7.loss_mask: 0.1909  d7.loss_dice: 1.9793  d8.loss_cls: 0.6291  d8.loss_mask: 0.1895  d8.loss_dice: 1.9751
05/08 06:22:51 - mmengine - INFO - Iter(train) [ 8100/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:03:30  time: 1.4128  data_time: 0.0568  memory: 28886  grad_norm: 80.3107  loss: 30.5110  loss_cls: 0.6360  loss_mask: 0.1907  loss_dice: 2.0576  d0.loss_cls: 1.0779  d0.loss_mask: 0.2324  d0.loss_dice: 2.2300  d1.loss_cls: 0.7675  d1.loss_mask: 0.2278  d1.loss_dice: 2.3260  d2.loss_cls: 0.6768  d2.loss_mask: 0.2084  d2.loss_dice: 2.2287  d3.loss_cls: 0.6763  d3.loss_mask: 0.2021  d3.loss_dice: 2.1294  d4.loss_cls: 0.6490  d4.loss_mask: 0.2009  d4.loss_dice: 2.1205  d5.loss_cls: 0.6408  d5.loss_mask: 0.1967  d5.loss_dice: 2.1079  d6.loss_cls: 0.6579  d6.loss_mask: 0.1956  d6.loss_dice: 2.0675  d7.loss_cls: 0.6462  d7.loss_mask: 0.1943  d7.loss_dice: 2.0703  d8.loss_cls: 0.6364  d8.loss_mask: 0.1915  d8.loss_dice: 2.0679
05/08 06:24:05 - mmengine - INFO - Iter(train) [ 8150/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:02:11  time: 1.4921  data_time: 0.1057  memory: 28591  grad_norm: 70.6109  loss: 31.2803  loss_cls: 0.6622  loss_mask: 0.2175  loss_dice: 2.0810  d0.loss_cls: 1.0979  d0.loss_mask: 0.2664  d0.loss_dice: 2.2816  d1.loss_cls: 0.7716  d1.loss_mask: 0.2572  d1.loss_dice: 2.3647  d2.loss_cls: 0.6974  d2.loss_mask: 0.2361  d2.loss_dice: 2.2519  d3.loss_cls: 0.6830  d3.loss_mask: 0.2299  d3.loss_dice: 2.1650  d4.loss_cls: 0.6714  d4.loss_mask: 0.2244  d4.loss_dice: 2.1439  d5.loss_cls: 0.6804  d5.loss_mask: 0.2219  d5.loss_dice: 2.1336  d6.loss_cls: 0.6802  d6.loss_mask: 0.2239  d6.loss_dice: 2.0966  d7.loss_cls: 0.6570  d7.loss_mask: 0.2200  d7.loss_dice: 2.0963  d8.loss_cls: 0.6561  d8.loss_mask: 0.2184  d8.loss_dice: 2.0930
05/08 06:25:17 - mmengine - INFO - Iter(train) [ 8200/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 5:00:48  time: 1.4362  data_time: 0.0631  memory: 28794  grad_norm: 60.3667  loss: 31.3453  loss_cls: 0.6963  loss_mask: 0.2131  loss_dice: 2.0810  d0.loss_cls: 1.0782  d0.loss_mask: 0.2576  d0.loss_dice: 2.2757  d1.loss_cls: 0.7652  d1.loss_mask: 0.2437  d1.loss_dice: 2.3687  d2.loss_cls: 0.6816  d2.loss_mask: 0.2249  d2.loss_dice: 2.2567  d3.loss_cls: 0.6946  d3.loss_mask: 0.2213  d3.loss_dice: 2.1638  d4.loss_cls: 0.7040  d4.loss_mask: 0.2204  d4.loss_dice: 2.1429  d5.loss_cls: 0.6955  d5.loss_mask: 0.2154  d5.loss_dice: 2.1373  d6.loss_cls: 0.7002  d6.loss_mask: 0.2159  d6.loss_dice: 2.1023  d7.loss_cls: 0.6786  d7.loss_mask: 0.2112  d7.loss_dice: 2.0958  d8.loss_cls: 0.6951  d8.loss_mask: 0.2125  d8.loss_dice: 2.0957
05/08 06:26:29 - mmengine - INFO - Iter(train) [ 8250/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:59:25  time: 1.4421  data_time: 0.0613  memory: 30003  grad_norm: 42.3330  loss: 30.3428  loss_cls: 0.6519  loss_mask: 0.1998  loss_dice: 2.0229  d0.loss_cls: 1.0811  d0.loss_mask: 0.2413  d0.loss_dice: 2.2070  d1.loss_cls: 0.7546  d1.loss_mask: 0.2302  d1.loss_dice: 2.2979  d2.loss_cls: 0.6718  d2.loss_mask: 0.2159  d2.loss_dice: 2.1884  d3.loss_cls: 0.6694  d3.loss_mask: 0.2084  d3.loss_dice: 2.0936  d4.loss_cls: 0.6777  d4.loss_mask: 0.2079  d4.loss_dice: 2.0757  d5.loss_cls: 0.6693  d5.loss_mask: 0.2042  d5.loss_dice: 2.0662  d6.loss_cls: 0.6681  d6.loss_mask: 0.2029  d6.loss_dice: 2.0355  d7.loss_cls: 0.6613  d7.loss_mask: 0.2006  d7.loss_dice: 2.0286  d8.loss_cls: 0.6807  d8.loss_mask: 0.2013  d8.loss_dice: 2.0288
05/08 06:27:42 - mmengine - INFO - Iter(train) [ 8300/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:58:03  time: 1.4521  data_time: 0.0669  memory: 28000  grad_norm: 42.9832  loss: 29.8309  loss_cls: 0.6321  loss_mask: 0.1952  loss_dice: 1.9815  d0.loss_cls: 1.0850  d0.loss_mask: 0.2389  d0.loss_dice: 2.1754  d1.loss_cls: 0.7489  d1.loss_mask: 0.2249  d1.loss_dice: 2.2653  d2.loss_cls: 0.6598  d2.loss_mask: 0.2108  d2.loss_dice: 2.1609  d3.loss_cls: 0.6667  d3.loss_mask: 0.2047  d3.loss_dice: 2.0591  d4.loss_cls: 0.6568  d4.loss_mask: 0.2036  d4.loss_dice: 2.0406  d5.loss_cls: 0.6581  d5.loss_mask: 0.2008  d5.loss_dice: 2.0316  d6.loss_cls: 0.6607  d6.loss_mask: 0.1992  d6.loss_dice: 1.9956  d7.loss_cls: 0.6504  d7.loss_mask: 0.1990  d7.loss_dice: 1.9874  d8.loss_cls: 0.6451  d8.loss_mask: 0.1990  d8.loss_dice: 1.9939
05/08 06:28:54 - mmengine - INFO - Iter(train) [ 8350/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:56:40  time: 1.4370  data_time: 0.0568  memory: 28828  grad_norm: 43.1874  loss: 30.4777  loss_cls: 0.6486  loss_mask: 0.1967  loss_dice: 2.0357  d0.loss_cls: 1.0853  d0.loss_mask: 0.2457  d0.loss_dice: 2.2319  d1.loss_cls: 0.7518  d1.loss_mask: 0.2322  d1.loss_dice: 2.3193  d2.loss_cls: 0.6713  d2.loss_mask: 0.2120  d2.loss_dice: 2.2177  d3.loss_cls: 0.6742  d3.loss_mask: 0.2069  d3.loss_dice: 2.1158  d4.loss_cls: 0.6630  d4.loss_mask: 0.2054  d4.loss_dice: 2.1036  d5.loss_cls: 0.6578  d5.loss_mask: 0.2015  d5.loss_dice: 2.0830  d6.loss_cls: 0.6706  d6.loss_mask: 0.1993  d6.loss_dice: 2.0459  d7.loss_cls: 0.6641  d7.loss_mask: 0.1989  d7.loss_dice: 2.0453  d8.loss_cls: 0.6484  d8.loss_mask: 0.1971  d8.loss_dice: 2.0488
05/08 06:30:07 - mmengine - INFO - Iter(train) [ 8400/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:55:20  time: 1.4671  data_time: 0.0635  memory: 29616  grad_norm: 45.2593  loss: 31.1408  loss_cls: 0.6534  loss_mask: 0.2042  loss_dice: 2.0885  d0.loss_cls: 1.0893  d0.loss_mask: 0.2540  d0.loss_dice: 2.2949  d1.loss_cls: 0.7675  d1.loss_mask: 0.2342  d1.loss_dice: 2.3809  d2.loss_cls: 0.6758  d2.loss_mask: 0.2207  d2.loss_dice: 2.2766  d3.loss_cls: 0.6736  d3.loss_mask: 0.2136  d3.loss_dice: 2.1667  d4.loss_cls: 0.6622  d4.loss_mask: 0.2119  d4.loss_dice: 2.1564  d5.loss_cls: 0.6641  d5.loss_mask: 0.2078  d5.loss_dice: 2.1423  d6.loss_cls: 0.6700  d6.loss_mask: 0.2068  d6.loss_dice: 2.1027  d7.loss_cls: 0.6533  d7.loss_mask: 0.2065  d7.loss_dice: 2.1012  d8.loss_cls: 0.6523  d8.loss_mask: 0.2049  d8.loss_dice: 2.1046
05/08 06:31:19 - mmengine - INFO - Iter(train) [ 8450/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:53:57  time: 1.4356  data_time: 0.0566  memory: 29625  grad_norm: 59.7606  loss: 29.4165  loss_cls: 0.6361  loss_mask: 0.1987  loss_dice: 1.9521  d0.loss_cls: 1.0690  d0.loss_mask: 0.2422  d0.loss_dice: 2.1202  d1.loss_cls: 0.7291  d1.loss_mask: 0.2306  d1.loss_dice: 2.2146  d2.loss_cls: 0.6587  d2.loss_mask: 0.2173  d2.loss_dice: 2.1140  d3.loss_cls: 0.6696  d3.loss_mask: 0.2105  d3.loss_dice: 2.0150  d4.loss_cls: 0.6615  d4.loss_mask: 0.2087  d4.loss_dice: 2.0090  d5.loss_cls: 0.6489  d5.loss_mask: 0.2043  d5.loss_dice: 2.0042  d6.loss_cls: 0.6395  d6.loss_mask: 0.2035  d6.loss_dice: 1.9691  d7.loss_cls: 0.6249  d7.loss_mask: 0.2008  d7.loss_dice: 1.9707  d8.loss_cls: 0.6293  d8.loss_mask: 0.1997  d8.loss_dice: 1.9647
05/08 06:32:31 - mmengine - INFO - Iter(train) [ 8500/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:52:36  time: 1.4528  data_time: 0.0612  memory: 29588  grad_norm: 50.1082  loss: 29.9264  loss_cls: 0.6484  loss_mask: 0.1928  loss_dice: 1.9856  d0.loss_cls: 1.0823  d0.loss_mask: 0.2369  d0.loss_dice: 2.1783  d1.loss_cls: 0.7543  d1.loss_mask: 0.2254  d1.loss_dice: 2.2681  d2.loss_cls: 0.6730  d2.loss_mask: 0.2097  d2.loss_dice: 2.1626  d3.loss_cls: 0.6806  d3.loss_mask: 0.2028  d3.loss_dice: 2.0671  d4.loss_cls: 0.6718  d4.loss_mask: 0.2014  d4.loss_dice: 2.0608  d5.loss_cls: 0.6516  d5.loss_mask: 0.1983  d5.loss_dice: 2.0464  d6.loss_cls: 0.6542  d6.loss_mask: 0.1959  d6.loss_dice: 2.0018  d7.loss_cls: 0.6377  d7.loss_mask: 0.1949  d7.loss_dice: 2.0075  d8.loss_cls: 0.6383  d8.loss_mask: 0.1935  d8.loss_dice: 2.0045
05/08 06:33:45 - mmengine - INFO - Iter(train) [ 8550/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:51:15  time: 1.4650  data_time: 0.1095  memory: 28955  grad_norm: 63.2919  loss: 29.0341  loss_cls: 0.6333  loss_mask: 0.1809  loss_dice: 1.9282  d0.loss_cls: 1.0516  d0.loss_mask: 0.2199  d0.loss_dice: 2.1007  d1.loss_cls: 0.7363  d1.loss_mask: 0.2134  d1.loss_dice: 2.1927  d2.loss_cls: 0.6647  d2.loss_mask: 0.2002  d2.loss_dice: 2.0860  d3.loss_cls: 0.6700  d3.loss_mask: 0.1939  d3.loss_dice: 2.0043  d4.loss_cls: 0.6577  d4.loss_mask: 0.1903  d4.loss_dice: 2.0039  d5.loss_cls: 0.6418  d5.loss_mask: 0.1865  d5.loss_dice: 1.9910  d6.loss_cls: 0.6453  d6.loss_mask: 0.1861  d6.loss_dice: 1.9370  d7.loss_cls: 0.6326  d7.loss_mask: 0.1834  d7.loss_dice: 1.9435  d8.loss_cls: 0.6275  d8.loss_mask: 0.1828  d8.loss_dice: 1.9487
05/08 06:34:56 - mmengine - INFO - Iter(train) [ 8600/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:49:53  time: 1.4351  data_time: 0.0553  memory: 28573  grad_norm: 63.0021  loss: 28.3447  loss_cls: 0.6509  loss_mask: 0.1908  loss_dice: 1.8446  d0.loss_cls: 1.0526  d0.loss_mask: 0.2354  d0.loss_dice: 2.0326  d1.loss_cls: 0.7334  d1.loss_mask: 0.2208  d1.loss_dice: 2.1092  d2.loss_cls: 0.6492  d2.loss_mask: 0.2064  d2.loss_dice: 2.0272  d3.loss_cls: 0.6572  d3.loss_mask: 0.2008  d3.loss_dice: 1.9314  d4.loss_cls: 0.6384  d4.loss_mask: 0.1994  d4.loss_dice: 1.9311  d5.loss_cls: 0.6320  d5.loss_mask: 0.1971  d5.loss_dice: 1.9071  d6.loss_cls: 0.6540  d6.loss_mask: 0.1940  d6.loss_dice: 1.8563  d7.loss_cls: 0.6438  d7.loss_mask: 0.1930  d7.loss_dice: 1.8625  d8.loss_cls: 0.6378  d8.loss_mask: 0.1922  d8.loss_dice: 1.8635
05/08 06:36:09 - mmengine - INFO - Iter(train) [ 8650/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:48:31  time: 1.4443  data_time: 0.0585  memory: 28878  grad_norm: 55.9855  loss: 31.3582  loss_cls: 0.6817  loss_mask: 0.2031  loss_dice: 2.1141  d0.loss_cls: 1.0906  d0.loss_mask: 0.2508  d0.loss_dice: 2.3013  d1.loss_cls: 0.7548  d1.loss_mask: 0.2373  d1.loss_dice: 2.4006  d2.loss_cls: 0.6719  d2.loss_mask: 0.2239  d2.loss_dice: 2.2927  d3.loss_cls: 0.6772  d3.loss_mask: 0.2153  d3.loss_dice: 2.1962  d4.loss_cls: 0.6601  d4.loss_mask: 0.2119  d4.loss_dice: 2.1775  d5.loss_cls: 0.6455  d5.loss_mask: 0.2090  d5.loss_dice: 2.1724  d6.loss_cls: 0.6735  d6.loss_mask: 0.2068  d6.loss_dice: 2.1199  d7.loss_cls: 0.6616  d7.loss_mask: 0.2044  d7.loss_dice: 2.1235  d8.loss_cls: 0.6521  d8.loss_mask: 0.2034  d8.loss_dice: 2.1252
05/08 06:37:21 - mmengine - INFO - Iter(train) [ 8700/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:47:09  time: 1.4413  data_time: 0.0555  memory: 29458  grad_norm: 51.7035  loss: 30.7456  loss_cls: 0.6650  loss_mask: 0.2020  loss_dice: 2.0541  d0.loss_cls: 1.0887  d0.loss_mask: 0.2481  d0.loss_dice: 2.2597  d1.loss_cls: 0.7574  d1.loss_mask: 0.2396  d1.loss_dice: 2.3309  d2.loss_cls: 0.6712  d2.loss_mask: 0.2182  d2.loss_dice: 2.2425  d3.loss_cls: 0.6708  d3.loss_mask: 0.2119  d3.loss_dice: 2.1428  d4.loss_cls: 0.6479  d4.loss_mask: 0.2100  d4.loss_dice: 2.1292  d5.loss_cls: 0.6496  d5.loss_mask: 0.2067  d5.loss_dice: 2.1159  d6.loss_cls: 0.6603  d6.loss_mask: 0.2048  d6.loss_dice: 2.0680  d7.loss_cls: 0.6503  d7.loss_mask: 0.2037  d7.loss_dice: 2.0743  d8.loss_cls: 0.6424  d8.loss_mask: 0.2023  d8.loss_dice: 2.0770
05/08 06:38:31 - mmengine - INFO - Iter(train) [ 8750/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:45:46  time: 1.4144  data_time: 0.0530  memory: 28756  grad_norm: 59.4417  loss: 30.6304  loss_cls: 0.6454  loss_mask: 0.2096  loss_dice: 2.0448  d0.loss_cls: 1.0869  d0.loss_mask: 0.2587  d0.loss_dice: 2.2517  d1.loss_cls: 0.7342  d1.loss_mask: 0.2464  d1.loss_dice: 2.3286  d2.loss_cls: 0.6640  d2.loss_mask: 0.2287  d2.loss_dice: 2.2304  d3.loss_cls: 0.6592  d3.loss_mask: 0.2221  d3.loss_dice: 2.1319  d4.loss_cls: 0.6433  d4.loss_mask: 0.2197  d4.loss_dice: 2.1213  d5.loss_cls: 0.6386  d5.loss_mask: 0.2151  d5.loss_dice: 2.1082  d6.loss_cls: 0.6516  d6.loss_mask: 0.2139  d6.loss_dice: 2.0560  d7.loss_cls: 0.6376  d7.loss_mask: 0.2126  d7.loss_dice: 2.0671  d8.loss_cls: 0.6305  d8.loss_mask: 0.2098  d8.loss_dice: 2.0626
05/08 06:39:44 - mmengine - INFO - Iter(train) [ 8800/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:44:26  time: 1.4569  data_time: 0.0625  memory: 29272  grad_norm: 63.6335  loss: 30.1944  loss_cls: 0.6614  loss_mask: 0.1960  loss_dice: 2.0105  d0.loss_cls: 1.0854  d0.loss_mask: 0.2367  d0.loss_dice: 2.2002  d1.loss_cls: 0.7471  d1.loss_mask: 0.2264  d1.loss_dice: 2.2811  d2.loss_cls: 0.6751  d2.loss_mask: 0.2168  d2.loss_dice: 2.1777  d3.loss_cls: 0.6813  d3.loss_mask: 0.2069  d3.loss_dice: 2.0850  d4.loss_cls: 0.6671  d4.loss_mask: 0.2029  d4.loss_dice: 2.0740  d5.loss_cls: 0.6634  d5.loss_mask: 0.1990  d5.loss_dice: 2.0712  d6.loss_cls: 0.6627  d6.loss_mask: 0.1984  d6.loss_dice: 2.0234  d7.loss_cls: 0.6485  d7.loss_mask: 0.1980  d7.loss_dice: 2.0347  d8.loss_cls: 0.6428  d8.loss_mask: 0.1959  d8.loss_dice: 2.0249
05/08 06:40:56 - mmengine - INFO - Iter(train) [ 8850/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:43:04  time: 1.4451  data_time: 0.0513  memory: 29320  grad_norm: 62.6887  loss: 30.9393  loss_cls: 0.6530  loss_mask: 0.2021  loss_dice: 2.0763  d0.loss_cls: 1.0943  d0.loss_mask: 0.2499  d0.loss_dice: 2.2693  d1.loss_cls: 0.7634  d1.loss_mask: 0.2323  d1.loss_dice: 2.3544  d2.loss_cls: 0.6744  d2.loss_mask: 0.2176  d2.loss_dice: 2.2708  d3.loss_cls: 0.6588  d3.loss_mask: 0.2127  d3.loss_dice: 2.1694  d4.loss_cls: 0.6468  d4.loss_mask: 0.2109  d4.loss_dice: 2.1536  d5.loss_cls: 0.6421  d5.loss_mask: 0.2086  d5.loss_dice: 2.1434  d6.loss_cls: 0.6561  d6.loss_mask: 0.2052  d6.loss_dice: 2.0939  d7.loss_cls: 0.6477  d7.loss_mask: 0.2045  d7.loss_dice: 2.0971  d8.loss_cls: 0.6384  d8.loss_mask: 0.2028  d8.loss_dice: 2.0893
05/08 06:42:10 - mmengine - INFO - Iter(train) [ 8900/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:41:44  time: 1.4631  data_time: 0.0524  memory: 29728  grad_norm: 68.5621  loss: 30.0047  loss_cls: 0.6542  loss_mask: 0.1943  loss_dice: 2.0037  d0.loss_cls: 1.0861  d0.loss_mask: 0.2389  d0.loss_dice: 2.1893  d1.loss_cls: 0.7454  d1.loss_mask: 0.2227  d1.loss_dice: 2.2780  d2.loss_cls: 0.6625  d2.loss_mask: 0.2127  d2.loss_dice: 2.1817  d3.loss_cls: 0.6541  d3.loss_mask: 0.2031  d3.loss_dice: 2.0849  d4.loss_cls: 0.6423  d4.loss_mask: 0.2011  d4.loss_dice: 2.0725  d5.loss_cls: 0.6353  d5.loss_mask: 0.1990  d5.loss_dice: 2.0605  d6.loss_cls: 0.6515  d6.loss_mask: 0.1984  d6.loss_dice: 2.0168  d7.loss_cls: 0.6419  d7.loss_mask: 0.1957  d7.loss_dice: 2.0269  d8.loss_cls: 0.6339  d8.loss_mask: 0.1946  d8.loss_dice: 2.0228
05/08 06:43:24 - mmengine - INFO - Iter(train) [ 8950/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:40:26  time: 1.4879  data_time: 0.1033  memory: 31117  grad_norm: 60.2404  loss: 30.7892  loss_cls: 0.6651  loss_mask: 0.1972  loss_dice: 2.0556  d0.loss_cls: 1.0945  d0.loss_mask: 0.2402  d0.loss_dice: 2.2603  d1.loss_cls: 0.7480  d1.loss_mask: 0.2300  d1.loss_dice: 2.3623  d2.loss_cls: 0.6669  d2.loss_mask: 0.2183  d2.loss_dice: 2.2597  d3.loss_cls: 0.6648  d3.loss_mask: 0.2086  d3.loss_dice: 2.1515  d4.loss_cls: 0.6473  d4.loss_mask: 0.2065  d4.loss_dice: 2.1353  d5.loss_cls: 0.6466  d5.loss_mask: 0.2012  d5.loss_dice: 2.1216  d6.loss_cls: 0.6780  d6.loss_mask: 0.2006  d6.loss_dice: 2.0764  d7.loss_cls: 0.6496  d7.loss_mask: 0.1994  d7.loss_dice: 2.0855  d8.loss_cls: 0.6477  d8.loss_mask: 0.1968  d8.loss_dice: 2.0739
05/08 06:44:36 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 06:44:36 - mmengine - INFO - Iter(train) [ 9000/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:39:05  time: 1.4387  data_time: 0.0562  memory: 29168  grad_norm: 65.9745  loss: 30.1529  loss_cls: 0.6450  loss_mask: 0.1986  loss_dice: 2.0170  d0.loss_cls: 1.0704  d0.loss_mask: 0.2408  d0.loss_dice: 2.2112  d1.loss_cls: 0.7539  d1.loss_mask: 0.2271  d1.loss_dice: 2.2882  d2.loss_cls: 0.6643  d2.loss_mask: 0.2170  d2.loss_dice: 2.1934  d3.loss_cls: 0.6699  d3.loss_mask: 0.2085  d3.loss_dice: 2.0832  d4.loss_cls: 0.6532  d4.loss_mask: 0.2065  d4.loss_dice: 2.0744  d5.loss_cls: 0.6458  d5.loss_mask: 0.2028  d5.loss_dice: 2.0662  d6.loss_cls: 0.6613  d6.loss_mask: 0.1997  d6.loss_dice: 2.0258  d7.loss_cls: 0.6411  d7.loss_mask: 0.1988  d7.loss_dice: 2.0315  d8.loss_cls: 0.6385  d8.loss_mask: 0.1988  d8.loss_dice: 2.0202
05/08 06:44:36 - mmengine - INFO - Saving checkpoint at 9000 iterations
05/08 06:45:28 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9764  data_time: 0.0269  memory: 3258  
05/08 06:45:50 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.47s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 06:45:58 - mmengine - INFO - start multi processing evaluation ...
DONE (t=53.73s).
Accumulating evaluation results...
DONE (t=0.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.380
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.744
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.322
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.246
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.435
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.885
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.505
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.875
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.469
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.367
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.568
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.933
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.505
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.505
05/08 06:46:52 - mmengine - INFO - segm_mAP_copypaste: 0.380 0.744 0.322 0.246 0.435 0.885
05/08 06:46:53 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.3800  coco/segm_mAP_50: 0.7440  coco/segm_mAP_75: 0.3220  coco/segm_mAP_s: 0.2460  coco/segm_mAP_m: 0.4350  coco/segm_mAP_l: 0.8850  data_time: 0.0268  time: 0.9744
05/08 06:48:05 - mmengine - INFO - Iter(train) [ 9050/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:39:27  time: 3.1409  data_time: 1.7672  memory: 29299  grad_norm: 69.0585  loss: 30.8952  loss_cls: 0.6855  loss_mask: 0.2022  loss_dice: 2.0558  d0.loss_cls: 1.0923  d0.loss_mask: 0.2435  d0.loss_dice: 2.2511  d1.loss_cls: 0.7658  d1.loss_mask: 0.2363  d1.loss_dice: 2.3214  d2.loss_cls: 0.6830  d2.loss_mask: 0.2231  d2.loss_dice: 2.2258  d3.loss_cls: 0.6983  d3.loss_mask: 0.2161  d3.loss_dice: 2.1226  d4.loss_cls: 0.6860  d4.loss_mask: 0.2130  d4.loss_dice: 2.1202  d5.loss_cls: 0.6838  d5.loss_mask: 0.2080  d5.loss_dice: 2.1037  d6.loss_cls: 0.6974  d6.loss_mask: 0.2078  d6.loss_dice: 2.0645  d7.loss_cls: 0.6756  d7.loss_mask: 0.2056  d7.loss_dice: 2.0727  d8.loss_cls: 0.6692  d8.loss_mask: 0.2025  d8.loss_dice: 2.0623
05/08 06:49:17 - mmengine - INFO - Iter(train) [ 9100/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:38:05  time: 1.4438  data_time: 0.0561  memory: 28298  grad_norm: 52.9928  loss: 28.4637  loss_cls: 0.6616  loss_mask: 0.1862  loss_dice: 1.8549  d0.loss_cls: 1.0668  d0.loss_mask: 0.2256  d0.loss_dice: 2.0509  d1.loss_cls: 0.7416  d1.loss_mask: 0.2150  d1.loss_dice: 2.1117  d2.loss_cls: 0.6615  d2.loss_mask: 0.2013  d2.loss_dice: 2.0141  d3.loss_cls: 0.6844  d3.loss_mask: 0.1958  d3.loss_dice: 1.9368  d4.loss_cls: 0.6568  d4.loss_mask: 0.1924  d4.loss_dice: 1.9196  d5.loss_cls: 0.6565  d5.loss_mask: 0.1896  d5.loss_dice: 1.9113  d6.loss_cls: 0.6475  d6.loss_mask: 0.1882  d6.loss_dice: 1.8754  d7.loss_cls: 0.6427  d7.loss_mask: 0.1862  d7.loss_dice: 1.8744  d8.loss_cls: 0.6600  d8.loss_mask: 0.1863  d8.loss_dice: 1.8686
05/08 06:50:28 - mmengine - INFO - Iter(train) [ 9150/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:36:42  time: 1.4220  data_time: 0.0533  memory: 28236  grad_norm: 61.0764  loss: 30.0632  loss_cls: 0.6605  loss_mask: 0.1913  loss_dice: 2.0068  d0.loss_cls: 1.0817  d0.loss_mask: 0.2378  d0.loss_dice: 2.1858  d1.loss_cls: 0.7306  d1.loss_mask: 0.2218  d1.loss_dice: 2.2636  d2.loss_cls: 0.6650  d2.loss_mask: 0.2076  d2.loss_dice: 2.1790  d3.loss_cls: 0.6695  d3.loss_mask: 0.1988  d3.loss_dice: 2.1001  d4.loss_cls: 0.6533  d4.loss_mask: 0.1992  d4.loss_dice: 2.0752  d5.loss_cls: 0.6574  d5.loss_mask: 0.1966  d5.loss_dice: 2.0669  d6.loss_cls: 0.6587  d6.loss_mask: 0.1932  d6.loss_dice: 2.0295  d7.loss_cls: 0.6353  d7.loss_mask: 0.1941  d7.loss_dice: 2.0445  d8.loss_cls: 0.6403  d8.loss_mask: 0.1920  d8.loss_dice: 2.0270
05/08 06:51:39 - mmengine - INFO - Iter(train) [ 9200/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:35:19  time: 1.4149  data_time: 0.0548  memory: 29000  grad_norm: 62.0789  loss: 29.8530  loss_cls: 0.6636  loss_mask: 0.1898  loss_dice: 1.9825  d0.loss_cls: 1.0674  d0.loss_mask: 0.2297  d0.loss_dice: 2.1784  d1.loss_cls: 0.7448  d1.loss_mask: 0.2201  d1.loss_dice: 2.2557  d2.loss_cls: 0.6787  d2.loss_mask: 0.2051  d2.loss_dice: 2.1607  d3.loss_cls: 0.6832  d3.loss_mask: 0.2004  d3.loss_dice: 2.0645  d4.loss_cls: 0.6704  d4.loss_mask: 0.1960  d4.loss_dice: 2.0455  d5.loss_cls: 0.6599  d5.loss_mask: 0.1937  d5.loss_dice: 2.0414  d6.loss_cls: 0.6585  d6.loss_mask: 0.1900  d6.loss_dice: 1.9970  d7.loss_cls: 0.6575  d7.loss_mask: 0.1897  d7.loss_dice: 1.9917  d8.loss_cls: 0.6535  d8.loss_mask: 0.1884  d8.loss_dice: 1.9949
05/08 06:52:50 - mmengine - INFO - Iter(train) [ 9250/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:33:56  time: 1.4286  data_time: 0.0603  memory: 29081  grad_norm: 45.6650  loss: 30.1082  loss_cls: 0.6278  loss_mask: 0.1995  loss_dice: 2.0129  d0.loss_cls: 1.0753  d0.loss_mask: 0.2453  d0.loss_dice: 2.2055  d1.loss_cls: 0.7324  d1.loss_mask: 0.2335  d1.loss_dice: 2.2788  d2.loss_cls: 0.6616  d2.loss_mask: 0.2186  d2.loss_dice: 2.1977  d3.loss_cls: 0.6761  d3.loss_mask: 0.2122  d3.loss_dice: 2.0911  d4.loss_cls: 0.6621  d4.loss_mask: 0.2079  d4.loss_dice: 2.0720  d5.loss_cls: 0.6411  d5.loss_mask: 0.2039  d5.loss_dice: 2.0730  d6.loss_cls: 0.6347  d6.loss_mask: 0.2015  d6.loss_dice: 2.0248  d7.loss_cls: 0.6387  d7.loss_mask: 0.2020  d7.loss_dice: 2.0188  d8.loss_cls: 0.6467  d8.loss_mask: 0.1997  d8.loss_dice: 2.0129
05/08 06:54:02 - mmengine - INFO - Iter(train) [ 9300/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:32:35  time: 1.4436  data_time: 0.0572  memory: 29644  grad_norm: 48.3909  loss: 31.2003  loss_cls: 0.6409  loss_mask: 0.2048  loss_dice: 2.0856  d0.loss_cls: 1.1001  d0.loss_mask: 0.2476  d0.loss_dice: 2.3086  d1.loss_cls: 0.7635  d1.loss_mask: 0.2384  d1.loss_dice: 2.3672  d2.loss_cls: 0.6927  d2.loss_mask: 0.2223  d2.loss_dice: 2.2671  d3.loss_cls: 0.7031  d3.loss_mask: 0.2165  d3.loss_dice: 2.1707  d4.loss_cls: 0.6940  d4.loss_mask: 0.2138  d4.loss_dice: 2.1526  d5.loss_cls: 0.6697  d5.loss_mask: 0.2097  d5.loss_dice: 2.1480  d6.loss_cls: 0.6597  d6.loss_mask: 0.2059  d6.loss_dice: 2.0975  d7.loss_cls: 0.6606  d7.loss_mask: 0.2055  d7.loss_dice: 2.0963  d8.loss_cls: 0.6595  d8.loss_mask: 0.2036  d8.loss_dice: 2.0949
05/08 06:55:16 - mmengine - INFO - Iter(train) [ 9350/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:31:16  time: 1.4815  data_time: 0.0996  memory: 29663  grad_norm: 59.8767  loss: 30.5084  loss_cls: 0.6416  loss_mask: 0.1941  loss_dice: 2.0470  d0.loss_cls: 1.1088  d0.loss_mask: 0.2369  d0.loss_dice: 2.2405  d1.loss_cls: 0.7553  d1.loss_mask: 0.2219  d1.loss_dice: 2.3089  d2.loss_cls: 0.6741  d2.loss_mask: 0.2095  d2.loss_dice: 2.2208  d3.loss_cls: 0.6809  d3.loss_mask: 0.2007  d3.loss_dice: 2.1195  d4.loss_cls: 0.6650  d4.loss_mask: 0.2016  d4.loss_dice: 2.1089  d5.loss_cls: 0.6627  d5.loss_mask: 0.2000  d5.loss_dice: 2.1000  d6.loss_cls: 0.6594  d6.loss_mask: 0.1959  d6.loss_dice: 2.0550  d7.loss_cls: 0.6387  d7.loss_mask: 0.1964  d7.loss_dice: 2.0649  d8.loss_cls: 0.6508  d8.loss_mask: 0.1943  d8.loss_dice: 2.0543
05/08 06:56:28 - mmengine - INFO - Iter(train) [ 9400/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:29:54  time: 1.4255  data_time: 0.0618  memory: 28374  grad_norm: 54.2286  loss: 30.1825  loss_cls: 0.6408  loss_mask: 0.2007  loss_dice: 2.0130  d0.loss_cls: 1.0790  d0.loss_mask: 0.2462  d0.loss_dice: 2.1949  d1.loss_cls: 0.7479  d1.loss_mask: 0.2326  d1.loss_dice: 2.2682  d2.loss_cls: 0.6687  d2.loss_mask: 0.2193  d2.loss_dice: 2.1834  d3.loss_cls: 0.6686  d3.loss_mask: 0.2089  d3.loss_dice: 2.0837  d4.loss_cls: 0.6573  d4.loss_mask: 0.2077  d4.loss_dice: 2.0754  d5.loss_cls: 0.6631  d5.loss_mask: 0.2063  d5.loss_dice: 2.0669  d6.loss_cls: 0.6697  d6.loss_mask: 0.2049  d6.loss_dice: 2.0235  d7.loss_cls: 0.6419  d7.loss_mask: 0.2048  d7.loss_dice: 2.0320  d8.loss_cls: 0.6465  d8.loss_mask: 0.2007  d8.loss_dice: 2.0258
05/08 06:57:40 - mmengine - INFO - Iter(train) [ 9450/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:28:33  time: 1.4435  data_time: 0.0572  memory: 29758  grad_norm: 52.6931  loss: 31.1617  loss_cls: 0.6450  loss_mask: 0.2014  loss_dice: 2.1123  d0.loss_cls: 1.0988  d0.loss_mask: 0.2409  d0.loss_dice: 2.2933  d1.loss_cls: 0.7570  d1.loss_mask: 0.2289  d1.loss_dice: 2.3563  d2.loss_cls: 0.6767  d2.loss_mask: 0.2177  d2.loss_dice: 2.2708  d3.loss_cls: 0.6887  d3.loss_mask: 0.2093  d3.loss_dice: 2.1648  d4.loss_cls: 0.6696  d4.loss_mask: 0.2082  d4.loss_dice: 2.1634  d5.loss_cls: 0.6617  d5.loss_mask: 0.2060  d5.loss_dice: 2.1568  d6.loss_cls: 0.6734  d6.loss_mask: 0.2035  d6.loss_dice: 2.1123  d7.loss_cls: 0.6600  d7.loss_mask: 0.2016  d7.loss_dice: 2.1164  d8.loss_cls: 0.6466  d8.loss_mask: 0.2011  d8.loss_dice: 2.1192
05/08 06:58:52 - mmengine - INFO - Iter(train) [ 9500/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:27:12  time: 1.4495  data_time: 0.0629  memory: 28543  grad_norm: 42.7463  loss: 30.7076  loss_cls: 0.6258  loss_mask: 0.1971  loss_dice: 2.0744  d0.loss_cls: 1.0937  d0.loss_mask: 0.2405  d0.loss_dice: 2.2762  d1.loss_cls: 0.7467  d1.loss_mask: 0.2293  d1.loss_dice: 2.3278  d2.loss_cls: 0.6592  d2.loss_mask: 0.2150  d2.loss_dice: 2.2509  d3.loss_cls: 0.6640  d3.loss_mask: 0.2074  d3.loss_dice: 2.1433  d4.loss_cls: 0.6442  d4.loss_mask: 0.2074  d4.loss_dice: 2.1453  d5.loss_cls: 0.6417  d5.loss_mask: 0.2024  d5.loss_dice: 2.1349  d6.loss_cls: 0.6531  d6.loss_mask: 0.1997  d6.loss_dice: 2.0882  d7.loss_cls: 0.6427  d7.loss_mask: 0.1989  d7.loss_dice: 2.0783  d8.loss_cls: 0.6244  d8.loss_mask: 0.1993  d8.loss_dice: 2.0956
05/08 07:00:05 - mmengine - INFO - Iter(train) [ 9550/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:25:52  time: 1.4538  data_time: 0.0636  memory: 29558  grad_norm: 40.4478  loss: 30.8581  loss_cls: 0.6395  loss_mask: 0.2003  loss_dice: 2.0798  d0.loss_cls: 1.0927  d0.loss_mask: 0.2480  d0.loss_dice: 2.2835  d1.loss_cls: 0.7491  d1.loss_mask: 0.2348  d1.loss_dice: 2.3437  d2.loss_cls: 0.6729  d2.loss_mask: 0.2184  d2.loss_dice: 2.2583  d3.loss_cls: 0.6676  d3.loss_mask: 0.2107  d3.loss_dice: 2.1555  d4.loss_cls: 0.6469  d4.loss_mask: 0.2100  d4.loss_dice: 2.1451  d5.loss_cls: 0.6455  d5.loss_mask: 0.2068  d5.loss_dice: 2.1349  d6.loss_cls: 0.6612  d6.loss_mask: 0.2042  d6.loss_dice: 2.0854  d7.loss_cls: 0.6432  d7.loss_mask: 0.2039  d7.loss_dice: 2.0875  d8.loss_cls: 0.6348  d8.loss_mask: 0.2022  d8.loss_dice: 2.0916
05/08 07:01:17 - mmengine - INFO - Iter(train) [ 9600/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:24:30  time: 1.4317  data_time: 0.0601  memory: 28709  grad_norm: 55.3340  loss: 29.0882  loss_cls: 0.6345  loss_mask: 0.1966  loss_dice: 1.9170  d0.loss_cls: 1.0706  d0.loss_mask: 0.2436  d0.loss_dice: 2.1285  d1.loss_cls: 0.7416  d1.loss_mask: 0.2293  d1.loss_dice: 2.1678  d2.loss_cls: 0.6595  d2.loss_mask: 0.2139  d2.loss_dice: 2.0862  d3.loss_cls: 0.6484  d3.loss_mask: 0.2077  d3.loss_dice: 1.9885  d4.loss_cls: 0.6362  d4.loss_mask: 0.2064  d4.loss_dice: 1.9807  d5.loss_cls: 0.6434  d5.loss_mask: 0.2022  d5.loss_dice: 1.9708  d6.loss_cls: 0.6652  d6.loss_mask: 0.2023  d6.loss_dice: 1.9203  d7.loss_cls: 0.6356  d7.loss_mask: 0.2006  d7.loss_dice: 1.9294  d8.loss_cls: 0.6372  d8.loss_mask: 0.1984  d8.loss_dice: 1.9259
05/08 07:02:29 - mmengine - INFO - Iter(train) [ 9650/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:23:10  time: 1.4576  data_time: 0.0565  memory: 29334  grad_norm: 53.6535  loss: 30.6429  loss_cls: 0.6722  loss_mask: 0.1958  loss_dice: 2.0231  d0.loss_cls: 1.0831  d0.loss_mask: 0.2398  d0.loss_dice: 2.2554  d1.loss_cls: 0.7557  d1.loss_mask: 0.2267  d1.loss_dice: 2.3283  d2.loss_cls: 0.6847  d2.loss_mask: 0.2165  d2.loss_dice: 2.2221  d3.loss_cls: 0.6840  d3.loss_mask: 0.2076  d3.loss_dice: 2.1140  d4.loss_cls: 0.6762  d4.loss_mask: 0.2059  d4.loss_dice: 2.0970  d5.loss_cls: 0.6828  d5.loss_mask: 0.2020  d5.loss_dice: 2.0883  d6.loss_cls: 0.7045  d6.loss_mask: 0.2008  d6.loss_dice: 2.0368  d7.loss_cls: 0.6803  d7.loss_mask: 0.1981  d7.loss_dice: 2.0523  d8.loss_cls: 0.6741  d8.loss_mask: 0.1957  d8.loss_dice: 2.0391
05/08 07:03:41 - mmengine - INFO - Iter(train) [ 9700/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:21:49  time: 1.4375  data_time: 0.0608  memory: 29016  grad_norm: 56.8860  loss: 30.0460  loss_cls: 0.6605  loss_mask: 0.1963  loss_dice: 1.9820  d0.loss_cls: 1.0765  d0.loss_mask: 0.2399  d0.loss_dice: 2.1720  d1.loss_cls: 0.7527  d1.loss_mask: 0.2260  d1.loss_dice: 2.2379  d2.loss_cls: 0.6728  d2.loss_mask: 0.2138  d2.loss_dice: 2.1587  d3.loss_cls: 0.6732  d3.loss_mask: 0.2053  d3.loss_dice: 2.0538  d4.loss_cls: 0.6811  d4.loss_mask: 0.2053  d4.loss_dice: 2.0435  d5.loss_cls: 0.6812  d5.loss_mask: 0.2033  d5.loss_dice: 2.0479  d6.loss_cls: 0.6969  d6.loss_mask: 0.2015  d6.loss_dice: 1.9959  d7.loss_cls: 0.7088  d7.loss_mask: 0.1993  d7.loss_dice: 1.9902  d8.loss_cls: 0.6746  d8.loss_mask: 0.1970  d8.loss_dice: 1.9981
05/08 07:04:53 - mmengine - INFO - Iter(train) [ 9750/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:20:29  time: 1.4441  data_time: 0.1019  memory: 28058  grad_norm: 58.4998  loss: 29.4741  loss_cls: 0.6376  loss_mask: 0.1892  loss_dice: 1.9544  d0.loss_cls: 1.0765  d0.loss_mask: 0.2320  d0.loss_dice: 2.1516  d1.loss_cls: 0.7439  d1.loss_mask: 0.2207  d1.loss_dice: 2.1880  d2.loss_cls: 0.6631  d2.loss_mask: 0.2063  d2.loss_dice: 2.1144  d3.loss_cls: 0.6581  d3.loss_mask: 0.1999  d3.loss_dice: 2.0275  d4.loss_cls: 0.6513  d4.loss_mask: 0.1974  d4.loss_dice: 2.0173  d5.loss_cls: 0.6520  d5.loss_mask: 0.1952  d5.loss_dice: 2.0180  d6.loss_cls: 0.6751  d6.loss_mask: 0.1943  d6.loss_dice: 1.9734  d7.loss_cls: 0.6514  d7.loss_mask: 0.1935  d7.loss_dice: 1.9822  d8.loss_cls: 0.6482  d8.loss_mask: 0.1914  d8.loss_dice: 1.9704
05/08 07:06:06 - mmengine - INFO - Iter(train) [ 9800/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:19:08  time: 1.4447  data_time: 0.0606  memory: 29827  grad_norm: 57.2175  loss: 29.0191  loss_cls: 0.6267  loss_mask: 0.1776  loss_dice: 1.9426  d0.loss_cls: 1.0633  d0.loss_mask: 0.2217  d0.loss_dice: 2.1296  d1.loss_cls: 0.7346  d1.loss_mask: 0.2079  d1.loss_dice: 2.1658  d2.loss_cls: 0.6624  d2.loss_mask: 0.1923  d2.loss_dice: 2.0954  d3.loss_cls: 0.6525  d3.loss_mask: 0.1855  d3.loss_dice: 2.0050  d4.loss_cls: 0.6354  d4.loss_mask: 0.1828  d4.loss_dice: 1.9963  d5.loss_cls: 0.6358  d5.loss_mask: 0.1801  d5.loss_dice: 1.9926  d6.loss_cls: 0.6552  d6.loss_mask: 0.1796  d6.loss_dice: 1.9549  d7.loss_cls: 0.6327  d7.loss_mask: 0.1800  d7.loss_dice: 1.9644  d8.loss_cls: 0.6447  d8.loss_mask: 0.1794  d8.loss_dice: 1.9424
05/08 07:07:18 - mmengine - INFO - Iter(train) [ 9850/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:17:48  time: 1.4458  data_time: 0.0607  memory: 30294  grad_norm: 41.3353  loss: 31.6901  loss_cls: 0.6558  loss_mask: 0.2090  loss_dice: 2.1305  d0.loss_cls: 1.1100  d0.loss_mask: 0.2520  d0.loss_dice: 2.3653  d1.loss_cls: 0.7554  d1.loss_mask: 0.2446  d1.loss_dice: 2.4120  d2.loss_cls: 0.6905  d2.loss_mask: 0.2292  d2.loss_dice: 2.3195  d3.loss_cls: 0.6792  d3.loss_mask: 0.2190  d3.loss_dice: 2.2138  d4.loss_cls: 0.6520  d4.loss_mask: 0.2170  d4.loss_dice: 2.2005  d5.loss_cls: 0.6555  d5.loss_mask: 0.2131  d5.loss_dice: 2.1949  d6.loss_cls: 0.6703  d6.loss_mask: 0.2124  d6.loss_dice: 2.1459  d7.loss_cls: 0.6546  d7.loss_mask: 0.2108  d7.loss_dice: 2.1587  d8.loss_cls: 0.6606  d8.loss_mask: 0.2114  d8.loss_dice: 2.1466
05/08 07:08:30 - mmengine - INFO - Iter(train) [ 9900/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:16:28  time: 1.4457  data_time: 0.0609  memory: 30110  grad_norm: 47.6332  loss: 30.9994  loss_cls: 0.6660  loss_mask: 0.1955  loss_dice: 2.0598  d0.loss_cls: 1.0975  d0.loss_mask: 0.2363  d0.loss_dice: 2.2827  d1.loss_cls: 0.7690  d1.loss_mask: 0.2339  d1.loss_dice: 2.3303  d2.loss_cls: 0.7030  d2.loss_mask: 0.2176  d2.loss_dice: 2.2425  d3.loss_cls: 0.7029  d3.loss_mask: 0.2100  d3.loss_dice: 2.1361  d4.loss_cls: 0.6823  d4.loss_mask: 0.2027  d4.loss_dice: 2.1331  d5.loss_cls: 0.6770  d5.loss_mask: 0.2009  d5.loss_dice: 2.1325  d6.loss_cls: 0.6845  d6.loss_mask: 0.2000  d6.loss_dice: 2.0886  d7.loss_cls: 0.6831  d7.loss_mask: 0.1990  d7.loss_dice: 2.0823  d8.loss_cls: 0.6680  d8.loss_mask: 0.1966  d8.loss_dice: 2.0857
05/08 07:09:42 - mmengine - INFO - Iter(train) [ 9950/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:15:07  time: 1.4372  data_time: 0.0634  memory: 28937  grad_norm: 50.0064  loss: 30.9767  loss_cls: 0.6578  loss_mask: 0.2022  loss_dice: 2.0634  d0.loss_cls: 1.0888  d0.loss_mask: 0.2427  d0.loss_dice: 2.2721  d1.loss_cls: 0.7615  d1.loss_mask: 0.2298  d1.loss_dice: 2.3323  d2.loss_cls: 0.6833  d2.loss_mask: 0.2196  d2.loss_dice: 2.2380  d3.loss_cls: 0.6936  d3.loss_mask: 0.2143  d3.loss_dice: 2.1380  d4.loss_cls: 0.6797  d4.loss_mask: 0.2108  d4.loss_dice: 2.1389  d5.loss_cls: 0.6775  d5.loss_mask: 0.2096  d5.loss_dice: 2.1336  d6.loss_cls: 0.6764  d6.loss_mask: 0.2076  d6.loss_dice: 2.0853  d7.loss_cls: 0.6686  d7.loss_mask: 0.2059  d7.loss_dice: 2.0906  d8.loss_cls: 0.6693  d8.loss_mask: 0.2043  d8.loss_dice: 2.0813
05/08 07:10:53 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 07:10:53 - mmengine - INFO - Iter(train) [10000/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:13:46  time: 1.4248  data_time: 0.0515  memory: 28555  grad_norm: 47.3314  loss: 29.4875  loss_cls: 0.6488  loss_mask: 0.1911  loss_dice: 1.9446  d0.loss_cls: 1.0725  d0.loss_mask: 0.2292  d0.loss_dice: 2.1407  d1.loss_cls: 0.7431  d1.loss_mask: 0.2195  d1.loss_dice: 2.1859  d2.loss_cls: 0.6825  d2.loss_mask: 0.2081  d2.loss_dice: 2.0988  d3.loss_cls: 0.6886  d3.loss_mask: 0.2020  d3.loss_dice: 2.0121  d4.loss_cls: 0.6847  d4.loss_mask: 0.1996  d4.loss_dice: 2.0062  d5.loss_cls: 0.6720  d5.loss_mask: 0.1954  d5.loss_dice: 2.0043  d6.loss_cls: 0.6785  d6.loss_mask: 0.1945  d6.loss_dice: 1.9641  d7.loss_cls: 0.6529  d7.loss_mask: 0.1923  d7.loss_dice: 1.9679  d8.loss_cls: 0.6588  d8.loss_mask: 0.1927  d8.loss_dice: 1.9560
05/08 07:10:53 - mmengine - INFO - Saving checkpoint at 10000 iterations
05/08 07:11:45 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9836  data_time: 0.0273  memory: 3258  
05/08 07:12:09 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.32s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 07:12:16 - mmengine - INFO - start multi processing evaluation ...
DONE (t=53.55s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.386
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.724
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.342
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.195
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.471
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.892
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.502
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.883
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.492
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.353
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.572
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.933
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.502
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.502
05/08 07:13:10 - mmengine - INFO - segm_mAP_copypaste: 0.386 0.724 0.342 0.195 0.471 0.892
05/08 07:13:11 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.3860  coco/segm_mAP_50: 0.7240  coco/segm_mAP_75: 0.3420  coco/segm_mAP_s: 0.1950  coco/segm_mAP_m: 0.4710  coco/segm_mAP_l: 0.8920  data_time: 0.0272  time: 0.9814
05/08 07:14:22 - mmengine - INFO - Iter(train) [10050/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:13:50  time: 3.1354  data_time: 1.7622  memory: 29942  grad_norm: 77.8043  loss: 31.4521  loss_cls: 0.6508  loss_mask: 0.2004  loss_dice: 2.1171  d0.loss_cls: 1.0987  d0.loss_mask: 0.2430  d0.loss_dice: 2.3541  d1.loss_cls: 0.7531  d1.loss_mask: 0.2288  d1.loss_dice: 2.4064  d2.loss_cls: 0.6760  d2.loss_mask: 0.2174  d2.loss_dice: 2.3090  d3.loss_cls: 0.6815  d3.loss_mask: 0.2101  d3.loss_dice: 2.1999  d4.loss_cls: 0.6741  d4.loss_mask: 0.2085  d4.loss_dice: 2.1882  d5.loss_cls: 0.6578  d5.loss_mask: 0.2048  d5.loss_dice: 2.1833  d6.loss_cls: 0.6689  d6.loss_mask: 0.2033  d6.loss_dice: 2.1363  d7.loss_cls: 0.6544  d7.loss_mask: 0.2017  d7.loss_dice: 2.1350  d8.loss_cls: 0.6605  d8.loss_mask: 0.2015  d8.loss_dice: 2.1273
05/08 07:15:35 - mmengine - INFO - Iter(train) [10100/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:12:29  time: 1.4484  data_time: 0.0576  memory: 29577  grad_norm: 41.0062  loss: 31.2331  loss_cls: 0.6521  loss_mask: 0.1965  loss_dice: 2.1083  d0.loss_cls: 1.0978  d0.loss_mask: 0.2378  d0.loss_dice: 2.3064  d1.loss_cls: 0.7633  d1.loss_mask: 0.2257  d1.loss_dice: 2.3742  d2.loss_cls: 0.6838  d2.loss_mask: 0.2133  d2.loss_dice: 2.2853  d3.loss_cls: 0.6855  d3.loss_mask: 0.2068  d3.loss_dice: 2.1825  d4.loss_cls: 0.6755  d4.loss_mask: 0.2036  d4.loss_dice: 2.1698  d5.loss_cls: 0.6776  d5.loss_mask: 0.1996  d5.loss_dice: 2.1628  d6.loss_cls: 0.6637  d6.loss_mask: 0.1981  d6.loss_dice: 2.1232  d7.loss_cls: 0.6507  d7.loss_mask: 0.1976  d7.loss_dice: 2.1253  d8.loss_cls: 0.6418  d8.loss_mask: 0.1970  d8.loss_dice: 2.1272
05/08 07:16:48 - mmengine - INFO - Iter(train) [10150/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:11:10  time: 1.4733  data_time: 0.1025  memory: 29931  grad_norm: 48.4403  loss: 31.0522  loss_cls: 0.6493  loss_mask: 0.2051  loss_dice: 2.0698  d0.loss_cls: 1.1125  d0.loss_mask: 0.2524  d0.loss_dice: 2.2746  d1.loss_cls: 0.7651  d1.loss_mask: 0.2420  d1.loss_dice: 2.3372  d2.loss_cls: 0.6926  d2.loss_mask: 0.2254  d2.loss_dice: 2.2418  d3.loss_cls: 0.6929  d3.loss_mask: 0.2170  d3.loss_dice: 2.1528  d4.loss_cls: 0.6812  d4.loss_mask: 0.2160  d4.loss_dice: 2.1475  d5.loss_cls: 0.6831  d5.loss_mask: 0.2116  d5.loss_dice: 2.1287  d6.loss_cls: 0.6640  d6.loss_mask: 0.2077  d6.loss_dice: 2.0918  d7.loss_cls: 0.6482  d7.loss_mask: 0.2076  d7.loss_dice: 2.0970  d8.loss_cls: 0.6456  d8.loss_mask: 0.2055  d8.loss_dice: 2.0862
05/08 07:18:01 - mmengine - INFO - Iter(train) [10200/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:09:50  time: 1.4495  data_time: 0.0569  memory: 28562  grad_norm: 49.0987  loss: 29.2968  loss_cls: 0.6652  loss_mask: 0.1891  loss_dice: 1.9165  d0.loss_cls: 1.0734  d0.loss_mask: 0.2341  d0.loss_dice: 2.1295  d1.loss_cls: 0.7426  d1.loss_mask: 0.2193  d1.loss_dice: 2.1971  d2.loss_cls: 0.6730  d2.loss_mask: 0.2048  d2.loss_dice: 2.0901  d3.loss_cls: 0.6884  d3.loss_mask: 0.1978  d3.loss_dice: 1.9987  d4.loss_cls: 0.6976  d4.loss_mask: 0.1963  d4.loss_dice: 1.9795  d5.loss_cls: 0.6827  d5.loss_mask: 0.1947  d5.loss_dice: 1.9760  d6.loss_cls: 0.6690  d6.loss_mask: 0.1924  d6.loss_dice: 1.9374  d7.loss_cls: 0.6563  d7.loss_mask: 0.1916  d7.loss_dice: 1.9323  d8.loss_cls: 0.6510  d8.loss_mask: 0.1908  d8.loss_dice: 1.9298
05/08 07:19:13 - mmengine - INFO - Iter(train) [10250/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:08:29  time: 1.4440  data_time: 0.0566  memory: 28603  grad_norm: 57.2142  loss: 28.9199  loss_cls: 0.6276  loss_mask: 0.1896  loss_dice: 1.9074  d0.loss_cls: 1.0856  d0.loss_mask: 0.2356  d0.loss_dice: 2.1137  d1.loss_cls: 0.7283  d1.loss_mask: 0.2202  d1.loss_dice: 2.1807  d2.loss_cls: 0.6486  d2.loss_mask: 0.2062  d2.loss_dice: 2.0834  d3.loss_cls: 0.6543  d3.loss_mask: 0.2006  d3.loss_dice: 1.9876  d4.loss_cls: 0.6476  d4.loss_mask: 0.1982  d4.loss_dice: 1.9737  d5.loss_cls: 0.6389  d5.loss_mask: 0.1955  d5.loss_dice: 1.9642  d6.loss_cls: 0.6482  d6.loss_mask: 0.1928  d6.loss_dice: 1.9204  d7.loss_cls: 0.6244  d7.loss_mask: 0.1929  d7.loss_dice: 1.9204  d8.loss_cls: 0.6249  d8.loss_mask: 0.1923  d8.loss_dice: 1.9161
05/08 07:20:25 - mmengine - INFO - Iter(train) [10300/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:07:08  time: 1.4410  data_time: 0.0574  memory: 29457  grad_norm: 55.2335  loss: 30.8078  loss_cls: 0.6649  loss_mask: 0.2069  loss_dice: 2.0544  d0.loss_cls: 1.0840  d0.loss_mask: 0.2524  d0.loss_dice: 2.2522  d1.loss_cls: 0.7593  d1.loss_mask: 0.2419  d1.loss_dice: 2.3228  d2.loss_cls: 0.6795  d2.loss_mask: 0.2270  d2.loss_dice: 2.2201  d3.loss_cls: 0.6823  d3.loss_mask: 0.2185  d3.loss_dice: 2.1267  d4.loss_cls: 0.6673  d4.loss_mask: 0.2166  d4.loss_dice: 2.1160  d5.loss_cls: 0.6755  d5.loss_mask: 0.2140  d5.loss_dice: 2.0925  d6.loss_cls: 0.6814  d6.loss_mask: 0.2109  d6.loss_dice: 2.0687  d7.loss_cls: 0.6709  d7.loss_mask: 0.2086  d7.loss_dice: 2.0635  d8.loss_cls: 0.6627  d8.loss_mask: 0.2074  d8.loss_dice: 2.0587
05/08 07:21:38 - mmengine - INFO - Iter(train) [10350/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:05:48  time: 1.4511  data_time: 0.0571  memory: 29372  grad_norm: 52.0118  loss: 29.4691  loss_cls: 0.6485  loss_mask: 0.1857  loss_dice: 1.9550  d0.loss_cls: 1.0857  d0.loss_mask: 0.2278  d0.loss_dice: 2.1535  d1.loss_cls: 0.7557  d1.loss_mask: 0.2169  d1.loss_dice: 2.2101  d2.loss_cls: 0.6642  d2.loss_mask: 0.2014  d2.loss_dice: 2.1155  d3.loss_cls: 0.6658  d3.loss_mask: 0.1964  d3.loss_dice: 2.0236  d4.loss_cls: 0.6495  d4.loss_mask: 0.1972  d4.loss_dice: 2.0264  d5.loss_cls: 0.6804  d5.loss_mask: 0.1935  d5.loss_dice: 1.9883  d6.loss_cls: 0.6623  d6.loss_mask: 0.1904  d6.loss_dice: 1.9628  d7.loss_cls: 0.6595  d7.loss_mask: 0.1879  d7.loss_dice: 1.9650  d8.loss_cls: 0.6538  d8.loss_mask: 0.1866  d8.loss_dice: 1.9596
05/08 07:22:50 - mmengine - INFO - Iter(train) [10400/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:04:28  time: 1.4482  data_time: 0.0641  memory: 29639  grad_norm: 63.3001  loss: 32.0402  loss_cls: 0.6873  loss_mask: 0.2151  loss_dice: 2.1441  d0.loss_cls: 1.1138  d0.loss_mask: 0.2531  d0.loss_dice: 2.3754  d1.loss_cls: 0.7742  d1.loss_mask: 0.2449  d1.loss_dice: 2.4356  d2.loss_cls: 0.6835  d2.loss_mask: 0.2310  d2.loss_dice: 2.3278  d3.loss_cls: 0.6892  d3.loss_mask: 0.2254  d3.loss_dice: 2.2307  d4.loss_cls: 0.6669  d4.loss_mask: 0.2207  d4.loss_dice: 2.2372  d5.loss_cls: 0.6943  d5.loss_mask: 0.2173  d5.loss_dice: 2.2078  d6.loss_cls: 0.6709  d6.loss_mask: 0.2150  d6.loss_dice: 2.1676  d7.loss_cls: 0.6834  d7.loss_mask: 0.2140  d7.loss_dice: 2.1575  d8.loss_cls: 0.6876  d8.loss_mask: 0.2139  d8.loss_dice: 2.1550
05/08 07:24:02 - mmengine - INFO - Iter(train) [10450/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:03:08  time: 1.4465  data_time: 0.0612  memory: 28371  grad_norm: 78.8338  loss: 28.9571  loss_cls: 0.6485  loss_mask: 0.1906  loss_dice: 1.9076  d0.loss_cls: 1.0785  d0.loss_mask: 0.2321  d0.loss_dice: 2.1025  d1.loss_cls: 0.7563  d1.loss_mask: 0.2196  d1.loss_dice: 2.1467  d2.loss_cls: 0.6600  d2.loss_mask: 0.2079  d2.loss_dice: 2.0734  d3.loss_cls: 0.6592  d3.loss_mask: 0.2033  d3.loss_dice: 1.9882  d4.loss_cls: 0.6376  d4.loss_mask: 0.1981  d4.loss_dice: 1.9822  d5.loss_cls: 0.6349  d5.loss_mask: 0.1959  d5.loss_dice: 1.9742  d6.loss_cls: 0.6353  d6.loss_mask: 0.1920  d6.loss_dice: 1.9363  d7.loss_cls: 0.6313  d7.loss_mask: 0.1915  d7.loss_dice: 1.9298  d8.loss_cls: 0.6284  d8.loss_mask: 0.1901  d8.loss_dice: 1.9250
05/08 07:25:15 - mmengine - INFO - Iter(train) [10500/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:01:48  time: 1.4544  data_time: 0.0571  memory: 28931  grad_norm: 46.7662  loss: 30.6761  loss_cls: 0.6557  loss_mask: 0.2018  loss_dice: 2.0580  d0.loss_cls: 1.0855  d0.loss_mask: 0.2436  d0.loss_dice: 2.2506  d1.loss_cls: 0.7589  d1.loss_mask: 0.2319  d1.loss_dice: 2.3153  d2.loss_cls: 0.6701  d2.loss_mask: 0.2159  d2.loss_dice: 2.2377  d3.loss_cls: 0.6793  d3.loss_mask: 0.2109  d3.loss_dice: 2.1402  d4.loss_cls: 0.6512  d4.loss_mask: 0.2074  d4.loss_dice: 2.1331  d5.loss_cls: 0.6409  d5.loss_mask: 0.2060  d5.loss_dice: 2.1225  d6.loss_cls: 0.6416  d6.loss_mask: 0.2039  d6.loss_dice: 2.0810  d7.loss_cls: 0.6386  d7.loss_mask: 0.2037  d7.loss_dice: 2.0751  d8.loss_cls: 0.6428  d8.loss_mask: 0.2014  d8.loss_dice: 2.0718
05/08 07:26:29 - mmengine - INFO - Iter(train) [10550/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 4:00:30  time: 1.4699  data_time: 0.1036  memory: 28984  grad_norm: 44.2252  loss: 30.6718  loss_cls: 0.6358  loss_mask: 0.2026  loss_dice: 2.0606  d0.loss_cls: 1.1039  d0.loss_mask: 0.2449  d0.loss_dice: 2.2682  d1.loss_cls: 0.7471  d1.loss_mask: 0.2335  d1.loss_dice: 2.3338  d2.loss_cls: 0.6564  d2.loss_mask: 0.2206  d2.loss_dice: 2.2372  d3.loss_cls: 0.6699  d3.loss_mask: 0.2132  d3.loss_dice: 2.1444  d4.loss_cls: 0.6453  d4.loss_mask: 0.2094  d4.loss_dice: 2.1379  d5.loss_cls: 0.6290  d5.loss_mask: 0.2054  d5.loss_dice: 2.1318  d6.loss_cls: 0.6285  d6.loss_mask: 0.2031  d6.loss_dice: 2.0879  d7.loss_cls: 0.6264  d7.loss_mask: 0.2030  d7.loss_dice: 2.0840  d8.loss_cls: 0.6257  d8.loss_mask: 0.2021  d8.loss_dice: 2.0803
05/08 07:27:42 - mmengine - INFO - Iter(train) [10600/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:59:10  time: 1.4588  data_time: 0.0567  memory: 29212  grad_norm: 61.4333  loss: 29.8573  loss_cls: 0.6309  loss_mask: 0.1926  loss_dice: 2.0003  d0.loss_cls: 1.0863  d0.loss_mask: 0.2365  d0.loss_dice: 2.1939  d1.loss_cls: 0.7367  d1.loss_mask: 0.2242  d1.loss_dice: 2.2670  d2.loss_cls: 0.6590  d2.loss_mask: 0.2131  d2.loss_dice: 2.1723  d3.loss_cls: 0.6600  d3.loss_mask: 0.2037  d3.loss_dice: 2.0808  d4.loss_cls: 0.6286  d4.loss_mask: 0.2028  d4.loss_dice: 2.0761  d5.loss_cls: 0.6162  d5.loss_mask: 0.2004  d5.loss_dice: 2.0643  d6.loss_cls: 0.6236  d6.loss_mask: 0.1978  d6.loss_dice: 2.0216  d7.loss_cls: 0.6230  d7.loss_mask: 0.1959  d7.loss_dice: 2.0159  d8.loss_cls: 0.6221  d8.loss_mask: 0.1943  d8.loss_dice: 2.0173
05/08 07:28:54 - mmengine - INFO - Iter(train) [10650/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:57:50  time: 1.4427  data_time: 0.0560  memory: 28911  grad_norm: 54.1112  loss: 28.8228  loss_cls: 0.6271  loss_mask: 0.1818  loss_dice: 1.9120  d0.loss_cls: 1.0730  d0.loss_mask: 0.2247  d0.loss_dice: 2.1157  d1.loss_cls: 0.7294  d1.loss_mask: 0.2130  d1.loss_dice: 2.1702  d2.loss_cls: 0.6544  d2.loss_mask: 0.1997  d2.loss_dice: 2.0860  d3.loss_cls: 0.6558  d3.loss_mask: 0.1944  d3.loss_dice: 1.9948  d4.loss_cls: 0.6236  d4.loss_mask: 0.1907  d4.loss_dice: 1.9924  d5.loss_cls: 0.6104  d5.loss_mask: 0.1862  d5.loss_dice: 1.9878  d6.loss_cls: 0.6128  d6.loss_mask: 0.1847  d6.loss_dice: 1.9415  d7.loss_cls: 0.6140  d7.loss_mask: 0.1833  d7.loss_dice: 1.9358  d8.loss_cls: 0.6168  d8.loss_mask: 0.1823  d8.loss_dice: 1.9283
05/08 07:30:06 - mmengine - INFO - Iter(train) [10700/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:56:30  time: 1.4478  data_time: 0.0673  memory: 30520  grad_norm: 59.4733  loss: 31.5301  loss_cls: 0.6500  loss_mask: 0.2017  loss_dice: 2.1310  d0.loss_cls: 1.1031  d0.loss_mask: 0.2474  d0.loss_dice: 2.3646  d1.loss_cls: 0.7522  d1.loss_mask: 0.2372  d1.loss_dice: 2.4167  d2.loss_cls: 0.6734  d2.loss_mask: 0.2182  d2.loss_dice: 2.3294  d3.loss_cls: 0.6745  d3.loss_mask: 0.2135  d3.loss_dice: 2.2254  d4.loss_cls: 0.6465  d4.loss_mask: 0.2094  d4.loss_dice: 2.2162  d5.loss_cls: 0.6272  d5.loss_mask: 0.2068  d5.loss_dice: 2.2069  d6.loss_cls: 0.6364  d6.loss_mask: 0.2052  d6.loss_dice: 2.1522  d7.loss_cls: 0.6358  d7.loss_mask: 0.2045  d7.loss_dice: 2.1540  d8.loss_cls: 0.6412  d8.loss_mask: 0.2034  d8.loss_dice: 2.1462
05/08 07:31:19 - mmengine - INFO - Iter(train) [10750/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:55:11  time: 1.4562  data_time: 0.0635  memory: 28938  grad_norm: 74.2739  loss: 30.7928  loss_cls: 0.6676  loss_mask: 0.1939  loss_dice: 2.0750  d0.loss_cls: 1.0915  d0.loss_mask: 0.2409  d0.loss_dice: 2.2693  d1.loss_cls: 0.7596  d1.loss_mask: 0.2244  d1.loss_dice: 2.3367  d2.loss_cls: 0.6725  d2.loss_mask: 0.2106  d2.loss_dice: 2.2518  d3.loss_cls: 0.6794  d3.loss_mask: 0.2036  d3.loss_dice: 2.1522  d4.loss_cls: 0.6500  d4.loss_mask: 0.2010  d4.loss_dice: 2.1448  d5.loss_cls: 0.6355  d5.loss_mask: 0.1981  d5.loss_dice: 2.1404  d6.loss_cls: 0.6511  d6.loss_mask: 0.1956  d6.loss_dice: 2.0870  d7.loss_cls: 0.6433  d7.loss_mask: 0.1952  d7.loss_dice: 2.0871  d8.loss_cls: 0.6502  d8.loss_mask: 0.1953  d8.loss_dice: 2.0891
05/08 07:32:31 - mmengine - INFO - Iter(train) [10800/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:53:51  time: 1.4475  data_time: 0.0583  memory: 29038  grad_norm: 55.2334  loss: 29.8581  loss_cls: 0.6897  loss_mask: 0.1955  loss_dice: 1.9770  d0.loss_cls: 1.0747  d0.loss_mask: 0.2408  d0.loss_dice: 2.1962  d1.loss_cls: 0.7430  d1.loss_mask: 0.2231  d1.loss_dice: 2.2629  d2.loss_cls: 0.6708  d2.loss_mask: 0.2109  d2.loss_dice: 2.1500  d3.loss_cls: 0.6701  d3.loss_mask: 0.2062  d3.loss_dice: 2.0553  d4.loss_cls: 0.6442  d4.loss_mask: 0.2019  d4.loss_dice: 2.0449  d5.loss_cls: 0.6313  d5.loss_mask: 0.1979  d5.loss_dice: 2.0319  d6.loss_cls: 0.6601  d6.loss_mask: 0.1974  d6.loss_dice: 1.9903  d7.loss_cls: 0.6528  d7.loss_mask: 0.1951  d7.loss_dice: 1.9892  d8.loss_cls: 0.6769  d8.loss_mask: 0.1953  d8.loss_dice: 1.9826
05/08 07:33:45 - mmengine - INFO - Iter(train) [10850/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:52:33  time: 1.4669  data_time: 0.0638  memory: 29502  grad_norm: 61.6040  loss: 30.8865  loss_cls: 0.7034  loss_mask: 0.2057  loss_dice: 2.0503  d0.loss_cls: 1.1145  d0.loss_mask: 0.2502  d0.loss_dice: 2.2608  d1.loss_cls: 0.7734  d1.loss_mask: 0.2392  d1.loss_dice: 2.3183  d2.loss_cls: 0.6919  d2.loss_mask: 0.2242  d2.loss_dice: 2.2189  d3.loss_cls: 0.6934  d3.loss_mask: 0.2160  d3.loss_dice: 2.1242  d4.loss_cls: 0.6699  d4.loss_mask: 0.2139  d4.loss_dice: 2.1113  d5.loss_cls: 0.6539  d5.loss_mask: 0.2087  d5.loss_dice: 2.1052  d6.loss_cls: 0.6732  d6.loss_mask: 0.2076  d6.loss_dice: 2.0621  d7.loss_cls: 0.6776  d7.loss_mask: 0.2071  d7.loss_dice: 2.0529  d8.loss_cls: 0.6969  d8.loss_mask: 0.2057  d8.loss_dice: 2.0560
05/08 07:34:57 - mmengine - INFO - Iter(train) [10900/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:51:13  time: 1.4420  data_time: 0.0566  memory: 29469  grad_norm: 57.2392  loss: 28.9423  loss_cls: 0.6412  loss_mask: 0.1980  loss_dice: 1.9099  d0.loss_cls: 1.0853  d0.loss_mask: 0.2419  d0.loss_dice: 2.1029  d1.loss_cls: 0.7365  d1.loss_mask: 0.2254  d1.loss_dice: 2.1542  d2.loss_cls: 0.6590  d2.loss_mask: 0.2142  d2.loss_dice: 2.0684  d3.loss_cls: 0.6728  d3.loss_mask: 0.2095  d3.loss_dice: 1.9741  d4.loss_cls: 0.6398  d4.loss_mask: 0.2046  d4.loss_dice: 1.9649  d5.loss_cls: 0.6292  d5.loss_mask: 0.1995  d5.loss_dice: 1.9544  d6.loss_cls: 0.6408  d6.loss_mask: 0.1988  d6.loss_dice: 1.9164  d7.loss_cls: 0.6436  d7.loss_mask: 0.1986  d7.loss_dice: 1.9117  d8.loss_cls: 0.6301  d8.loss_mask: 0.1956  d8.loss_dice: 1.9209
05/08 07:36:10 - mmengine - INFO - Iter(train) [10950/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:49:54  time: 1.4647  data_time: 0.1047  memory: 28442  grad_norm: 72.1649  loss: 30.6967  loss_cls: 0.6446  loss_mask: 0.2081  loss_dice: 2.0532  d0.loss_cls: 1.1041  d0.loss_mask: 0.2582  d0.loss_dice: 2.2812  d1.loss_cls: 0.7570  d1.loss_mask: 0.2411  d1.loss_dice: 2.3198  d2.loss_cls: 0.6775  d2.loss_mask: 0.2256  d2.loss_dice: 2.2202  d3.loss_cls: 0.6740  d3.loss_mask: 0.2198  d3.loss_dice: 2.1258  d4.loss_cls: 0.6451  d4.loss_mask: 0.2158  d4.loss_dice: 2.1152  d5.loss_cls: 0.6379  d5.loss_mask: 0.2107  d5.loss_dice: 2.1009  d6.loss_cls: 0.6585  d6.loss_mask: 0.2101  d6.loss_dice: 2.0659  d7.loss_cls: 0.6299  d7.loss_mask: 0.2098  d7.loss_dice: 2.0725  d8.loss_cls: 0.6394  d8.loss_mask: 0.2077  d8.loss_dice: 2.0671
05/08 07:37:22 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 07:37:22 - mmengine - INFO - Iter(train) [11000/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:48:34  time: 1.4337  data_time: 0.0630  memory: 29095  grad_norm: 62.7173  loss: 30.0757  loss_cls: 0.6506  loss_mask: 0.1972  loss_dice: 2.0058  d0.loss_cls: 1.0936  d0.loss_mask: 0.2413  d0.loss_dice: 2.2062  d1.loss_cls: 0.7493  d1.loss_mask: 0.2275  d1.loss_dice: 2.2615  d2.loss_cls: 0.6707  d2.loss_mask: 0.2178  d2.loss_dice: 2.1690  d3.loss_cls: 0.6708  d3.loss_mask: 0.2120  d3.loss_dice: 2.0821  d4.loss_cls: 0.6558  d4.loss_mask: 0.2071  d4.loss_dice: 2.0629  d5.loss_cls: 0.6504  d5.loss_mask: 0.2019  d5.loss_dice: 2.0492  d6.loss_cls: 0.6523  d6.loss_mask: 0.2020  d6.loss_dice: 2.0187  d7.loss_cls: 0.6507  d7.loss_mask: 0.1996  d7.loss_dice: 2.0109  d8.loss_cls: 0.6479  d8.loss_mask: 0.1977  d8.loss_dice: 2.0132
05/08 07:37:22 - mmengine - INFO - Saving checkpoint at 11000 iterations
05/08 07:38:14 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9873  data_time: 0.0278  memory: 3258  
05/08 07:38:37 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.36s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 07:38:45 - mmengine - INFO - start multi processing evaluation ...
DONE (t=52.06s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.426
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.765
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.389
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.519
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.892
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.543
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.922
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.531
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.406
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.608
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.933
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.543
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.543
05/08 07:39:37 - mmengine - INFO - segm_mAP_copypaste: 0.426 0.765 0.389 0.223 0.519 0.892
05/08 07:39:38 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.4260  coco/segm_mAP_50: 0.7650  coco/segm_mAP_75: 0.3890  coco/segm_mAP_s: 0.2230  coco/segm_mAP_m: 0.5190  coco/segm_mAP_l: 0.8920  data_time: 0.0278  time: 0.9851
05/08 07:40:51 - mmengine - INFO - Iter(train) [11050/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:48:24  time: 3.1398  data_time: 1.7471  memory: 28865  grad_norm: 61.4394  loss: 30.7021  loss_cls: 0.6482  loss_mask: 0.2051  loss_dice: 2.0562  d0.loss_cls: 1.1097  d0.loss_mask: 0.2463  d0.loss_dice: 2.2659  d1.loss_cls: 0.7536  d1.loss_mask: 0.2347  d1.loss_dice: 2.3256  d2.loss_cls: 0.6709  d2.loss_mask: 0.2181  d2.loss_dice: 2.2376  d3.loss_cls: 0.6630  d3.loss_mask: 0.2134  d3.loss_dice: 2.1332  d4.loss_cls: 0.6528  d4.loss_mask: 0.2112  d4.loss_dice: 2.1156  d5.loss_cls: 0.6420  d5.loss_mask: 0.2085  d5.loss_dice: 2.1033  d6.loss_cls: 0.6617  d6.loss_mask: 0.2089  d6.loss_dice: 2.0700  d7.loss_cls: 0.6722  d7.loss_mask: 0.2050  d7.loss_dice: 2.0545  d8.loss_cls: 0.6471  d8.loss_mask: 0.2047  d8.loss_dice: 2.0629
05/08 07:42:03 - mmengine - INFO - Iter(train) [11100/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:47:04  time: 1.4472  data_time: 0.0636  memory: 30236  grad_norm: 57.2844  loss: 33.4846  loss_cls: 0.6823  loss_mask: 0.2183  loss_dice: 2.2731  d0.loss_cls: 1.1415  d0.loss_mask: 0.2618  d0.loss_dice: 2.4951  d1.loss_cls: 0.7784  d1.loss_mask: 0.2549  d1.loss_dice: 2.5568  d2.loss_cls: 0.7116  d2.loss_mask: 0.2355  d2.loss_dice: 2.4630  d3.loss_cls: 0.7108  d3.loss_mask: 0.2310  d3.loss_dice: 2.3484  d4.loss_cls: 0.6988  d4.loss_mask: 0.2289  d4.loss_dice: 2.3395  d5.loss_cls: 0.6915  d5.loss_mask: 0.2248  d5.loss_dice: 2.3276  d6.loss_cls: 0.7063  d6.loss_mask: 0.2256  d6.loss_dice: 2.2864  d7.loss_cls: 0.7148  d7.loss_mask: 0.2210  d7.loss_dice: 2.2725  d8.loss_cls: 0.6846  d8.loss_mask: 0.2202  d8.loss_dice: 2.2794
05/08 07:43:17 - mmengine - INFO - Iter(train) [11150/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:45:45  time: 1.4724  data_time: 0.0611  memory: 30245  grad_norm: 64.4900  loss: 31.4250  loss_cls: 0.6638  loss_mask: 0.2094  loss_dice: 2.1100  d0.loss_cls: 1.1000  d0.loss_mask: 0.2490  d0.loss_dice: 2.3058  d1.loss_cls: 0.7544  d1.loss_mask: 0.2384  d1.loss_dice: 2.3720  d2.loss_cls: 0.6883  d2.loss_mask: 0.2260  d2.loss_dice: 2.2750  d3.loss_cls: 0.6874  d3.loss_mask: 0.2203  d3.loss_dice: 2.1714  d4.loss_cls: 0.6786  d4.loss_mask: 0.2172  d4.loss_dice: 2.1659  d5.loss_cls: 0.6819  d5.loss_mask: 0.2126  d5.loss_dice: 2.1519  d6.loss_cls: 0.6922  d6.loss_mask: 0.2142  d6.loss_dice: 2.1196  d7.loss_cls: 0.6995  d7.loss_mask: 0.2121  d7.loss_dice: 2.1098  d8.loss_cls: 0.6805  d8.loss_mask: 0.2091  d8.loss_dice: 2.1084
05/08 07:44:30 - mmengine - INFO - Iter(train) [11200/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:44:26  time: 1.4654  data_time: 0.0566  memory: 28640  grad_norm: 58.9632  loss: 29.5461  loss_cls: 0.6364  loss_mask: 0.1967  loss_dice: 1.9539  d0.loss_cls: 1.0804  d0.loss_mask: 0.2344  d0.loss_dice: 2.1409  d1.loss_cls: 0.7328  d1.loss_mask: 0.2293  d1.loss_dice: 2.1976  d2.loss_cls: 0.6752  d2.loss_mask: 0.2145  d2.loss_dice: 2.1003  d3.loss_cls: 0.6958  d3.loss_mask: 0.2065  d3.loss_dice: 2.0091  d4.loss_cls: 0.6691  d4.loss_mask: 0.2079  d4.loss_dice: 2.0098  d5.loss_cls: 0.6804  d5.loss_mask: 0.2022  d5.loss_dice: 1.9934  d6.loss_cls: 0.6834  d6.loss_mask: 0.1999  d6.loss_dice: 1.9557  d7.loss_cls: 0.6699  d7.loss_mask: 0.2026  d7.loss_dice: 1.9585  d8.loss_cls: 0.6582  d8.loss_mask: 0.1980  d8.loss_dice: 1.9533
05/08 07:45:41 - mmengine - INFO - Iter(train) [11250/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:43:05  time: 1.4220  data_time: 0.0588  memory: 28890  grad_norm: 55.4455  loss: 29.5994  loss_cls: 0.6571  loss_mask: 0.1800  loss_dice: 1.9670  d0.loss_cls: 1.0741  d0.loss_mask: 0.2183  d0.loss_dice: 2.1578  d1.loss_cls: 0.7489  d1.loss_mask: 0.2062  d1.loss_dice: 2.2121  d2.loss_cls: 0.6781  d2.loss_mask: 0.1929  d2.loss_dice: 2.1258  d3.loss_cls: 0.6885  d3.loss_mask: 0.1874  d3.loss_dice: 2.0332  d4.loss_cls: 0.6669  d4.loss_mask: 0.1879  d4.loss_dice: 2.0326  d5.loss_cls: 0.6715  d5.loss_mask: 0.1855  d5.loss_dice: 2.0202  d6.loss_cls: 0.6902  d6.loss_mask: 0.1831  d6.loss_dice: 1.9786  d7.loss_cls: 0.6815  d7.loss_mask: 0.1836  d7.loss_dice: 1.9708  d8.loss_cls: 0.6578  d8.loss_mask: 0.1817  d8.loss_dice: 1.9802
05/08 07:46:54 - mmengine - INFO - Iter(train) [11300/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:41:46  time: 1.4527  data_time: 0.0579  memory: 28456  grad_norm: 57.1752  loss: 29.9780  loss_cls: 0.6728  loss_mask: 0.2065  loss_dice: 1.9576  d0.loss_cls: 1.0888  d0.loss_mask: 0.2494  d0.loss_dice: 2.1765  d1.loss_cls: 0.7603  d1.loss_mask: 0.2322  d1.loss_dice: 2.2171  d2.loss_cls: 0.6983  d2.loss_mask: 0.2238  d2.loss_dice: 2.1192  d3.loss_cls: 0.6963  d3.loss_mask: 0.2175  d3.loss_dice: 2.0343  d4.loss_cls: 0.6817  d4.loss_mask: 0.2148  d4.loss_dice: 2.0279  d5.loss_cls: 0.6677  d5.loss_mask: 0.2132  d5.loss_dice: 2.0244  d6.loss_cls: 0.7055  d6.loss_mask: 0.2098  d6.loss_dice: 1.9700  d7.loss_cls: 0.6576  d7.loss_mask: 0.2114  d7.loss_dice: 1.9899  d8.loss_cls: 0.6579  d8.loss_mask: 0.2081  d8.loss_dice: 1.9876
05/08 07:48:06 - mmengine - INFO - Iter(train) [11350/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:40:26  time: 1.4374  data_time: 0.1073  memory: 28860  grad_norm: 57.2469  loss: 30.5553  loss_cls: 0.6402  loss_mask: 0.2001  loss_dice: 2.0361  d0.loss_cls: 1.0895  d0.loss_mask: 0.2466  d0.loss_dice: 2.2613  d1.loss_cls: 0.7703  d1.loss_mask: 0.2346  d1.loss_dice: 2.2891  d2.loss_cls: 0.6934  d2.loss_mask: 0.2205  d2.loss_dice: 2.1968  d3.loss_cls: 0.6928  d3.loss_mask: 0.2112  d3.loss_dice: 2.1060  d4.loss_cls: 0.6690  d4.loss_mask: 0.2094  d4.loss_dice: 2.1016  d5.loss_cls: 0.6507  d5.loss_mask: 0.2090  d5.loss_dice: 2.1000  d6.loss_cls: 0.6787  d6.loss_mask: 0.2051  d6.loss_dice: 2.0462  d7.loss_cls: 0.6284  d7.loss_mask: 0.2047  d7.loss_dice: 2.0647  d8.loss_cls: 0.6446  d8.loss_mask: 0.2022  d8.loss_dice: 2.0525
05/08 07:49:19 - mmengine - INFO - Iter(train) [11400/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:39:07  time: 1.4598  data_time: 0.0640  memory: 29668  grad_norm: 57.6113  loss: 31.3985  loss_cls: 0.6693  loss_mask: 0.1995  loss_dice: 2.0938  d0.loss_cls: 1.1217  d0.loss_mask: 0.2454  d0.loss_dice: 2.3354  d1.loss_cls: 0.7727  d1.loss_mask: 0.2317  d1.loss_dice: 2.3745  d2.loss_cls: 0.6992  d2.loss_mask: 0.2186  d2.loss_dice: 2.2787  d3.loss_cls: 0.6898  d3.loss_mask: 0.2099  d3.loss_dice: 2.1869  d4.loss_cls: 0.6721  d4.loss_mask: 0.2065  d4.loss_dice: 2.1793  d5.loss_cls: 0.6570  d5.loss_mask: 0.2043  d5.loss_dice: 2.1715  d6.loss_cls: 0.6931  d6.loss_mask: 0.2035  d6.loss_dice: 2.1190  d7.loss_cls: 0.6505  d7.loss_mask: 0.2035  d7.loss_dice: 2.1308  d8.loss_cls: 0.6683  d8.loss_mask: 0.2007  d8.loss_dice: 2.1113
05/08 07:50:32 - mmengine - INFO - Iter(train) [11450/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:37:48  time: 1.4687  data_time: 0.0670  memory: 29456  grad_norm: 72.8003  loss: 31.4002  loss_cls: 0.6673  loss_mask: 0.1921  loss_dice: 2.1104  d0.loss_cls: 1.1102  d0.loss_mask: 0.2399  d0.loss_dice: 2.3304  d1.loss_cls: 0.7665  d1.loss_mask: 0.2215  d1.loss_dice: 2.3988  d2.loss_cls: 0.6907  d2.loss_mask: 0.2127  d2.loss_dice: 2.3021  d3.loss_cls: 0.6921  d3.loss_mask: 0.2040  d3.loss_dice: 2.1953  d4.loss_cls: 0.6674  d4.loss_mask: 0.1993  d4.loss_dice: 2.1948  d5.loss_cls: 0.6518  d5.loss_mask: 0.1970  d5.loss_dice: 2.1856  d6.loss_cls: 0.6807  d6.loss_mask: 0.1968  d6.loss_dice: 2.1276  d7.loss_cls: 0.6522  d7.loss_mask: 0.1952  d7.loss_dice: 2.1284  d8.loss_cls: 0.6742  d8.loss_mask: 0.1944  d8.loss_dice: 2.1209
05/08 07:51:44 - mmengine - INFO - Iter(train) [11500/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:36:28  time: 1.4345  data_time: 0.0573  memory: 29760  grad_norm: 74.7321  loss: 30.6315  loss_cls: 0.6642  loss_mask: 0.1954  loss_dice: 2.0344  d0.loss_cls: 1.0948  d0.loss_mask: 0.2379  d0.loss_dice: 2.2334  d1.loss_cls: 0.7844  d1.loss_mask: 0.2304  d1.loss_dice: 2.2771  d2.loss_cls: 0.7109  d2.loss_mask: 0.2185  d2.loss_dice: 2.1936  d3.loss_cls: 0.7195  d3.loss_mask: 0.2099  d3.loss_dice: 2.1055  d4.loss_cls: 0.6940  d4.loss_mask: 0.2059  d4.loss_dice: 2.1023  d5.loss_cls: 0.6715  d5.loss_mask: 0.2000  d5.loss_dice: 2.0952  d6.loss_cls: 0.6872  d6.loss_mask: 0.2001  d6.loss_dice: 2.0436  d7.loss_cls: 0.6590  d7.loss_mask: 0.1979  d7.loss_dice: 2.0470  d8.loss_cls: 0.6700  d8.loss_mask: 0.1961  d8.loss_dice: 2.0520
05/08 07:52:58 - mmengine - INFO - Iter(train) [11550/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:35:10  time: 1.4779  data_time: 0.0624  memory: 29736  grad_norm: 67.0817  loss: 29.5916  loss_cls: 0.6385  loss_mask: 0.1891  loss_dice: 1.9584  d0.loss_cls: 1.1005  d0.loss_mask: 0.2329  d0.loss_dice: 2.1799  d1.loss_cls: 0.7554  d1.loss_mask: 0.2206  d1.loss_dice: 2.2270  d2.loss_cls: 0.6837  d2.loss_mask: 0.2078  d2.loss_dice: 2.1289  d3.loss_cls: 0.6795  d3.loss_mask: 0.2005  d3.loss_dice: 2.0322  d4.loss_cls: 0.6542  d4.loss_mask: 0.1989  d4.loss_dice: 2.0309  d5.loss_cls: 0.6511  d5.loss_mask: 0.1932  d5.loss_dice: 2.0130  d6.loss_cls: 0.6589  d6.loss_mask: 0.1919  d6.loss_dice: 1.9693  d7.loss_cls: 0.6348  d7.loss_mask: 0.1907  d7.loss_dice: 1.9696  d8.loss_cls: 0.6279  d8.loss_mask: 0.1907  d8.loss_dice: 1.9814
05/08 07:54:09 - mmengine - INFO - Iter(train) [11600/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:33:50  time: 1.4299  data_time: 0.0566  memory: 28708  grad_norm: 56.0671  loss: 28.8326  loss_cls: 0.6327  loss_mask: 0.1891  loss_dice: 1.8912  d0.loss_cls: 1.0830  d0.loss_mask: 0.2393  d0.loss_dice: 2.0881  d1.loss_cls: 0.7527  d1.loss_mask: 0.2198  d1.loss_dice: 2.1405  d2.loss_cls: 0.6805  d2.loss_mask: 0.2066  d2.loss_dice: 2.0461  d3.loss_cls: 0.6842  d3.loss_mask: 0.1998  d3.loss_dice: 1.9552  d4.loss_cls: 0.6689  d4.loss_mask: 0.1981  d4.loss_dice: 1.9474  d5.loss_cls: 0.6557  d5.loss_mask: 0.1946  d5.loss_dice: 1.9383  d6.loss_cls: 0.6707  d6.loss_mask: 0.1934  d6.loss_dice: 1.8927  d7.loss_cls: 0.6462  d7.loss_mask: 0.1919  d7.loss_dice: 1.8958  d8.loss_cls: 0.6295  d8.loss_mask: 0.1910  d8.loss_dice: 1.9095
05/08 07:55:21 - mmengine - INFO - Iter(train) [11650/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:32:30  time: 1.4438  data_time: 0.0639  memory: 29170  grad_norm: 52.7796  loss: 31.1165  loss_cls: 0.6656  loss_mask: 0.2019  loss_dice: 2.0688  d0.loss_cls: 1.1014  d0.loss_mask: 0.2461  d0.loss_dice: 2.2946  d1.loss_cls: 0.7736  d1.loss_mask: 0.2313  d1.loss_dice: 2.3504  d2.loss_cls: 0.6960  d2.loss_mask: 0.2175  d2.loss_dice: 2.2513  d3.loss_cls: 0.6975  d3.loss_mask: 0.2122  d3.loss_dice: 2.1505  d4.loss_cls: 0.6869  d4.loss_mask: 0.2119  d4.loss_dice: 2.1373  d5.loss_cls: 0.6849  d5.loss_mask: 0.2088  d5.loss_dice: 2.1298  d6.loss_cls: 0.6975  d6.loss_mask: 0.2052  d6.loss_dice: 2.0818  d7.loss_cls: 0.6724  d7.loss_mask: 0.2056  d7.loss_dice: 2.0855  d8.loss_cls: 0.6566  d8.loss_mask: 0.2036  d8.loss_dice: 2.0900
05/08 07:56:34 - mmengine - INFO - Iter(train) [11700/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:31:11  time: 1.4469  data_time: 0.0590  memory: 29323  grad_norm: 52.4233  loss: 31.1062  loss_cls: 0.6775  loss_mask: 0.1986  loss_dice: 2.0641  d0.loss_cls: 1.1114  d0.loss_mask: 0.2444  d0.loss_dice: 2.3084  d1.loss_cls: 0.7630  d1.loss_mask: 0.2338  d1.loss_dice: 2.3527  d2.loss_cls: 0.7049  d2.loss_mask: 0.2180  d2.loss_dice: 2.2412  d3.loss_cls: 0.7095  d3.loss_mask: 0.2124  d3.loss_dice: 2.1400  d4.loss_cls: 0.6892  d4.loss_mask: 0.2091  d4.loss_dice: 2.1403  d5.loss_cls: 0.6830  d5.loss_mask: 0.2050  d5.loss_dice: 2.1351  d6.loss_cls: 0.6979  d6.loss_mask: 0.2019  d6.loss_dice: 2.0742  d7.loss_cls: 0.6643  d7.loss_mask: 0.2017  d7.loss_dice: 2.0840  d8.loss_cls: 0.6639  d8.loss_mask: 0.2005  d8.loss_dice: 2.0763
05/08 07:57:47 - mmengine - INFO - Iter(train) [11750/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:29:53  time: 1.4720  data_time: 0.0988  memory: 29481  grad_norm: 57.4709  loss: 29.9189  loss_cls: 0.6746  loss_mask: 0.1845  loss_dice: 1.9792  d0.loss_cls: 1.0812  d0.loss_mask: 0.2243  d0.loss_dice: 2.1980  d1.loss_cls: 0.7733  d1.loss_mask: 0.2177  d1.loss_dice: 2.2376  d2.loss_cls: 0.6986  d2.loss_mask: 0.2018  d2.loss_dice: 2.1398  d3.loss_cls: 0.7035  d3.loss_mask: 0.1941  d3.loss_dice: 2.0457  d4.loss_cls: 0.6825  d4.loss_mask: 0.1933  d4.loss_dice: 2.0435  d5.loss_cls: 0.6778  d5.loss_mask: 0.1899  d5.loss_dice: 2.0376  d6.loss_cls: 0.6988  d6.loss_mask: 0.1882  d6.loss_dice: 1.9895  d7.loss_cls: 0.6632  d7.loss_mask: 0.1868  d7.loss_dice: 1.9898  d8.loss_cls: 0.6633  d8.loss_mask: 0.1858  d8.loss_dice: 1.9750
05/08 07:58:59 - mmengine - INFO - Iter(train) [11800/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:28:33  time: 1.4253  data_time: 0.0559  memory: 29734  grad_norm: 55.5939  loss: 28.2750  loss_cls: 0.6408  loss_mask: 0.1858  loss_dice: 1.8427  d0.loss_cls: 1.0712  d0.loss_mask: 0.2325  d0.loss_dice: 2.0768  d1.loss_cls: 0.7274  d1.loss_mask: 0.2164  d1.loss_dice: 2.1155  d2.loss_cls: 0.6500  d2.loss_mask: 0.2013  d2.loss_dice: 2.0157  d3.loss_cls: 0.6534  d3.loss_mask: 0.1955  d3.loss_dice: 1.9157  d4.loss_cls: 0.6395  d4.loss_mask: 0.1955  d4.loss_dice: 1.9052  d5.loss_cls: 0.6356  d5.loss_mask: 0.1918  d5.loss_dice: 1.9002  d6.loss_cls: 0.6481  d6.loss_mask: 0.1898  d6.loss_dice: 1.8594  d7.loss_cls: 0.6466  d7.loss_mask: 0.1884  d7.loss_dice: 1.8564  d8.loss_cls: 0.6347  d8.loss_mask: 0.1875  d8.loss_dice: 1.8561
05/08 08:00:11 - mmengine - INFO - Iter(train) [11850/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:27:14  time: 1.4440  data_time: 0.0533  memory: 29167  grad_norm: 90.8651  loss: 30.4848  loss_cls: 0.7003  loss_mask: 0.1947  loss_dice: 2.0193  d0.loss_cls: 1.0839  d0.loss_mask: 0.2387  d0.loss_dice: 2.2209  d1.loss_cls: 0.7636  d1.loss_mask: 0.2228  d1.loss_dice: 2.2873  d2.loss_cls: 0.6872  d2.loss_mask: 0.2103  d2.loss_dice: 2.1960  d3.loss_cls: 0.6926  d3.loss_mask: 0.2049  d3.loss_dice: 2.0979  d4.loss_cls: 0.6833  d4.loss_mask: 0.2029  d4.loss_dice: 2.0824  d5.loss_cls: 0.6889  d5.loss_mask: 0.1987  d5.loss_dice: 2.0753  d6.loss_cls: 0.6827  d6.loss_mask: 0.1998  d6.loss_dice: 2.0344  d7.loss_cls: 0.6854  d7.loss_mask: 0.1965  d7.loss_dice: 2.0303  d8.loss_cls: 0.6720  d8.loss_mask: 0.1960  d8.loss_dice: 2.0358
05/08 08:01:23 - mmengine - INFO - Iter(train) [11900/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:25:55  time: 1.4475  data_time: 0.0614  memory: 29464  grad_norm: 70.0846  loss: 31.5665  loss_cls: 0.7058  loss_mask: 0.2080  loss_dice: 2.1016  d0.loss_cls: 1.1019  d0.loss_mask: 0.2493  d0.loss_dice: 2.3165  d1.loss_cls: 0.7758  d1.loss_mask: 0.2308  d1.loss_dice: 2.3756  d2.loss_cls: 0.6963  d2.loss_mask: 0.2197  d2.loss_dice: 2.2839  d3.loss_cls: 0.6949  d3.loss_mask: 0.2140  d3.loss_dice: 2.1798  d4.loss_cls: 0.6939  d4.loss_mask: 0.2123  d4.loss_dice: 2.1619  d5.loss_cls: 0.6969  d5.loss_mask: 0.2077  d5.loss_dice: 2.1507  d6.loss_cls: 0.7108  d6.loss_mask: 0.2130  d6.loss_dice: 2.1121  d7.loss_cls: 0.7211  d7.loss_mask: 0.2086  d7.loss_dice: 2.1027  d8.loss_cls: 0.7037  d8.loss_mask: 0.2074  d8.loss_dice: 2.1098
05/08 08:02:36 - mmengine - INFO - Iter(train) [11950/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:24:36  time: 1.4476  data_time: 0.0546  memory: 29674  grad_norm: 53.1137  loss: 30.0300  loss_cls: 0.6481  loss_mask: 0.1974  loss_dice: 1.9989  d0.loss_cls: 1.0993  d0.loss_mask: 0.2357  d0.loss_dice: 2.2165  d1.loss_cls: 0.7657  d1.loss_mask: 0.2262  d1.loss_dice: 2.2570  d2.loss_cls: 0.6745  d2.loss_mask: 0.2133  d2.loss_dice: 2.1537  d3.loss_cls: 0.6733  d3.loss_mask: 0.2046  d3.loss_dice: 2.0573  d4.loss_cls: 0.6649  d4.loss_mask: 0.2047  d4.loss_dice: 2.0447  d5.loss_cls: 0.6736  d5.loss_mask: 0.1985  d5.loss_dice: 2.0325  d6.loss_cls: 0.6615  d6.loss_mask: 0.1996  d6.loss_dice: 2.0028  d7.loss_cls: 0.6624  d7.loss_mask: 0.1975  d7.loss_dice: 1.9930  d8.loss_cls: 0.6839  d8.loss_mask: 0.1958  d8.loss_dice: 1.9932
05/08 08:03:48 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 08:03:48 - mmengine - INFO - Iter(train) [12000/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:23:18  time: 1.4534  data_time: 0.0611  memory: 28345  grad_norm: 50.5161  loss: 29.1377  loss_cls: 0.6456  loss_mask: 0.1978  loss_dice: 1.9101  d0.loss_cls: 1.0780  d0.loss_mask: 0.2391  d0.loss_dice: 2.1233  d1.loss_cls: 0.7592  d1.loss_mask: 0.2283  d1.loss_dice: 2.1584  d2.loss_cls: 0.6847  d2.loss_mask: 0.2148  d2.loss_dice: 2.0602  d3.loss_cls: 0.6692  d3.loss_mask: 0.2070  d3.loss_dice: 1.9741  d4.loss_cls: 0.6746  d4.loss_mask: 0.2064  d4.loss_dice: 1.9519  d5.loss_cls: 0.6811  d5.loss_mask: 0.2023  d5.loss_dice: 1.9411  d6.loss_cls: 0.6779  d6.loss_mask: 0.2013  d6.loss_dice: 1.8988  d7.loss_cls: 0.6806  d7.loss_mask: 0.1973  d7.loss_dice: 1.8960  d8.loss_cls: 0.6798  d8.loss_mask: 0.1968  d8.loss_dice: 1.9022
05/08 08:03:48 - mmengine - INFO - Saving checkpoint at 12000 iterations
05/08 08:04:40 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9764  data_time: 0.0278  memory: 3258  
05/08 08:05:03 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.24s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 08:05:11 - mmengine - INFO - start multi processing evaluation ...
DONE (t=54.02s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.403
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.747
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.370
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.229
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.481
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.917
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.524
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.883
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.516
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.376
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.597
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.917
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.524
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.524
05/08 08:06:05 - mmengine - INFO - segm_mAP_copypaste: 0.403 0.747 0.370 0.229 0.481 0.917
05/08 08:06:06 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.4030  coco/segm_mAP_50: 0.7470  coco/segm_mAP_75: 0.3700  coco/segm_mAP_s: 0.2290  coco/segm_mAP_m: 0.4810  coco/segm_mAP_l: 0.9170  data_time: 0.0277  time: 0.9746
05/08 08:07:17 - mmengine - INFO - Iter(train) [12050/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:22:55  time: 3.1412  data_time: 1.7701  memory: 29095  grad_norm: 61.7022  loss: 30.9455  loss_cls: 0.6720  loss_mask: 0.1961  loss_dice: 2.0615  d0.loss_cls: 1.1005  d0.loss_mask: 0.2363  d0.loss_dice: 2.2956  d1.loss_cls: 0.7682  d1.loss_mask: 0.2247  d1.loss_dice: 2.3432  d2.loss_cls: 0.6922  d2.loss_mask: 0.2127  d2.loss_dice: 2.2406  d3.loss_cls: 0.6871  d3.loss_mask: 0.2057  d3.loss_dice: 2.1394  d4.loss_cls: 0.6794  d4.loss_mask: 0.2046  d4.loss_dice: 2.1196  d5.loss_cls: 0.7058  d5.loss_mask: 0.2016  d5.loss_dice: 2.0993  d6.loss_cls: 0.7200  d6.loss_mask: 0.2006  d6.loss_dice: 2.0301  d7.loss_cls: 0.7062  d7.loss_mask: 0.1968  d7.loss_dice: 2.0420  d8.loss_cls: 0.7106  d8.loss_mask: 0.1960  d8.loss_dice: 2.0572
05/08 08:08:29 - mmengine - INFO - Iter(train) [12100/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:21:35  time: 1.4461  data_time: 0.0570  memory: 28960  grad_norm: 51.7262  loss: 29.4630  loss_cls: 0.6619  loss_mask: 0.1872  loss_dice: 1.9376  d0.loss_cls: 1.0895  d0.loss_mask: 0.2295  d0.loss_dice: 2.1850  d1.loss_cls: 0.7483  d1.loss_mask: 0.2152  d1.loss_dice: 2.2146  d2.loss_cls: 0.6846  d2.loss_mask: 0.2032  d2.loss_dice: 2.1064  d3.loss_cls: 0.6738  d3.loss_mask: 0.1952  d3.loss_dice: 2.0125  d4.loss_cls: 0.6640  d4.loss_mask: 0.1935  d4.loss_dice: 2.0051  d5.loss_cls: 0.6843  d5.loss_mask: 0.1908  d5.loss_dice: 1.9831  d6.loss_cls: 0.6985  d6.loss_mask: 0.1899  d6.loss_dice: 1.9151  d7.loss_cls: 0.6621  d7.loss_mask: 0.1906  d7.loss_dice: 1.9381  d8.loss_cls: 0.6902  d8.loss_mask: 0.1869  d8.loss_dice: 1.9263
05/08 08:09:41 - mmengine - INFO - Iter(train) [12150/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:20:16  time: 1.4311  data_time: 0.0578  memory: 29416  grad_norm: 53.6141  loss: 30.1501  loss_cls: 0.6772  loss_mask: 0.1915  loss_dice: 1.9827  d0.loss_cls: 1.0970  d0.loss_mask: 0.2301  d0.loss_dice: 2.2222  d1.loss_cls: 0.7786  d1.loss_mask: 0.2153  d1.loss_dice: 2.2688  d2.loss_cls: 0.6950  d2.loss_mask: 0.2051  d2.loss_dice: 2.1737  d3.loss_cls: 0.6849  d3.loss_mask: 0.2005  d3.loss_dice: 2.0623  d4.loss_cls: 0.6757  d4.loss_mask: 0.1990  d4.loss_dice: 2.0449  d5.loss_cls: 0.7091  d5.loss_mask: 0.1953  d5.loss_dice: 2.0327  d6.loss_cls: 0.7159  d6.loss_mask: 0.1949  d6.loss_dice: 1.9696  d7.loss_cls: 0.6730  d7.loss_mask: 0.1940  d7.loss_dice: 1.9955  d8.loss_cls: 0.6963  d8.loss_mask: 0.1904  d8.loss_dice: 1.9790
05/08 08:10:54 - mmengine - INFO - Iter(train) [12200/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:18:57  time: 1.4699  data_time: 0.1069  memory: 28761  grad_norm: 55.5868  loss: 29.8777  loss_cls: 0.6504  loss_mask: 0.1968  loss_dice: 1.9673  d0.loss_cls: 1.0999  d0.loss_mask: 0.2402  d0.loss_dice: 2.2103  d1.loss_cls: 0.7597  d1.loss_mask: 0.2284  d1.loss_dice: 2.2562  d2.loss_cls: 0.6792  d2.loss_mask: 0.2143  d2.loss_dice: 2.1489  d3.loss_cls: 0.6630  d3.loss_mask: 0.2071  d3.loss_dice: 2.0549  d4.loss_cls: 0.6509  d4.loss_mask: 0.2043  d4.loss_dice: 2.0379  d5.loss_cls: 0.6653  d5.loss_mask: 0.2008  d5.loss_dice: 2.0197  d6.loss_cls: 0.6770  d6.loss_mask: 0.2003  d6.loss_dice: 1.9712  d7.loss_cls: 0.6508  d7.loss_mask: 0.1985  d7.loss_dice: 1.9862  d8.loss_cls: 0.6629  d8.loss_mask: 0.1996  d8.loss_dice: 1.9759
05/08 08:12:06 - mmengine - INFO - Iter(train) [12250/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:17:38  time: 1.4388  data_time: 0.0570  memory: 28958  grad_norm: 48.8019  loss: 28.3239  loss_cls: 0.6332  loss_mask: 0.1794  loss_dice: 1.8596  d0.loss_cls: 1.0801  d0.loss_mask: 0.2189  d0.loss_dice: 2.0746  d1.loss_cls: 0.7330  d1.loss_mask: 0.2079  d1.loss_dice: 2.1156  d2.loss_cls: 0.6617  d2.loss_mask: 0.1939  d2.loss_dice: 2.0227  d3.loss_cls: 0.6543  d3.loss_mask: 0.1890  d3.loss_dice: 1.9264  d4.loss_cls: 0.6515  d4.loss_mask: 0.1886  d4.loss_dice: 1.9009  d5.loss_cls: 0.6674  d5.loss_mask: 0.1842  d5.loss_dice: 1.8957  d6.loss_cls: 0.6570  d6.loss_mask: 0.1848  d6.loss_dice: 1.8598  d7.loss_cls: 0.6326  d7.loss_mask: 0.1815  d7.loss_dice: 1.8742  d8.loss_cls: 0.6406  d8.loss_mask: 0.1834  d8.loss_dice: 1.8715
05/08 08:13:18 - mmengine - INFO - Iter(train) [12300/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:16:18  time: 1.4413  data_time: 0.0575  memory: 29857  grad_norm: 60.6621  loss: 30.1107  loss_cls: 0.6681  loss_mask: 0.1842  loss_dice: 1.9950  d0.loss_cls: 1.1068  d0.loss_mask: 0.2251  d0.loss_dice: 2.2175  d1.loss_cls: 0.7661  d1.loss_mask: 0.2154  d1.loss_dice: 2.2568  d2.loss_cls: 0.6905  d2.loss_mask: 0.2024  d2.loss_dice: 2.1757  d3.loss_cls: 0.6772  d3.loss_mask: 0.1942  d3.loss_dice: 2.0758  d4.loss_cls: 0.6820  d4.loss_mask: 0.1926  d4.loss_dice: 2.0440  d5.loss_cls: 0.6812  d5.loss_mask: 0.1903  d5.loss_dice: 2.0427  d6.loss_cls: 0.6861  d6.loss_mask: 0.1878  d6.loss_dice: 2.0103  d7.loss_cls: 0.6648  d7.loss_mask: 0.1874  d7.loss_dice: 2.0198  d8.loss_cls: 0.6743  d8.loss_mask: 0.1865  d8.loss_dice: 2.0100
05/08 08:14:30 - mmengine - INFO - Iter(train) [12350/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:14:59  time: 1.4309  data_time: 0.0572  memory: 29160  grad_norm: 71.4109  loss: 29.5273  loss_cls: 0.6785  loss_mask: 0.1919  loss_dice: 1.9384  d0.loss_cls: 1.0839  d0.loss_mask: 0.2350  d0.loss_dice: 2.1623  d1.loss_cls: 0.7582  d1.loss_mask: 0.2202  d1.loss_dice: 2.1917  d2.loss_cls: 0.6739  d2.loss_mask: 0.2073  d2.loss_dice: 2.1126  d3.loss_cls: 0.6797  d3.loss_mask: 0.1998  d3.loss_dice: 1.9992  d4.loss_cls: 0.6748  d4.loss_mask: 0.1979  d4.loss_dice: 1.9719  d5.loss_cls: 0.6796  d5.loss_mask: 0.1954  d5.loss_dice: 1.9819  d6.loss_cls: 0.6942  d6.loss_mask: 0.1971  d6.loss_dice: 1.9587  d7.loss_cls: 0.6833  d7.loss_mask: 0.1957  d7.loss_dice: 1.9552  d8.loss_cls: 0.6666  d8.loss_mask: 0.1928  d8.loss_dice: 1.9496
05/08 08:15:42 - mmengine - INFO - Iter(train) [12400/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:13:40  time: 1.4393  data_time: 0.0602  memory: 29085  grad_norm: 50.6462  loss: 29.9117  loss_cls: 0.6532  loss_mask: 0.1928  loss_dice: 1.9837  d0.loss_cls: 1.0999  d0.loss_mask: 0.2310  d0.loss_dice: 2.2077  d1.loss_cls: 0.7605  d1.loss_mask: 0.2167  d1.loss_dice: 2.2329  d2.loss_cls: 0.6919  d2.loss_mask: 0.2087  d2.loss_dice: 2.1393  d3.loss_cls: 0.6801  d3.loss_mask: 0.2025  d3.loss_dice: 2.0486  d4.loss_cls: 0.6760  d4.loss_mask: 0.1982  d4.loss_dice: 2.0287  d5.loss_cls: 0.6701  d5.loss_mask: 0.1961  d5.loss_dice: 2.0266  d6.loss_cls: 0.6956  d6.loss_mask: 0.1955  d6.loss_dice: 1.9845  d7.loss_cls: 0.6855  d7.loss_mask: 0.1932  d7.loss_dice: 1.9713  d8.loss_cls: 0.6655  d8.loss_mask: 0.1933  d8.loss_dice: 1.9823
05/08 08:16:53 - mmengine - INFO - Iter(train) [12450/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:12:20  time: 1.4144  data_time: 0.0562  memory: 28629  grad_norm: 89.3115  loss: 29.7066  loss_cls: 0.6366  loss_mask: 0.2004  loss_dice: 1.9794  d0.loss_cls: 1.0792  d0.loss_mask: 0.2448  d0.loss_dice: 2.1842  d1.loss_cls: 0.7528  d1.loss_mask: 0.2253  d1.loss_dice: 2.2127  d2.loss_cls: 0.6755  d2.loss_mask: 0.2166  d2.loss_dice: 2.1199  d3.loss_cls: 0.6775  d3.loss_mask: 0.2078  d3.loss_dice: 2.0304  d4.loss_cls: 0.6543  d4.loss_mask: 0.2047  d4.loss_dice: 2.0240  d5.loss_cls: 0.6522  d5.loss_mask: 0.2020  d5.loss_dice: 2.0094  d6.loss_cls: 0.6826  d6.loss_mask: 0.2024  d6.loss_dice: 1.9660  d7.loss_cls: 0.6802  d7.loss_mask: 0.1995  d7.loss_dice: 1.9540  d8.loss_cls: 0.6507  d8.loss_mask: 0.2014  d8.loss_dice: 1.9799
05/08 08:18:04 - mmengine - INFO - Iter(train) [12500/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:11:00  time: 1.4283  data_time: 0.0559  memory: 28645  grad_norm: 65.9266  loss: 29.2285  loss_cls: 0.6362  loss_mask: 0.1989  loss_dice: 1.9440  d0.loss_cls: 1.0707  d0.loss_mask: 0.2389  d0.loss_dice: 2.1472  d1.loss_cls: 0.7393  d1.loss_mask: 0.2243  d1.loss_dice: 2.1659  d2.loss_cls: 0.6664  d2.loss_mask: 0.2149  d2.loss_dice: 2.0791  d3.loss_cls: 0.6699  d3.loss_mask: 0.2085  d3.loss_dice: 1.9800  d4.loss_cls: 0.6432  d4.loss_mask: 0.2068  d4.loss_dice: 1.9925  d5.loss_cls: 0.6404  d5.loss_mask: 0.2020  d5.loss_dice: 1.9799  d6.loss_cls: 0.6718  d6.loss_mask: 0.1994  d6.loss_dice: 1.9391  d7.loss_cls: 0.6520  d7.loss_mask: 0.1995  d7.loss_dice: 1.9343  d8.loss_cls: 0.6284  d8.loss_mask: 0.1991  d8.loss_dice: 1.9558
05/08 08:19:17 - mmengine - INFO - Iter(train) [12550/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:09:42  time: 1.4507  data_time: 0.0676  memory: 29204  grad_norm: 62.4448  loss: 29.9322  loss_cls: 0.6813  loss_mask: 0.1904  loss_dice: 1.9889  d0.loss_cls: 1.0936  d0.loss_mask: 0.2293  d0.loss_dice: 2.1930  d1.loss_cls: 0.7510  d1.loss_mask: 0.2188  d1.loss_dice: 2.2260  d2.loss_cls: 0.6795  d2.loss_mask: 0.2043  d2.loss_dice: 2.1386  d3.loss_cls: 0.6911  d3.loss_mask: 0.2001  d3.loss_dice: 2.0434  d4.loss_cls: 0.6770  d4.loss_mask: 0.1970  d4.loss_dice: 2.0358  d5.loss_cls: 0.6724  d5.loss_mask: 0.1948  d5.loss_dice: 2.0233  d6.loss_cls: 0.7024  d6.loss_mask: 0.1911  d6.loss_dice: 1.9771  d7.loss_cls: 0.7009  d7.loss_mask: 0.1921  d7.loss_dice: 1.9787  d8.loss_cls: 0.6786  d8.loss_mask: 0.1902  d8.loss_dice: 1.9913
05/08 08:20:30 - mmengine - INFO - Iter(train) [12600/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:08:23  time: 1.4584  data_time: 0.1097  memory: 29214  grad_norm: 66.0626  loss: 29.7775  loss_cls: 0.6666  loss_mask: 0.1921  loss_dice: 1.9762  d0.loss_cls: 1.0832  d0.loss_mask: 0.2317  d0.loss_dice: 2.1880  d1.loss_cls: 0.7466  d1.loss_mask: 0.2213  d1.loss_dice: 2.2238  d2.loss_cls: 0.6709  d2.loss_mask: 0.2073  d2.loss_dice: 2.1457  d3.loss_cls: 0.6671  d3.loss_mask: 0.2029  d3.loss_dice: 2.0562  d4.loss_cls: 0.6596  d4.loss_mask: 0.1992  d4.loss_dice: 2.0336  d5.loss_cls: 0.6566  d5.loss_mask: 0.1962  d5.loss_dice: 2.0178  d6.loss_cls: 0.6864  d6.loss_mask: 0.1940  d6.loss_dice: 1.9679  d7.loss_cls: 0.6793  d7.loss_mask: 0.1946  d7.loss_dice: 1.9680  d8.loss_cls: 0.6746  d8.loss_mask: 0.1927  d8.loss_dice: 1.9774
05/08 08:21:42 - mmengine - INFO - Iter(train) [12650/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:07:05  time: 1.4574  data_time: 0.0572  memory: 29366  grad_norm: 57.4881  loss: 30.6430  loss_cls: 0.6918  loss_mask: 0.1878  loss_dice: 2.0498  d0.loss_cls: 1.0962  d0.loss_mask: 0.2235  d0.loss_dice: 2.2704  d1.loss_cls: 0.7610  d1.loss_mask: 0.2185  d1.loss_dice: 2.3028  d2.loss_cls: 0.6807  d2.loss_mask: 0.2066  d2.loss_dice: 2.2122  d3.loss_cls: 0.6996  d3.loss_mask: 0.1960  d3.loss_dice: 2.1087  d4.loss_cls: 0.6903  d4.loss_mask: 0.1961  d4.loss_dice: 2.1065  d5.loss_cls: 0.6867  d5.loss_mask: 0.1926  d5.loss_dice: 2.0996  d6.loss_cls: 0.6853  d6.loss_mask: 0.1931  d6.loss_dice: 2.0541  d7.loss_cls: 0.6872  d7.loss_mask: 0.1896  d7.loss_dice: 2.0336  d8.loss_cls: 0.6445  d8.loss_mask: 0.1892  d8.loss_dice: 2.0892
05/08 08:22:55 - mmengine - INFO - Iter(train) [12700/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:05:46  time: 1.4513  data_time: 0.0660  memory: 29361  grad_norm: 47.9948  loss: 30.8399  loss_cls: 0.6824  loss_mask: 0.1956  loss_dice: 2.0529  d0.loss_cls: 1.1025  d0.loss_mask: 0.2357  d0.loss_dice: 2.2739  d1.loss_cls: 0.7907  d1.loss_mask: 0.2258  d1.loss_dice: 2.3013  d2.loss_cls: 0.7041  d2.loss_mask: 0.2138  d2.loss_dice: 2.2137  d3.loss_cls: 0.7124  d3.loss_mask: 0.2048  d3.loss_dice: 2.1192  d4.loss_cls: 0.7000  d4.loss_mask: 0.2045  d4.loss_dice: 2.1204  d5.loss_cls: 0.6908  d5.loss_mask: 0.1994  d5.loss_dice: 2.1194  d6.loss_cls: 0.6627  d6.loss_mask: 0.1997  d6.loss_dice: 2.0762  d7.loss_cls: 0.6601  d7.loss_mask: 0.1984  d7.loss_dice: 2.0587  d8.loss_cls: 0.6511  d8.loss_mask: 0.1967  d8.loss_dice: 2.0730
05/08 08:24:06 - mmengine - INFO - Iter(train) [12750/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:04:27  time: 1.4297  data_time: 0.0515  memory: 29027  grad_norm: 54.7800  loss: 30.0426  loss_cls: 0.6560  loss_mask: 0.1980  loss_dice: 1.9868  d0.loss_cls: 1.0938  d0.loss_mask: 0.2458  d0.loss_dice: 2.2121  d1.loss_cls: 0.7547  d1.loss_mask: 0.2317  d1.loss_dice: 2.2435  d2.loss_cls: 0.6821  d2.loss_mask: 0.2166  d2.loss_dice: 2.1534  d3.loss_cls: 0.6779  d3.loss_mask: 0.2086  d3.loss_dice: 2.0611  d4.loss_cls: 0.6696  d4.loss_mask: 0.2095  d4.loss_dice: 2.0696  d5.loss_cls: 0.6735  d5.loss_mask: 0.2036  d5.loss_dice: 2.0504  d6.loss_cls: 0.6522  d6.loss_mask: 0.2003  d6.loss_dice: 2.0153  d7.loss_cls: 0.6344  d7.loss_mask: 0.1997  d7.loss_dice: 2.0091  d8.loss_cls: 0.6332  d8.loss_mask: 0.1975  d8.loss_dice: 2.0027
05/08 08:25:20 - mmengine - INFO - Iter(train) [12800/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:03:09  time: 1.4614  data_time: 0.0609  memory: 28776  grad_norm: 61.0051  loss: 28.6446  loss_cls: 0.6512  loss_mask: 0.1839  loss_dice: 1.8659  d0.loss_cls: 1.0771  d0.loss_mask: 0.2223  d0.loss_dice: 2.0945  d1.loss_cls: 0.7515  d1.loss_mask: 0.2133  d1.loss_dice: 2.1158  d2.loss_cls: 0.6628  d2.loss_mask: 0.2027  d2.loss_dice: 2.0407  d3.loss_cls: 0.6639  d3.loss_mask: 0.1939  d3.loss_dice: 1.9516  d4.loss_cls: 0.6514  d4.loss_mask: 0.1933  d4.loss_dice: 1.9547  d5.loss_cls: 0.6799  d5.loss_mask: 0.1907  d5.loss_dice: 1.9347  d6.loss_cls: 0.6481  d6.loss_mask: 0.1874  d6.loss_dice: 1.8962  d7.loss_cls: 0.6273  d7.loss_mask: 0.1866  d7.loss_dice: 1.8923  d8.loss_cls: 0.6360  d8.loss_mask: 0.1843  d8.loss_dice: 1.8905
05/08 08:26:32 - mmengine - INFO - Iter(train) [12850/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:01:50  time: 1.4475  data_time: 0.0633  memory: 29270  grad_norm: 42.6918  loss: 30.2719  loss_cls: 0.6694  loss_mask: 0.1979  loss_dice: 1.9778  d0.loss_cls: 1.1018  d0.loss_mask: 0.2425  d0.loss_dice: 2.2425  d1.loss_cls: 0.7661  d1.loss_mask: 0.2275  d1.loss_dice: 2.2584  d2.loss_cls: 0.6864  d2.loss_mask: 0.2161  d2.loss_dice: 2.1614  d3.loss_cls: 0.6867  d3.loss_mask: 0.2089  d3.loss_dice: 2.0732  d4.loss_cls: 0.7005  d4.loss_mask: 0.2078  d4.loss_dice: 2.0634  d5.loss_cls: 0.7187  d5.loss_mask: 0.2050  d5.loss_dice: 2.0464  d6.loss_cls: 0.6915  d6.loss_mask: 0.2025  d6.loss_dice: 1.9962  d7.loss_cls: 0.6578  d7.loss_mask: 0.2008  d7.loss_dice: 2.0061  d8.loss_cls: 0.6561  d8.loss_mask: 0.1993  d8.loss_dice: 2.0028
05/08 08:27:44 - mmengine - INFO - Iter(train) [12900/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 3:00:32  time: 1.4416  data_time: 0.0623  memory: 29451  grad_norm: 64.2018  loss: 28.7859  loss_cls: 0.6476  loss_mask: 0.1909  loss_dice: 1.8840  d0.loss_cls: 1.0741  d0.loss_mask: 0.2312  d0.loss_dice: 2.0974  d1.loss_cls: 0.7429  d1.loss_mask: 0.2175  d1.loss_dice: 2.1206  d2.loss_cls: 0.6717  d2.loss_mask: 0.2051  d2.loss_dice: 2.0351  d3.loss_cls: 0.6744  d3.loss_mask: 0.1985  d3.loss_dice: 1.9549  d4.loss_cls: 0.6696  d4.loss_mask: 0.1966  d4.loss_dice: 1.9427  d5.loss_cls: 0.6732  d5.loss_mask: 0.1953  d5.loss_dice: 1.9328  d6.loss_cls: 0.6844  d6.loss_mask: 0.1916  d6.loss_dice: 1.8720  d7.loss_cls: 0.6683  d7.loss_mask: 0.1937  d7.loss_dice: 1.8826  d8.loss_cls: 0.6608  d8.loss_mask: 0.1930  d8.loss_dice: 1.8832
05/08 08:28:57 - mmengine - INFO - Iter(train) [12950/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:59:14  time: 1.4674  data_time: 0.0617  memory: 29718  grad_norm: 57.7121  loss: 29.5534  loss_cls: 0.6524  loss_mask: 0.1875  loss_dice: 1.9568  d0.loss_cls: 1.1004  d0.loss_mask: 0.2346  d0.loss_dice: 2.1744  d1.loss_cls: 0.7502  d1.loss_mask: 0.2148  d1.loss_dice: 2.2164  d2.loss_cls: 0.6745  d2.loss_mask: 0.2023  d2.loss_dice: 2.1308  d3.loss_cls: 0.6602  d3.loss_mask: 0.1993  d3.loss_dice: 2.0483  d4.loss_cls: 0.6524  d4.loss_mask: 0.1953  d4.loss_dice: 2.0247  d5.loss_cls: 0.6523  d5.loss_mask: 0.1929  d5.loss_dice: 2.0071  d6.loss_cls: 0.6721  d6.loss_mask: 0.1889  d6.loss_dice: 1.9504  d7.loss_cls: 0.6492  d7.loss_mask: 0.1897  d7.loss_dice: 1.9706  d8.loss_cls: 0.6430  d8.loss_mask: 0.1886  d8.loss_dice: 1.9732
05/08 08:30:11 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 08:30:11 - mmengine - INFO - Iter(train) [13000/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:57:56  time: 1.4788  data_time: 0.1108  memory: 29090  grad_norm: 63.6674  loss: 31.3879  loss_cls: 0.6820  loss_mask: 0.2076  loss_dice: 2.0844  d0.loss_cls: 1.1160  d0.loss_mask: 0.2514  d0.loss_dice: 2.3187  d1.loss_cls: 0.7684  d1.loss_mask: 0.2416  d1.loss_dice: 2.3595  d2.loss_cls: 0.7068  d2.loss_mask: 0.2265  d2.loss_dice: 2.2732  d3.loss_cls: 0.6881  d3.loss_mask: 0.2180  d3.loss_dice: 2.1693  d4.loss_cls: 0.6841  d4.loss_mask: 0.2153  d4.loss_dice: 2.1557  d5.loss_cls: 0.6829  d5.loss_mask: 0.2130  d5.loss_dice: 2.1432  d6.loss_cls: 0.7047  d6.loss_mask: 0.2100  d6.loss_dice: 2.0883  d7.loss_cls: 0.6751  d7.loss_mask: 0.2119  d7.loss_dice: 2.1013  d8.loss_cls: 0.6763  d8.loss_mask: 0.2090  d8.loss_dice: 2.1055
05/08 08:30:11 - mmengine - INFO - Saving checkpoint at 13000 iterations
05/08 08:31:03 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9842  data_time: 0.0280  memory: 3258  
05/08 08:31:27 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.40s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 08:31:34 - mmengine - INFO - start multi processing evaluation ...
DONE (t=52.93s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.414
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.752
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.371
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.244
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.488
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.513
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.914
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.492
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.371
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.580
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.933
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.513
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.513
05/08 08:32:27 - mmengine - INFO - segm_mAP_copypaste: 0.414 0.752 0.371 0.244 0.488 0.861
05/08 08:32:28 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.4140  coco/segm_mAP_50: 0.7520  coco/segm_mAP_75: 0.3710  coco/segm_mAP_s: 0.2440  coco/segm_mAP_m: 0.4880  coco/segm_mAP_l: 0.8610  data_time: 0.0280  time: 0.9821
05/08 08:33:40 - mmengine - INFO - Iter(train) [13050/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:57:23  time: 3.1275  data_time: 1.7541  memory: 28566  grad_norm: 51.9923  loss: 29.4137  loss_cls: 0.6878  loss_mask: 0.1972  loss_dice: 1.9189  d0.loss_cls: 1.0862  d0.loss_mask: 0.2373  d0.loss_dice: 2.1378  d1.loss_cls: 0.7474  d1.loss_mask: 0.2230  d1.loss_dice: 2.1650  d2.loss_cls: 0.6890  d2.loss_mask: 0.2115  d2.loss_dice: 2.0744  d3.loss_cls: 0.6875  d3.loss_mask: 0.2059  d3.loss_dice: 1.9780  d4.loss_cls: 0.6921  d4.loss_mask: 0.2018  d4.loss_dice: 1.9611  d5.loss_cls: 0.6953  d5.loss_mask: 0.2005  d5.loss_dice: 1.9598  d6.loss_cls: 0.6890  d6.loss_mask: 0.2011  d6.loss_dice: 1.9432  d7.loss_cls: 0.6893  d7.loss_mask: 0.1976  d7.loss_dice: 1.9273  d8.loss_cls: 0.6924  d8.loss_mask: 0.1970  d8.loss_dice: 1.9193
05/08 08:34:52 - mmengine - INFO - Iter(train) [13100/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:56:04  time: 1.4400  data_time: 0.0608  memory: 28832  grad_norm: 63.5975  loss: 30.6245  loss_cls: 0.6967  loss_mask: 0.1912  loss_dice: 2.0441  d0.loss_cls: 1.0883  d0.loss_mask: 0.2298  d0.loss_dice: 2.2624  d1.loss_cls: 0.7523  d1.loss_mask: 0.2214  d1.loss_dice: 2.3003  d2.loss_cls: 0.6940  d2.loss_mask: 0.2103  d2.loss_dice: 2.2035  d3.loss_cls: 0.6857  d3.loss_mask: 0.2034  d3.loss_dice: 2.1034  d4.loss_cls: 0.6948  d4.loss_mask: 0.2000  d4.loss_dice: 2.0805  d5.loss_cls: 0.7144  d5.loss_mask: 0.1989  d5.loss_dice: 2.0764  d6.loss_cls: 0.6742  d6.loss_mask: 0.1961  d6.loss_dice: 2.0756  d7.loss_cls: 0.6724  d7.loss_mask: 0.1925  d7.loss_dice: 2.0566  d8.loss_cls: 0.6608  d8.loss_mask: 0.1932  d8.loss_dice: 2.0514
05/08 08:36:05 - mmengine - INFO - Iter(train) [13150/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:54:45  time: 1.4624  data_time: 0.0570  memory: 29636  grad_norm: 59.6380  loss: 30.8780  loss_cls: 0.7042  loss_mask: 0.2037  loss_dice: 2.0514  d0.loss_cls: 1.1010  d0.loss_mask: 0.2420  d0.loss_dice: 2.2689  d1.loss_cls: 0.7571  d1.loss_mask: 0.2261  d1.loss_dice: 2.3105  d2.loss_cls: 0.6978  d2.loss_mask: 0.2165  d2.loss_dice: 2.2156  d3.loss_cls: 0.7015  d3.loss_mask: 0.2097  d3.loss_dice: 2.1083  d4.loss_cls: 0.6864  d4.loss_mask: 0.2077  d4.loss_dice: 2.1037  d5.loss_cls: 0.7069  d5.loss_mask: 0.2064  d5.loss_dice: 2.0893  d6.loss_cls: 0.7095  d6.loss_mask: 0.2055  d6.loss_dice: 2.0591  d7.loss_cls: 0.6674  d7.loss_mask: 0.2040  d7.loss_dice: 2.0654  d8.loss_cls: 0.6759  d8.loss_mask: 0.2044  d8.loss_dice: 2.0720
05/08 08:37:18 - mmengine - INFO - Iter(train) [13200/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:53:27  time: 1.4568  data_time: 0.0669  memory: 29604  grad_norm: 57.5814  loss: 29.9168  loss_cls: 0.6474  loss_mask: 0.2046  loss_dice: 2.0015  d0.loss_cls: 1.0926  d0.loss_mask: 0.2465  d0.loss_dice: 2.1924  d1.loss_cls: 0.7506  d1.loss_mask: 0.2264  d1.loss_dice: 2.2353  d2.loss_cls: 0.6923  d2.loss_mask: 0.2141  d2.loss_dice: 2.1292  d3.loss_cls: 0.6894  d3.loss_mask: 0.2079  d3.loss_dice: 2.0246  d4.loss_cls: 0.6707  d4.loss_mask: 0.2058  d4.loss_dice: 2.0239  d5.loss_cls: 0.6826  d5.loss_mask: 0.2054  d5.loss_dice: 2.0091  d6.loss_cls: 0.7016  d6.loss_mask: 0.2027  d6.loss_dice: 1.9596  d7.loss_cls: 0.6619  d7.loss_mask: 0.2029  d7.loss_dice: 1.9720  d8.loss_cls: 0.6616  d8.loss_mask: 0.2027  d8.loss_dice: 1.9999
05/08 08:38:30 - mmengine - INFO - Iter(train) [13250/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:52:08  time: 1.4453  data_time: 0.0564  memory: 28699  grad_norm: 68.9927  loss: 29.5483  loss_cls: 0.6389  loss_mask: 0.1993  loss_dice: 1.9629  d0.loss_cls: 1.0850  d0.loss_mask: 0.2431  d0.loss_dice: 2.1544  d1.loss_cls: 0.7503  d1.loss_mask: 0.2275  d1.loss_dice: 2.2035  d2.loss_cls: 0.6912  d2.loss_mask: 0.2134  d2.loss_dice: 2.1079  d3.loss_cls: 0.6850  d3.loss_mask: 0.2059  d3.loss_dice: 2.0022  d4.loss_cls: 0.6647  d4.loss_mask: 0.2049  d4.loss_dice: 1.9891  d5.loss_cls: 0.6653  d5.loss_mask: 0.2035  d5.loss_dice: 1.9869  d6.loss_cls: 0.7092  d6.loss_mask: 0.1993  d6.loss_dice: 1.9231  d7.loss_cls: 0.6926  d7.loss_mask: 0.1986  d7.loss_dice: 1.9243  d8.loss_cls: 0.6516  d8.loss_mask: 0.1995  d8.loss_dice: 1.9652
05/08 08:39:43 - mmengine - INFO - Iter(train) [13300/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:50:50  time: 1.4581  data_time: 0.0563  memory: 29396  grad_norm: 175.4571  loss: 28.5835  loss_cls: 0.6177  loss_mask: 0.1943  loss_dice: 1.8706  d0.loss_cls: 1.0980  d0.loss_mask: 0.2361  d0.loss_dice: 2.0954  d1.loss_cls: 0.7584  d1.loss_mask: 0.2257  d1.loss_dice: 2.1243  d2.loss_cls: 0.6838  d2.loss_mask: 0.2100  d2.loss_dice: 2.0282  d3.loss_cls: 0.6914  d3.loss_mask: 0.2011  d3.loss_dice: 1.9048  d4.loss_cls: 0.6726  d4.loss_mask: 0.2011  d4.loss_dice: 1.9039  d5.loss_cls: 0.6643  d5.loss_mask: 0.1982  d5.loss_dice: 1.8922  d6.loss_cls: 0.6719  d6.loss_mask: 0.1955  d6.loss_dice: 1.8508  d7.loss_cls: 0.6708  d7.loss_mask: 0.1931  d7.loss_dice: 1.8334  d8.loss_cls: 0.6344  d8.loss_mask: 0.1938  d8.loss_dice: 1.8676
05/08 08:40:55 - mmengine - INFO - Iter(train) [13350/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:49:31  time: 1.4389  data_time: 0.0528  memory: 29217  grad_norm: 60.3280  loss: 31.0557  loss_cls: 0.6510  loss_mask: 0.1929  loss_dice: 2.0837  d0.loss_cls: 1.0994  d0.loss_mask: 0.2312  d0.loss_dice: 2.3178  d1.loss_cls: 0.7606  d1.loss_mask: 0.2194  d1.loss_dice: 2.3470  d2.loss_cls: 0.7194  d2.loss_mask: 0.2104  d2.loss_dice: 2.2534  d3.loss_cls: 0.7230  d3.loss_mask: 0.1986  d3.loss_dice: 2.1247  d4.loss_cls: 0.7228  d4.loss_mask: 0.1996  d4.loss_dice: 2.1183  d5.loss_cls: 0.7021  d5.loss_mask: 0.1977  d5.loss_dice: 2.1231  d6.loss_cls: 0.6793  d6.loss_mask: 0.1959  d6.loss_dice: 2.0883  d7.loss_cls: 0.6771  d7.loss_mask: 0.1950  d7.loss_dice: 2.0807  d8.loss_cls: 0.6734  d8.loss_mask: 0.1921  d8.loss_dice: 2.0779
05/08 08:42:09 - mmengine - INFO - Iter(train) [13400/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:48:14  time: 1.4914  data_time: 0.0950  memory: 29367  grad_norm: 61.6996  loss: 30.7244  loss_cls: 0.6757  loss_mask: 0.1933  loss_dice: 2.0340  d0.loss_cls: 1.1021  d0.loss_mask: 0.2344  d0.loss_dice: 2.2583  d1.loss_cls: 0.7750  d1.loss_mask: 0.2229  d1.loss_dice: 2.3009  d2.loss_cls: 0.7258  d2.loss_mask: 0.2105  d2.loss_dice: 2.2073  d3.loss_cls: 0.7400  d3.loss_mask: 0.2016  d3.loss_dice: 2.0888  d4.loss_cls: 0.7178  d4.loss_mask: 0.2000  d4.loss_dice: 2.0895  d5.loss_cls: 0.7080  d5.loss_mask: 0.1992  d5.loss_dice: 2.0822  d6.loss_cls: 0.6779  d6.loss_mask: 0.1957  d6.loss_dice: 2.0546  d7.loss_cls: 0.6808  d7.loss_mask: 0.1930  d7.loss_dice: 2.0430  d8.loss_cls: 0.7074  d8.loss_mask: 0.1922  d8.loss_dice: 2.0128
05/08 08:43:22 - mmengine - INFO - Iter(train) [13450/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:46:55  time: 1.4531  data_time: 0.0519  memory: 29281  grad_norm: 60.5230  loss: 29.3533  loss_cls: 0.6574  loss_mask: 0.1883  loss_dice: 1.9291  d0.loss_cls: 1.0887  d0.loss_mask: 0.2312  d0.loss_dice: 2.1505  d1.loss_cls: 0.7518  d1.loss_mask: 0.2195  d1.loss_dice: 2.1868  d2.loss_cls: 0.6994  d2.loss_mask: 0.2054  d2.loss_dice: 2.0958  d3.loss_cls: 0.7009  d3.loss_mask: 0.1989  d3.loss_dice: 1.9901  d4.loss_cls: 0.6765  d4.loss_mask: 0.1977  d4.loss_dice: 1.9848  d5.loss_cls: 0.6742  d5.loss_mask: 0.1949  d5.loss_dice: 1.9776  d6.loss_cls: 0.6541  d6.loss_mask: 0.1925  d6.loss_dice: 1.9452  d7.loss_cls: 0.6614  d7.loss_mask: 0.1913  d7.loss_dice: 1.9328  d8.loss_cls: 0.6569  d8.loss_mask: 0.1905  d8.loss_dice: 1.9292
05/08 08:44:34 - mmengine - INFO - Iter(train) [13500/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:45:37  time: 1.4322  data_time: 0.0591  memory: 28732  grad_norm: 69.6215  loss: 29.8481  loss_cls: 0.6956  loss_mask: 0.1904  loss_dice: 1.9511  d0.loss_cls: 1.0948  d0.loss_mask: 0.2308  d0.loss_dice: 2.1859  d1.loss_cls: 0.7646  d1.loss_mask: 0.2159  d1.loss_dice: 2.2171  d2.loss_cls: 0.7232  d2.loss_mask: 0.2052  d2.loss_dice: 2.1209  d3.loss_cls: 0.7152  d3.loss_mask: 0.1989  d3.loss_dice: 2.0187  d4.loss_cls: 0.7217  d4.loss_mask: 0.1977  d4.loss_dice: 1.9931  d5.loss_cls: 0.7016  d5.loss_mask: 0.1944  d5.loss_dice: 1.9806  d6.loss_cls: 0.6869  d6.loss_mask: 0.1919  d6.loss_dice: 1.9620  d7.loss_cls: 0.6997  d7.loss_mask: 0.1890  d7.loss_dice: 1.9501  d8.loss_cls: 0.7025  d8.loss_mask: 0.1910  d8.loss_dice: 1.9577
05/08 08:45:46 - mmengine - INFO - Iter(train) [13550/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:44:18  time: 1.4479  data_time: 0.0520  memory: 28763  grad_norm: 52.2395  loss: 29.5103  loss_cls: 0.6906  loss_mask: 0.1930  loss_dice: 1.9188  d0.loss_cls: 1.0906  d0.loss_mask: 0.2347  d0.loss_dice: 2.1484  d1.loss_cls: 0.7513  d1.loss_mask: 0.2213  d1.loss_dice: 2.1927  d2.loss_cls: 0.7203  d2.loss_mask: 0.2083  d2.loss_dice: 2.1037  d3.loss_cls: 0.7098  d3.loss_mask: 0.2052  d3.loss_dice: 1.9960  d4.loss_cls: 0.7055  d4.loss_mask: 0.2009  d4.loss_dice: 1.9738  d5.loss_cls: 0.6857  d5.loss_mask: 0.1985  d5.loss_dice: 1.9593  d6.loss_cls: 0.6812  d6.loss_mask: 0.1965  d6.loss_dice: 1.9185  d7.loss_cls: 0.6902  d7.loss_mask: 0.1947  d7.loss_dice: 1.9165  d8.loss_cls: 0.6757  d8.loss_mask: 0.1962  d8.loss_dice: 1.9325
05/08 08:46:59 - mmengine - INFO - Iter(train) [13600/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:43:00  time: 1.4533  data_time: 0.0569  memory: 29534  grad_norm: 49.8568  loss: 30.3178  loss_cls: 0.7413  loss_mask: 0.1947  loss_dice: 1.9347  d0.loss_cls: 1.0755  d0.loss_mask: 0.2320  d0.loss_dice: 2.2331  d1.loss_cls: 0.7468  d1.loss_mask: 0.2210  d1.loss_dice: 2.2548  d2.loss_cls: 0.7412  d2.loss_mask: 0.2151  d2.loss_dice: 2.1493  d3.loss_cls: 0.7420  d3.loss_mask: 0.2100  d3.loss_dice: 2.0524  d4.loss_cls: 0.7532  d4.loss_mask: 0.2056  d4.loss_dice: 2.0300  d5.loss_cls: 0.7308  d5.loss_mask: 0.1998  d5.loss_dice: 1.9973  d6.loss_cls: 0.7376  d6.loss_mask: 0.1972  d6.loss_dice: 1.9453  d7.loss_cls: 0.7357  d7.loss_mask: 0.1961  d7.loss_dice: 1.9561  d8.loss_cls: 0.7174  d8.loss_mask: 0.1961  d8.loss_dice: 1.9756
05/08 08:48:12 - mmengine - INFO - Iter(train) [13650/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:41:42  time: 1.4596  data_time: 0.0590  memory: 29177  grad_norm: 56.9917  loss: 29.1806  loss_cls: 0.7283  loss_mask: 0.1884  loss_dice: 1.8378  d0.loss_cls: 1.0934  d0.loss_mask: 0.2281  d0.loss_dice: 2.1158  d1.loss_cls: 0.7545  d1.loss_mask: 0.2170  d1.loss_dice: 2.1364  d2.loss_cls: 0.7244  d2.loss_mask: 0.2034  d2.loss_dice: 2.0539  d3.loss_cls: 0.7384  d3.loss_mask: 0.1991  d3.loss_dice: 1.9473  d4.loss_cls: 0.7210  d4.loss_mask: 0.2011  d4.loss_dice: 1.9493  d5.loss_cls: 0.7234  d5.loss_mask: 0.1945  d5.loss_dice: 1.9117  d6.loss_cls: 0.7042  d6.loss_mask: 0.1910  d6.loss_dice: 1.8783  d7.loss_cls: 0.7060  d7.loss_mask: 0.1918  d7.loss_dice: 1.8780  d8.loss_cls: 0.6984  d8.loss_mask: 0.1887  d8.loss_dice: 1.8770
05/08 08:49:23 - mmengine - INFO - Iter(train) [13700/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:40:23  time: 1.4266  data_time: 0.0636  memory: 28237  grad_norm: 69.3108  loss: 29.0875  loss_cls: 0.6885  loss_mask: 0.1977  loss_dice: 1.8735  d0.loss_cls: 1.0783  d0.loss_mask: 0.2316  d0.loss_dice: 2.1198  d1.loss_cls: 0.7453  d1.loss_mask: 0.2212  d1.loss_dice: 2.1399  d2.loss_cls: 0.7141  d2.loss_mask: 0.2090  d2.loss_dice: 2.0376  d3.loss_cls: 0.7153  d3.loss_mask: 0.2024  d3.loss_dice: 1.9542  d4.loss_cls: 0.7241  d4.loss_mask: 0.1993  d4.loss_dice: 1.9342  d5.loss_cls: 0.6804  d5.loss_mask: 0.1962  d5.loss_dice: 1.9277  d6.loss_cls: 0.7232  d6.loss_mask: 0.1932  d6.loss_dice: 1.8760  d7.loss_cls: 0.6645  d7.loss_mask: 0.1965  d7.loss_dice: 1.8972  d8.loss_cls: 0.6737  d8.loss_mask: 0.1947  d8.loss_dice: 1.8783
05/08 08:50:35 - mmengine - INFO - Iter(train) [13750/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:39:05  time: 1.4296  data_time: 0.0531  memory: 29267  grad_norm: 68.7464  loss: 31.2820  loss_cls: 0.6770  loss_mask: 0.2145  loss_dice: 2.0830  d0.loss_cls: 1.1135  d0.loss_mask: 0.2503  d0.loss_dice: 2.3091  d1.loss_cls: 0.7653  d1.loss_mask: 0.2404  d1.loss_dice: 2.3447  d2.loss_cls: 0.7379  d2.loss_mask: 0.2257  d2.loss_dice: 2.2523  d3.loss_cls: 0.7232  d3.loss_mask: 0.2180  d3.loss_dice: 2.1380  d4.loss_cls: 0.7144  d4.loss_mask: 0.2165  d4.loss_dice: 2.1333  d5.loss_cls: 0.6590  d5.loss_mask: 0.2152  d5.loss_dice: 2.1423  d6.loss_cls: 0.6931  d6.loss_mask: 0.2124  d6.loss_dice: 2.0830  d7.loss_cls: 0.6480  d7.loss_mask: 0.2139  d7.loss_dice: 2.0891  d8.loss_cls: 0.6841  d8.loss_mask: 0.2113  d8.loss_dice: 2.0733
05/08 08:51:49 - mmengine - INFO - Iter(train) [13800/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:37:47  time: 1.4798  data_time: 0.1080  memory: 28870  grad_norm: 54.8013  loss: 30.4315  loss_cls: 0.6433  loss_mask: 0.1954  loss_dice: 2.0419  d0.loss_cls: 1.1011  d0.loss_mask: 0.2362  d0.loss_dice: 2.2446  d1.loss_cls: 0.7625  d1.loss_mask: 0.2271  d1.loss_dice: 2.2779  d2.loss_cls: 0.7357  d2.loss_mask: 0.2105  d2.loss_dice: 2.1940  d3.loss_cls: 0.7100  d3.loss_mask: 0.2015  d3.loss_dice: 2.1023  d4.loss_cls: 0.6904  d4.loss_mask: 0.2011  d4.loss_dice: 2.0910  d5.loss_cls: 0.6424  d5.loss_mask: 0.1990  d5.loss_dice: 2.0870  d6.loss_cls: 0.6633  d6.loss_mask: 0.1954  d6.loss_dice: 2.0330  d7.loss_cls: 0.6334  d7.loss_mask: 0.1959  d7.loss_dice: 2.0295  d8.loss_cls: 0.6669  d8.loss_mask: 0.1948  d8.loss_dice: 2.0240
05/08 08:53:01 - mmengine - INFO - Iter(train) [13850/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:36:29  time: 1.4514  data_time: 0.0559  memory: 29213  grad_norm: 61.8881  loss: 30.5739  loss_cls: 0.6484  loss_mask: 0.1977  loss_dice: 2.0335  d0.loss_cls: 1.1059  d0.loss_mask: 0.2413  d0.loss_dice: 2.2537  d1.loss_cls: 0.7772  d1.loss_mask: 0.2285  d1.loss_dice: 2.2860  d2.loss_cls: 0.7426  d2.loss_mask: 0.2173  d2.loss_dice: 2.2031  d3.loss_cls: 0.7186  d3.loss_mask: 0.2111  d3.loss_dice: 2.0996  d4.loss_cls: 0.6982  d4.loss_mask: 0.2060  d4.loss_dice: 2.0896  d5.loss_cls: 0.6523  d5.loss_mask: 0.2029  d5.loss_dice: 2.0894  d6.loss_cls: 0.6622  d6.loss_mask: 0.2009  d6.loss_dice: 2.0366  d7.loss_cls: 0.6401  d7.loss_mask: 0.2001  d7.loss_dice: 2.0432  d8.loss_cls: 0.6622  d8.loss_mask: 0.1987  d8.loss_dice: 2.0270
05/08 08:54:14 - mmengine - INFO - Iter(train) [13900/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:35:11  time: 1.4506  data_time: 0.0545  memory: 28596  grad_norm: 69.6190  loss: 30.1825  loss_cls: 0.6519  loss_mask: 0.1937  loss_dice: 2.0238  d0.loss_cls: 1.0876  d0.loss_mask: 0.2315  d0.loss_dice: 2.2119  d1.loss_cls: 0.7711  d1.loss_mask: 0.2218  d1.loss_dice: 2.2459  d2.loss_cls: 0.7284  d2.loss_mask: 0.2096  d2.loss_dice: 2.1630  d3.loss_cls: 0.7147  d3.loss_mask: 0.2023  d3.loss_dice: 2.0583  d4.loss_cls: 0.6991  d4.loss_mask: 0.1996  d4.loss_dice: 2.0470  d5.loss_cls: 0.6476  d5.loss_mask: 0.1972  d5.loss_dice: 2.0608  d6.loss_cls: 0.6514  d6.loss_mask: 0.1926  d6.loss_dice: 2.0166  d7.loss_cls: 0.6639  d7.loss_mask: 0.1942  d7.loss_dice: 2.0035  d8.loss_cls: 0.6907  d8.loss_mask: 0.1954  d8.loss_dice: 2.0078
05/08 08:55:27 - mmengine - INFO - Iter(train) [13950/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:33:54  time: 1.4638  data_time: 0.0515  memory: 28917  grad_norm: 68.6563  loss: 29.9777  loss_cls: 0.6411  loss_mask: 0.1923  loss_dice: 2.0111  d0.loss_cls: 1.0859  d0.loss_mask: 0.2332  d0.loss_dice: 2.1727  d1.loss_cls: 0.7589  d1.loss_mask: 0.2178  d1.loss_dice: 2.2404  d2.loss_cls: 0.7094  d2.loss_mask: 0.2054  d2.loss_dice: 2.1692  d3.loss_cls: 0.6838  d3.loss_mask: 0.1996  d3.loss_dice: 2.0747  d4.loss_cls: 0.6637  d4.loss_mask: 0.1978  d4.loss_dice: 2.0622  d5.loss_cls: 0.6447  d5.loss_mask: 0.1955  d5.loss_dice: 2.0586  d6.loss_cls: 0.6424  d6.loss_mask: 0.1917  d6.loss_dice: 2.0169  d7.loss_cls: 0.6497  d7.loss_mask: 0.1925  d7.loss_dice: 2.0098  d8.loss_cls: 0.6526  d8.loss_mask: 0.1923  d8.loss_dice: 2.0116
05/08 08:56:39 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 08:56:39 - mmengine - INFO - Iter(train) [14000/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:32:35  time: 1.4423  data_time: 0.0533  memory: 28777  grad_norm: 69.4333  loss: 28.3012  loss_cls: 0.6138  loss_mask: 0.1887  loss_dice: 1.8672  d0.loss_cls: 1.0838  d0.loss_mask: 0.2354  d0.loss_dice: 2.0488  d1.loss_cls: 0.7334  d1.loss_mask: 0.2169  d1.loss_dice: 2.0981  d2.loss_cls: 0.6924  d2.loss_mask: 0.2056  d2.loss_dice: 2.0018  d3.loss_cls: 0.6829  d3.loss_mask: 0.2006  d3.loss_dice: 1.9012  d4.loss_cls: 0.6590  d4.loss_mask: 0.1960  d4.loss_dice: 1.8955  d5.loss_cls: 0.6262  d5.loss_mask: 0.1929  d5.loss_dice: 1.9006  d6.loss_cls: 0.6364  d6.loss_mask: 0.1889  d6.loss_dice: 1.8547  d7.loss_cls: 0.6468  d7.loss_mask: 0.1910  d7.loss_dice: 1.8552  d8.loss_cls: 0.6421  d8.loss_mask: 0.1887  d8.loss_dice: 1.8566
05/08 08:56:39 - mmengine - INFO - Saving checkpoint at 14000 iterations
05/08 08:57:31 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9854  data_time: 0.0400  memory: 3258  
05/08 08:57:54 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.48s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 08:58:03 - mmengine - INFO - start multi processing evaluation ...
DONE (t=53.06s).
Accumulating evaluation results...
DONE (t=0.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.380
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.701
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.340
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.175
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.480
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.800
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.520
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.883
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.516
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.371
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.597
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.883
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.520
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.520
05/08 08:58:56 - mmengine - INFO - segm_mAP_copypaste: 0.380 0.701 0.340 0.175 0.480 0.800
05/08 08:58:57 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.3800  coco/segm_mAP_50: 0.7010  coco/segm_mAP_75: 0.3400  coco/segm_mAP_s: 0.1750  coco/segm_mAP_m: 0.4800  coco/segm_mAP_l: 0.8000  data_time: 0.0397  time: 0.9831
05/08 09:00:08 - mmengine - INFO - Iter(train) [14050/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:31:53  time: 3.1461  data_time: 1.7664  memory: 28958  grad_norm: 65.1961  loss: 29.8935  loss_cls: 0.6579  loss_mask: 0.1990  loss_dice: 1.9590  d0.loss_cls: 1.0978  d0.loss_mask: 0.2409  d0.loss_dice: 2.1787  d1.loss_cls: 0.7505  d1.loss_mask: 0.2281  d1.loss_dice: 2.2231  d2.loss_cls: 0.7298  d2.loss_mask: 0.2181  d2.loss_dice: 2.1198  d3.loss_cls: 0.7389  d3.loss_mask: 0.2081  d3.loss_dice: 1.9931  d4.loss_cls: 0.7214  d4.loss_mask: 0.2096  d4.loss_dice: 1.9925  d5.loss_cls: 0.7245  d5.loss_mask: 0.2063  d5.loss_dice: 1.9764  d6.loss_cls: 0.7039  d6.loss_mask: 0.2009  d6.loss_dice: 1.9403  d7.loss_cls: 0.6982  d7.loss_mask: 0.2011  d7.loss_dice: 1.9458  d8.loss_cls: 0.6880  d8.loss_mask: 0.1991  d8.loss_dice: 1.9429
05/08 09:01:21 - mmengine - INFO - Iter(train) [14100/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:30:35  time: 1.4532  data_time: 0.0553  memory: 28237  grad_norm: 60.5939  loss: 28.8981  loss_cls: 0.7097  loss_mask: 0.1768  loss_dice: 1.8587  d0.loss_cls: 1.0817  d0.loss_mask: 0.2144  d0.loss_dice: 2.1014  d1.loss_cls: 0.7606  d1.loss_mask: 0.2007  d1.loss_dice: 2.1319  d2.loss_cls: 0.7306  d2.loss_mask: 0.1911  d2.loss_dice: 2.0362  d3.loss_cls: 0.7233  d3.loss_mask: 0.1845  d3.loss_dice: 1.9138  d4.loss_cls: 0.7205  d4.loss_mask: 0.1855  d4.loss_dice: 1.8947  d5.loss_cls: 0.7310  d5.loss_mask: 0.1811  d5.loss_dice: 1.8790  d6.loss_cls: 0.7166  d6.loss_mask: 0.1806  d6.loss_dice: 1.8656  d7.loss_cls: 0.7332  d7.loss_mask: 0.1779  d7.loss_dice: 1.8501  d8.loss_cls: 0.7356  d8.loss_mask: 0.1782  d8.loss_dice: 1.8530
05/08 09:02:33 - mmengine - INFO - Iter(train) [14150/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:29:17  time: 1.4379  data_time: 0.0536  memory: 28781  grad_norm: 70.5913  loss: 29.3860  loss_cls: 0.7354  loss_mask: 0.1923  loss_dice: 1.8636  d0.loss_cls: 1.0858  d0.loss_mask: 0.2327  d0.loss_dice: 2.1521  d1.loss_cls: 0.7568  d1.loss_mask: 0.2157  d1.loss_dice: 2.1765  d2.loss_cls: 0.7287  d2.loss_mask: 0.2065  d2.loss_dice: 2.0744  d3.loss_cls: 0.7414  d3.loss_mask: 0.1978  d3.loss_dice: 1.9373  d4.loss_cls: 0.7284  d4.loss_mask: 0.2003  d4.loss_dice: 1.9158  d5.loss_cls: 0.7256  d5.loss_mask: 0.1986  d5.loss_dice: 1.9050  d6.loss_cls: 0.7288  d6.loss_mask: 0.1963  d6.loss_dice: 1.8830  d7.loss_cls: 0.7375  d7.loss_mask: 0.1930  d7.loss_dice: 1.8683  d8.loss_cls: 0.7054  d8.loss_mask: 0.1975  d8.loss_dice: 1.9054
05/08 09:03:47 - mmengine - INFO - Iter(train) [14200/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:27:59  time: 1.4813  data_time: 0.1023  memory: 29112  grad_norm: 66.8005  loss: 28.8996  loss_cls: 0.7293  loss_mask: 0.1924  loss_dice: 1.8213  d0.loss_cls: 1.0924  d0.loss_mask: 0.2387  d0.loss_dice: 2.0973  d1.loss_cls: 0.7568  d1.loss_mask: 0.2205  d1.loss_dice: 2.1340  d2.loss_cls: 0.7172  d2.loss_mask: 0.2075  d2.loss_dice: 2.0339  d3.loss_cls: 0.7156  d3.loss_mask: 0.1990  d3.loss_dice: 1.9254  d4.loss_cls: 0.6960  d4.loss_mask: 0.1992  d4.loss_dice: 1.9033  d5.loss_cls: 0.6808  d5.loss_mask: 0.1975  d5.loss_dice: 1.8920  d6.loss_cls: 0.7161  d6.loss_mask: 0.1955  d6.loss_dice: 1.8503  d7.loss_cls: 0.6573  d7.loss_mask: 0.1942  d7.loss_dice: 1.8827  d8.loss_cls: 0.6841  d8.loss_mask: 0.1955  d8.loss_dice: 1.8740
05/08 09:05:00 - mmengine - INFO - Iter(train) [14250/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:26:41  time: 1.4628  data_time: 0.0675  memory: 28919  grad_norm: 94.3846  loss: 29.2220  loss_cls: 0.7154  loss_mask: 0.1979  loss_dice: 1.8721  d0.loss_cls: 1.0949  d0.loss_mask: 0.2370  d0.loss_dice: 2.1210  d1.loss_cls: 0.7866  d1.loss_mask: 0.2224  d1.loss_dice: 2.1456  d2.loss_cls: 0.7236  d2.loss_mask: 0.2116  d2.loss_dice: 2.0559  d3.loss_cls: 0.7260  d3.loss_mask: 0.2049  d3.loss_dice: 1.9411  d4.loss_cls: 0.7022  d4.loss_mask: 0.2026  d4.loss_dice: 1.9136  d5.loss_cls: 0.6846  d5.loss_mask: 0.2003  d5.loss_dice: 1.9150  d6.loss_cls: 0.7348  d6.loss_mask: 0.1976  d6.loss_dice: 1.8623  d7.loss_cls: 0.6743  d7.loss_mask: 0.1984  d7.loss_dice: 1.8918  d8.loss_cls: 0.7303  d8.loss_mask: 0.1964  d8.loss_dice: 1.8620
05/08 09:06:12 - mmengine - INFO - Iter(train) [14300/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:25:23  time: 1.4293  data_time: 0.0618  memory: 29622  grad_norm: 63.2722  loss: 29.9024  loss_cls: 0.6525  loss_mask: 0.1974  loss_dice: 1.9694  d0.loss_cls: 1.1069  d0.loss_mask: 0.2392  d0.loss_dice: 2.1972  d1.loss_cls: 0.7626  d1.loss_mask: 0.2249  d1.loss_dice: 2.2344  d2.loss_cls: 0.7200  d2.loss_mask: 0.2109  d2.loss_dice: 2.1340  d3.loss_cls: 0.7159  d3.loss_mask: 0.2049  d3.loss_dice: 2.0099  d4.loss_cls: 0.6877  d4.loss_mask: 0.2032  d4.loss_dice: 1.9974  d5.loss_cls: 0.6583  d5.loss_mask: 0.2031  d5.loss_dice: 2.0149  d6.loss_cls: 0.7049  d6.loss_mask: 0.1981  d6.loss_dice: 1.9551  d7.loss_cls: 0.6895  d7.loss_mask: 0.1984  d7.loss_dice: 1.9612  d8.loss_cls: 0.6961  d8.loss_mask: 0.1966  d8.loss_dice: 1.9579
05/08 09:07:24 - mmengine - INFO - Iter(train) [14350/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:24:05  time: 1.4557  data_time: 0.0544  memory: 29328  grad_norm: 77.3389  loss: 30.1139  loss_cls: 0.6631  loss_mask: 0.1971  loss_dice: 1.9788  d0.loss_cls: 1.1030  d0.loss_mask: 0.2377  d0.loss_dice: 2.2144  d1.loss_cls: 0.7711  d1.loss_mask: 0.2256  d1.loss_dice: 2.2453  d2.loss_cls: 0.7272  d2.loss_mask: 0.2110  d2.loss_dice: 2.1363  d3.loss_cls: 0.7162  d3.loss_mask: 0.2045  d3.loss_dice: 2.0386  d4.loss_cls: 0.7135  d4.loss_mask: 0.2032  d4.loss_dice: 2.0269  d5.loss_cls: 0.6581  d5.loss_mask: 0.2011  d5.loss_dice: 2.0521  d6.loss_cls: 0.7014  d6.loss_mask: 0.1986  d6.loss_dice: 1.9724  d7.loss_cls: 0.6605  d7.loss_mask: 0.2008  d7.loss_dice: 1.9993  d8.loss_cls: 0.6378  d8.loss_mask: 0.1996  d8.loss_dice: 2.0186
05/08 09:08:36 - mmengine - INFO - Iter(train) [14400/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:22:46  time: 1.4264  data_time: 0.0592  memory: 30219  grad_norm: 65.1179  loss: 29.3682  loss_cls: 0.6672  loss_mask: 0.1940  loss_dice: 1.9138  d0.loss_cls: 1.0761  d0.loss_mask: 0.2361  d0.loss_dice: 2.1256  d1.loss_cls: 0.7543  d1.loss_mask: 0.2224  d1.loss_dice: 2.1754  d2.loss_cls: 0.7037  d2.loss_mask: 0.2089  d2.loss_dice: 2.0818  d3.loss_cls: 0.6949  d3.loss_mask: 0.2018  d3.loss_dice: 1.9900  d4.loss_cls: 0.6870  d4.loss_mask: 0.2008  d4.loss_dice: 1.9797  d5.loss_cls: 0.6708  d5.loss_mask: 0.1976  d5.loss_dice: 1.9690  d6.loss_cls: 0.7101  d6.loss_mask: 0.1985  d6.loss_dice: 1.9155  d7.loss_cls: 0.6443  d7.loss_mask: 0.1989  d7.loss_dice: 1.9591  d8.loss_cls: 0.6412  d8.loss_mask: 0.1947  d8.loss_dice: 1.9547
05/08 09:09:48 - mmengine - INFO - Iter(train) [14450/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:21:28  time: 1.4394  data_time: 0.0613  memory: 30196  grad_norm: 76.6510  loss: 31.1702  loss_cls: 0.6764  loss_mask: 0.2015  loss_dice: 2.0664  d0.loss_cls: 1.1145  d0.loss_mask: 0.2432  d0.loss_dice: 2.2963  d1.loss_cls: 0.7651  d1.loss_mask: 0.2333  d1.loss_dice: 2.3445  d2.loss_cls: 0.7193  d2.loss_mask: 0.2156  d2.loss_dice: 2.2567  d3.loss_cls: 0.7116  d3.loss_mask: 0.2093  d3.loss_dice: 2.1411  d4.loss_cls: 0.7145  d4.loss_mask: 0.2090  d4.loss_dice: 2.1255  d5.loss_cls: 0.6926  d5.loss_mask: 0.2041  d5.loss_dice: 2.1082  d6.loss_cls: 0.7368  d6.loss_mask: 0.2042  d6.loss_dice: 2.0501  d7.loss_cls: 0.6600  d7.loss_mask: 0.2043  d7.loss_dice: 2.1077  d8.loss_cls: 0.6679  d8.loss_mask: 0.2000  d8.loss_dice: 2.0904
05/08 09:11:00 - mmengine - INFO - Iter(train) [14500/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:20:10  time: 1.4356  data_time: 0.0544  memory: 28599  grad_norm: 95.0927  loss: 29.8156  loss_cls: 0.7025  loss_mask: 0.1952  loss_dice: 1.9325  d0.loss_cls: 1.0850  d0.loss_mask: 0.2307  d0.loss_dice: 2.1599  d1.loss_cls: 0.7697  d1.loss_mask: 0.2238  d1.loss_dice: 2.1785  d2.loss_cls: 0.7211  d2.loss_mask: 0.2079  d2.loss_dice: 2.1078  d3.loss_cls: 0.7327  d3.loss_mask: 0.1966  d3.loss_dice: 2.0024  d4.loss_cls: 0.7400  d4.loss_mask: 0.1992  d4.loss_dice: 1.9939  d5.loss_cls: 0.6999  d5.loss_mask: 0.1937  d5.loss_dice: 1.9824  d6.loss_cls: 0.7589  d6.loss_mask: 0.1945  d6.loss_dice: 1.9154  d7.loss_cls: 0.6980  d7.loss_mask: 0.1965  d7.loss_dice: 1.9544  d8.loss_cls: 0.7140  d8.loss_mask: 0.1934  d8.loss_dice: 1.9351
05/08 09:12:12 - mmengine - INFO - Iter(train) [14550/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:18:52  time: 1.4489  data_time: 0.0547  memory: 28776  grad_norm: 82.6307  loss: 31.0721  loss_cls: 0.7087  loss_mask: 0.2035  loss_dice: 2.0337  d0.loss_cls: 1.1113  d0.loss_mask: 0.2438  d0.loss_dice: 2.2790  d1.loss_cls: 0.7879  d1.loss_mask: 0.2343  d1.loss_dice: 2.2959  d2.loss_cls: 0.7334  d2.loss_mask: 0.2182  d2.loss_dice: 2.2093  d3.loss_cls: 0.7306  d3.loss_mask: 0.2143  d3.loss_dice: 2.0916  d4.loss_cls: 0.7480  d4.loss_mask: 0.2158  d4.loss_dice: 2.0726  d5.loss_cls: 0.7424  d5.loss_mask: 0.2059  d5.loss_dice: 2.0544  d6.loss_cls: 0.7562  d6.loss_mask: 0.2083  d6.loss_dice: 1.9967  d7.loss_cls: 0.7550  d7.loss_mask: 0.2088  d7.loss_dice: 2.0198  d8.loss_cls: 0.7624  d8.loss_mask: 0.2067  d8.loss_dice: 2.0233
05/08 09:13:26 - mmengine - INFO - Iter(train) [14600/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:17:34  time: 1.4706  data_time: 0.1059  memory: 28900  grad_norm: 61.4723  loss: 29.5993  loss_cls: 0.6849  loss_mask: 0.1912  loss_dice: 1.9183  d0.loss_cls: 1.0983  d0.loss_mask: 0.2318  d0.loss_dice: 2.1582  d1.loss_cls: 0.7613  d1.loss_mask: 0.2202  d1.loss_dice: 2.2040  d2.loss_cls: 0.7109  d2.loss_mask: 0.2094  d2.loss_dice: 2.1124  d3.loss_cls: 0.7083  d3.loss_mask: 0.2050  d3.loss_dice: 2.0052  d4.loss_cls: 0.7155  d4.loss_mask: 0.2021  d4.loss_dice: 1.9862  d5.loss_cls: 0.6949  d5.loss_mask: 0.1977  d5.loss_dice: 1.9811  d6.loss_cls: 0.6827  d6.loss_mask: 0.1983  d6.loss_dice: 1.9179  d7.loss_cls: 0.6880  d7.loss_mask: 0.1950  d7.loss_dice: 1.9203  d8.loss_cls: 0.6894  d8.loss_mask: 0.1935  d8.loss_dice: 1.9169
05/08 09:14:39 - mmengine - INFO - Iter(train) [14650/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:16:17  time: 1.4696  data_time: 0.0554  memory: 30467  grad_norm: 72.0219  loss: 29.2158  loss_cls: 0.6978  loss_mask: 0.1935  loss_dice: 1.8690  d0.loss_cls: 1.1022  d0.loss_mask: 0.2379  d0.loss_dice: 2.1222  d1.loss_cls: 0.7698  d1.loss_mask: 0.2204  d1.loss_dice: 2.1685  d2.loss_cls: 0.7167  d2.loss_mask: 0.2123  d2.loss_dice: 2.0750  d3.loss_cls: 0.7101  d3.loss_mask: 0.2060  d3.loss_dice: 1.9600  d4.loss_cls: 0.6953  d4.loss_mask: 0.2061  d4.loss_dice: 1.9525  d5.loss_cls: 0.6782  d5.loss_mask: 0.2002  d5.loss_dice: 1.9516  d6.loss_cls: 0.6823  d6.loss_mask: 0.1945  d6.loss_dice: 1.8797  d7.loss_cls: 0.6680  d7.loss_mask: 0.1967  d7.loss_dice: 1.8859  d8.loss_cls: 0.7002  d8.loss_mask: 0.1943  d8.loss_dice: 1.8692
05/08 09:15:51 - mmengine - INFO - Iter(train) [14700/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:14:59  time: 1.4398  data_time: 0.0590  memory: 28465  grad_norm: 64.6303  loss: 30.2858  loss_cls: 0.7372  loss_mask: 0.1965  loss_dice: 1.9396  d0.loss_cls: 1.1024  d0.loss_mask: 0.2356  d0.loss_dice: 2.2271  d1.loss_cls: 0.7757  d1.loss_mask: 0.2279  d1.loss_dice: 2.2581  d2.loss_cls: 0.7251  d2.loss_mask: 0.2136  d2.loss_dice: 2.1679  d3.loss_cls: 0.7220  d3.loss_mask: 0.2047  d3.loss_dice: 2.0457  d4.loss_cls: 0.6934  d4.loss_mask: 0.2059  d4.loss_dice: 2.0427  d5.loss_cls: 0.7005  d5.loss_mask: 0.2019  d5.loss_dice: 2.0331  d6.loss_cls: 0.7270  d6.loss_mask: 0.1963  d6.loss_dice: 1.9432  d7.loss_cls: 0.7167  d7.loss_mask: 0.1995  d7.loss_dice: 1.9490  d8.loss_cls: 0.7571  d8.loss_mask: 0.1975  d8.loss_dice: 1.9433
05/08 09:17:02 - mmengine - INFO - Iter(train) [14750/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:13:40  time: 1.4152  data_time: 0.0535  memory: 28329  grad_norm: 60.1369  loss: 28.5418  loss_cls: 0.7366  loss_mask: 0.1985  loss_dice: 1.7936  d0.loss_cls: 1.0802  d0.loss_mask: 0.2379  d0.loss_dice: 2.0585  d1.loss_cls: 0.7551  d1.loss_mask: 0.2242  d1.loss_dice: 2.0792  d2.loss_cls: 0.7023  d2.loss_mask: 0.2139  d2.loss_dice: 1.9999  d3.loss_cls: 0.6964  d3.loss_mask: 0.2072  d3.loss_dice: 1.8878  d4.loss_cls: 0.6754  d4.loss_mask: 0.2054  d4.loss_dice: 1.8850  d5.loss_cls: 0.7003  d5.loss_mask: 0.2028  d5.loss_dice: 1.8567  d6.loss_cls: 0.7320  d6.loss_mask: 0.1983  d6.loss_dice: 1.7724  d7.loss_cls: 0.6993  d7.loss_mask: 0.2017  d7.loss_dice: 1.8149  d8.loss_cls: 0.7385  d8.loss_mask: 0.2006  d8.loss_dice: 1.7872
05/08 09:18:15 - mmengine - INFO - Iter(train) [14800/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:12:23  time: 1.4640  data_time: 0.0545  memory: 29182  grad_norm: 63.6950  loss: 31.1998  loss_cls: 0.7445  loss_mask: 0.1994  loss_dice: 2.0324  d0.loss_cls: 1.1047  d0.loss_mask: 0.2330  d0.loss_dice: 2.2894  d1.loss_cls: 0.7935  d1.loss_mask: 0.2285  d1.loss_dice: 2.3169  d2.loss_cls: 0.7388  d2.loss_mask: 0.2146  d2.loss_dice: 2.2431  d3.loss_cls: 0.7302  d3.loss_mask: 0.2074  d3.loss_dice: 2.1356  d4.loss_cls: 0.7073  d4.loss_mask: 0.2075  d4.loss_dice: 2.1168  d5.loss_cls: 0.7521  d5.loss_mask: 0.2019  d5.loss_dice: 2.0821  d6.loss_cls: 0.7616  d6.loss_mask: 0.2000  d6.loss_dice: 2.0116  d7.loss_cls: 0.7319  d7.loss_mask: 0.2014  d7.loss_dice: 2.0435  d8.loss_cls: 0.7056  d8.loss_mask: 0.2037  d8.loss_dice: 2.0608
05/08 09:19:27 - mmengine - INFO - Iter(train) [14850/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:11:05  time: 1.4401  data_time: 0.0584  memory: 29725  grad_norm: 63.8008  loss: 29.4611  loss_cls: 0.7366  loss_mask: 0.1911  loss_dice: 1.8708  d0.loss_cls: 1.0852  d0.loss_mask: 0.2256  d0.loss_dice: 2.1497  d1.loss_cls: 0.7727  d1.loss_mask: 0.2165  d1.loss_dice: 2.1787  d2.loss_cls: 0.7381  d2.loss_mask: 0.2075  d2.loss_dice: 2.0676  d3.loss_cls: 0.7521  d3.loss_mask: 0.1993  d3.loss_dice: 1.9505  d4.loss_cls: 0.7193  d4.loss_mask: 0.1998  d4.loss_dice: 1.9294  d5.loss_cls: 0.7598  d5.loss_mask: 0.1931  d5.loss_dice: 1.9272  d6.loss_cls: 0.7194  d6.loss_mask: 0.1930  d6.loss_dice: 1.8998  d7.loss_cls: 0.7201  d7.loss_mask: 0.1900  d7.loss_dice: 1.8866  d8.loss_cls: 0.7038  d8.loss_mask: 0.1920  d8.loss_dice: 1.8860
05/08 09:20:40 - mmengine - INFO - Iter(train) [14900/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:09:47  time: 1.4555  data_time: 0.0631  memory: 28845  grad_norm: 57.0400  loss: 30.4413  loss_cls: 0.6644  loss_mask: 0.2048  loss_dice: 2.0151  d0.loss_cls: 1.0874  d0.loss_mask: 0.2353  d0.loss_dice: 2.2531  d1.loss_cls: 0.7788  d1.loss_mask: 0.2210  d1.loss_dice: 2.2793  d2.loss_cls: 0.7334  d2.loss_mask: 0.2136  d2.loss_dice: 2.1707  d3.loss_cls: 0.7516  d3.loss_mask: 0.2053  d3.loss_dice: 2.0436  d4.loss_cls: 0.7148  d4.loss_mask: 0.2043  d4.loss_dice: 2.0166  d5.loss_cls: 0.7450  d5.loss_mask: 0.2017  d5.loss_dice: 2.0171  d6.loss_cls: 0.7131  d6.loss_mask: 0.1990  d6.loss_dice: 1.9890  d7.loss_cls: 0.7480  d7.loss_mask: 0.1939  d7.loss_dice: 1.9547  d8.loss_cls: 0.7393  d8.loss_mask: 0.1960  d8.loss_dice: 1.9515
05/08 09:21:52 - mmengine - INFO - Iter(train) [14950/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:08:29  time: 1.4370  data_time: 0.0623  memory: 29477  grad_norm: 61.6288  loss: 29.8728  loss_cls: 0.6825  loss_mask: 0.1957  loss_dice: 1.9290  d0.loss_cls: 1.0997  d0.loss_mask: 0.2278  d0.loss_dice: 2.2182  d1.loss_cls: 0.7789  d1.loss_mask: 0.2164  d1.loss_dice: 2.2192  d2.loss_cls: 0.7525  d2.loss_mask: 0.2095  d2.loss_dice: 2.0973  d3.loss_cls: 0.7642  d3.loss_mask: 0.2046  d3.loss_dice: 1.9723  d4.loss_cls: 0.7580  d4.loss_mask: 0.2003  d4.loss_dice: 1.9282  d5.loss_cls: 0.7591  d5.loss_mask: 0.1940  d5.loss_dice: 1.9347  d6.loss_cls: 0.7593  d6.loss_mask: 0.1940  d6.loss_dice: 1.8997  d7.loss_cls: 0.7690  d7.loss_mask: 0.1892  d7.loss_dice: 1.8798  d8.loss_cls: 0.7333  d8.loss_mask: 0.1938  d8.loss_dice: 1.9126
05/08 09:23:05 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 09:23:05 - mmengine - INFO - Iter(train) [15000/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:07:12  time: 1.4709  data_time: 0.1130  memory: 29636  grad_norm: 64.5174  loss: 29.3299  loss_cls: 0.7766  loss_mask: 0.1923  loss_dice: 1.8147  d0.loss_cls: 1.0953  d0.loss_mask: 0.2291  d0.loss_dice: 2.1594  d1.loss_cls: 0.7757  d1.loss_mask: 0.2151  d1.loss_dice: 2.1671  d2.loss_cls: 0.7464  d2.loss_mask: 0.2069  d2.loss_dice: 2.0445  d3.loss_cls: 0.7543  d3.loss_mask: 0.2012  d3.loss_dice: 1.8910  d4.loss_cls: 0.7393  d4.loss_mask: 0.2016  d4.loss_dice: 1.8767  d5.loss_cls: 0.7489  d5.loss_mask: 0.1977  d5.loss_dice: 1.8873  d6.loss_cls: 0.7512  d6.loss_mask: 0.1961  d6.loss_dice: 1.8286  d7.loss_cls: 0.7771  d7.loss_mask: 0.1947  d7.loss_dice: 1.8483  d8.loss_cls: 0.7826  d8.loss_mask: 0.1951  d8.loss_dice: 1.8351
05/08 09:23:05 - mmengine - INFO - Saving checkpoint at 15000 iterations
05/08 09:23:57 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9693  data_time: 0.0276  memory: 3258  
05/08 09:24:20 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.30s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 09:24:28 - mmengine - INFO - start multi processing evaluation ...
DONE (t=52.53s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.280
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.541
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.178
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.346
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.597
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.520
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.859
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.523
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.375
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.589
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.933
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.520
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.520
05/08 09:25:20 - mmengine - INFO - segm_mAP_copypaste: 0.280 0.541 0.221 0.178 0.346 0.597
05/08 09:25:21 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.2800  coco/segm_mAP_50: 0.5410  coco/segm_mAP_75: 0.2210  coco/segm_mAP_s: 0.1780  coco/segm_mAP_m: 0.3460  coco/segm_mAP_l: 0.5970  data_time: 0.0275  time: 0.9674
05/08 09:26:33 - mmengine - INFO - Iter(train) [15050/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:06:22  time: 3.1336  data_time: 1.7542  memory: 29189  grad_norm: 76.1191  loss: 30.4664  loss_cls: 0.7888  loss_mask: 0.1934  loss_dice: 1.9133  d0.loss_cls: 1.0964  d0.loss_mask: 0.2305  d0.loss_dice: 2.2452  d1.loss_cls: 0.7897  d1.loss_mask: 0.2212  d1.loss_dice: 2.2523  d2.loss_cls: 0.7818  d2.loss_mask: 0.2106  d2.loss_dice: 2.1335  d3.loss_cls: 0.7745  d3.loss_mask: 0.2030  d3.loss_dice: 1.9886  d4.loss_cls: 0.7637  d4.loss_mask: 0.2020  d4.loss_dice: 1.9802  d5.loss_cls: 0.7612  d5.loss_mask: 0.1995  d5.loss_dice: 1.9958  d6.loss_cls: 0.7867  d6.loss_mask: 0.1981  d6.loss_dice: 1.9243  d7.loss_cls: 0.7649  d7.loss_mask: 0.2007  d7.loss_dice: 1.9453  d8.loss_cls: 0.7835  d8.loss_mask: 0.1981  d8.loss_dice: 1.9397
05/08 09:27:45 - mmengine - INFO - Iter(train) [15100/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:05:04  time: 1.4339  data_time: 0.0525  memory: 28458  grad_norm: 62.7208  loss: 28.5729  loss_cls: 0.6915  loss_mask: 0.1855  loss_dice: 1.8130  d0.loss_cls: 1.0674  d0.loss_mask: 0.2241  d0.loss_dice: 2.0743  d1.loss_cls: 0.7422  d1.loss_mask: 0.2098  d1.loss_dice: 2.1160  d2.loss_cls: 0.7229  d2.loss_mask: 0.1994  d2.loss_dice: 2.0041  d3.loss_cls: 0.7240  d3.loss_mask: 0.1920  d3.loss_dice: 1.8716  d4.loss_cls: 0.7209  d4.loss_mask: 0.1920  d4.loss_dice: 1.8573  d5.loss_cls: 0.7167  d5.loss_mask: 0.1919  d5.loss_dice: 1.8723  d6.loss_cls: 0.7439  d6.loss_mask: 0.1882  d6.loss_dice: 1.8033  d7.loss_cls: 0.6932  d7.loss_mask: 0.1889  d7.loss_dice: 1.8361  d8.loss_cls: 0.7003  d8.loss_mask: 0.1896  d8.loss_dice: 1.8405
05/08 09:28:57 - mmengine - INFO - Iter(train) [15150/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:03:46  time: 1.4392  data_time: 0.0575  memory: 29226  grad_norm: 64.1592  loss: 30.2825  loss_cls: 0.7355  loss_mask: 0.1951  loss_dice: 1.9371  d0.loss_cls: 1.0942  d0.loss_mask: 0.2341  d0.loss_dice: 2.2107  d1.loss_cls: 0.7661  d1.loss_mask: 0.2251  d1.loss_dice: 2.2551  d2.loss_cls: 0.7508  d2.loss_mask: 0.2148  d2.loss_dice: 2.1496  d3.loss_cls: 0.7431  d3.loss_mask: 0.2074  d3.loss_dice: 2.0056  d4.loss_cls: 0.7219  d4.loss_mask: 0.2049  d4.loss_dice: 2.0155  d5.loss_cls: 0.7209  d5.loss_mask: 0.2014  d5.loss_dice: 2.0094  d6.loss_cls: 0.7214  d6.loss_mask: 0.2005  d6.loss_dice: 1.9432  d7.loss_cls: 0.7279  d7.loss_mask: 0.2003  d7.loss_dice: 1.9542  d8.loss_cls: 0.7844  d8.loss_mask: 0.1979  d8.loss_dice: 1.9542
05/08 09:30:08 - mmengine - INFO - Iter(train) [15200/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:02:28  time: 1.4183  data_time: 0.0612  memory: 28264  grad_norm: 90.1721  loss: 29.2401  loss_cls: 0.6529  loss_mask: 0.2038  loss_dice: 1.8978  d0.loss_cls: 1.0624  d0.loss_mask: 0.2410  d0.loss_dice: 2.1272  d1.loss_cls: 0.7520  d1.loss_mask: 0.2293  d1.loss_dice: 2.1621  d2.loss_cls: 0.7324  d2.loss_mask: 0.2148  d2.loss_dice: 2.0587  d3.loss_cls: 0.7177  d3.loss_mask: 0.2114  d3.loss_dice: 1.9357  d4.loss_cls: 0.7147  d4.loss_mask: 0.2109  d4.loss_dice: 1.9336  d5.loss_cls: 0.7171  d5.loss_mask: 0.2063  d5.loss_dice: 1.9127  d6.loss_cls: 0.6456  d6.loss_mask: 0.2092  d6.loss_dice: 1.9110  d7.loss_cls: 0.6745  d7.loss_mask: 0.2036  d7.loss_dice: 1.9058  d8.loss_cls: 0.7000  d8.loss_mask: 0.2046  d8.loss_dice: 1.8912
05/08 09:31:19 - mmengine - INFO - Iter(train) [15250/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 2:01:09  time: 1.4250  data_time: 0.0546  memory: 29302  grad_norm: 101.1633  loss: 30.6522  loss_cls: 0.6692  loss_mask: 0.1959  loss_dice: 2.0268  d0.loss_cls: 1.1110  d0.loss_mask: 0.2378  d0.loss_dice: 2.2604  d1.loss_cls: 0.7807  d1.loss_mask: 0.2234  d1.loss_dice: 2.3070  d2.loss_cls: 0.7327  d2.loss_mask: 0.2107  d2.loss_dice: 2.2121  d3.loss_cls: 0.7082  d3.loss_mask: 0.2042  d3.loss_dice: 2.0982  d4.loss_cls: 0.6828  d4.loss_mask: 0.2037  d4.loss_dice: 2.0940  d5.loss_cls: 0.6937  d5.loss_mask: 0.2035  d5.loss_dice: 2.0757  d6.loss_cls: 0.6439  d6.loss_mask: 0.2004  d6.loss_dice: 2.0516  d7.loss_cls: 0.6579  d7.loss_mask: 0.2003  d7.loss_dice: 2.0539  d8.loss_cls: 0.6539  d8.loss_mask: 0.1984  d8.loss_dice: 2.0600
05/08 09:32:31 - mmengine - INFO - Iter(train) [15300/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:59:52  time: 1.4406  data_time: 0.0532  memory: 28876  grad_norm: 71.9904  loss: 29.6750  loss_cls: 0.6820  loss_mask: 0.1954  loss_dice: 1.9296  d0.loss_cls: 1.0917  d0.loss_mask: 0.2363  d0.loss_dice: 2.1703  d1.loss_cls: 0.7658  d1.loss_mask: 0.2262  d1.loss_dice: 2.2161  d2.loss_cls: 0.7382  d2.loss_mask: 0.2115  d2.loss_dice: 2.1172  d3.loss_cls: 0.6973  d3.loss_mask: 0.2051  d3.loss_dice: 1.9984  d4.loss_cls: 0.6653  d4.loss_mask: 0.2037  d4.loss_dice: 2.0038  d5.loss_cls: 0.6894  d5.loss_mask: 0.2015  d5.loss_dice: 1.9787  d6.loss_cls: 0.6670  d6.loss_mask: 0.1984  d6.loss_dice: 1.9517  d7.loss_cls: 0.6821  d7.loss_mask: 0.1967  d7.loss_dice: 1.9406  d8.loss_cls: 0.6869  d8.loss_mask: 0.1958  d8.loss_dice: 1.9324
05/08 09:33:44 - mmengine - INFO - Iter(train) [15350/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:58:34  time: 1.4565  data_time: 0.0537  memory: 29461  grad_norm: 71.0732  loss: 28.9746  loss_cls: 0.6924  loss_mask: 0.1899  loss_dice: 1.8574  d0.loss_cls: 1.0858  d0.loss_mask: 0.2258  d0.loss_dice: 2.1184  d1.loss_cls: 0.7627  d1.loss_mask: 0.2181  d1.loss_dice: 2.1321  d2.loss_cls: 0.7535  d2.loss_mask: 0.2073  d2.loss_dice: 2.0209  d3.loss_cls: 0.7325  d3.loss_mask: 0.1983  d3.loss_dice: 1.9114  d4.loss_cls: 0.7105  d4.loss_mask: 0.1965  d4.loss_dice: 1.8992  d5.loss_cls: 0.7165  d5.loss_mask: 0.1947  d5.loss_dice: 1.8958  d6.loss_cls: 0.7179  d6.loss_mask: 0.1896  d6.loss_dice: 1.8412  d7.loss_cls: 0.7176  d7.loss_mask: 0.1902  d7.loss_dice: 1.8446  d8.loss_cls: 0.7300  d8.loss_mask: 0.1879  d8.loss_dice: 1.8358
05/08 09:34:57 - mmengine - INFO - Iter(train) [15400/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:57:17  time: 1.4683  data_time: 0.1006  memory: 29729  grad_norm: 84.9356  loss: 30.5292  loss_cls: 0.6828  loss_mask: 0.2015  loss_dice: 1.9993  d0.loss_cls: 1.1122  d0.loss_mask: 0.2405  d0.loss_dice: 2.2491  d1.loss_cls: 0.7776  d1.loss_mask: 0.2306  d1.loss_dice: 2.2629  d2.loss_cls: 0.7528  d2.loss_mask: 0.2153  d2.loss_dice: 2.1495  d3.loss_cls: 0.7369  d3.loss_mask: 0.2087  d3.loss_dice: 2.0459  d4.loss_cls: 0.7180  d4.loss_mask: 0.2049  d4.loss_dice: 2.0455  d5.loss_cls: 0.7195  d5.loss_mask: 0.2047  d5.loss_dice: 2.0230  d6.loss_cls: 0.7479  d6.loss_mask: 0.2019  d6.loss_dice: 1.9722  d7.loss_cls: 0.7479  d7.loss_mask: 0.2019  d7.loss_dice: 1.9715  d8.loss_cls: 0.7395  d8.loss_mask: 0.1989  d8.loss_dice: 1.9665
05/08 09:36:10 - mmengine - INFO - Iter(train) [15450/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:55:59  time: 1.4564  data_time: 0.0521  memory: 29645  grad_norm: 80.8341  loss: 30.8801  loss_cls: 0.6940  loss_mask: 0.1955  loss_dice: 2.0311  d0.loss_cls: 1.1148  d0.loss_mask: 0.2321  d0.loss_dice: 2.2848  d1.loss_cls: 0.7686  d1.loss_mask: 0.2219  d1.loss_dice: 2.3268  d2.loss_cls: 0.7518  d2.loss_mask: 0.2083  d2.loss_dice: 2.2021  d3.loss_cls: 0.7242  d3.loss_mask: 0.2024  d3.loss_dice: 2.0977  d4.loss_cls: 0.7068  d4.loss_mask: 0.1984  d4.loss_dice: 2.1021  d5.loss_cls: 0.7056  d5.loss_mask: 0.2010  d5.loss_dice: 2.0905  d6.loss_cls: 0.7430  d6.loss_mask: 0.1980  d6.loss_dice: 2.0182  d7.loss_cls: 0.7145  d7.loss_mask: 0.1976  d7.loss_dice: 2.0339  d8.loss_cls: 0.7040  d8.loss_mask: 0.1959  d8.loss_dice: 2.0144
05/08 09:37:23 - mmengine - INFO - Iter(train) [15500/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:54:41  time: 1.4531  data_time: 0.0601  memory: 28691  grad_norm: 101.4567  loss: 29.3144  loss_cls: 0.7377  loss_mask: 0.1962  loss_dice: 1.8623  d0.loss_cls: 1.0880  d0.loss_mask: 0.2343  d0.loss_dice: 2.1292  d1.loss_cls: 0.7637  d1.loss_mask: 0.2223  d1.loss_dice: 2.1492  d2.loss_cls: 0.7518  d2.loss_mask: 0.2135  d2.loss_dice: 2.0492  d3.loss_cls: 0.7395  d3.loss_mask: 0.2064  d3.loss_dice: 1.9363  d4.loss_cls: 0.7318  d4.loss_mask: 0.2017  d4.loss_dice: 1.9252  d5.loss_cls: 0.7133  d5.loss_mask: 0.1972  d5.loss_dice: 1.9199  d6.loss_cls: 0.7374  d6.loss_mask: 0.1972  d6.loss_dice: 1.8560  d7.loss_cls: 0.7201  d7.loss_mask: 0.1971  d7.loss_dice: 1.8690  d8.loss_cls: 0.7354  d8.loss_mask: 0.1942  d8.loss_dice: 1.8393
05/08 09:38:34 - mmengine - INFO - Iter(train) [15550/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:53:23  time: 1.4209  data_time: 0.0509  memory: 27979  grad_norm: 67.2764  loss: 28.3620  loss_cls: 0.6614  loss_mask: 0.1906  loss_dice: 1.8033  d0.loss_cls: 1.0697  d0.loss_mask: 0.2278  d0.loss_dice: 2.0829  d1.loss_cls: 0.7509  d1.loss_mask: 0.2188  d1.loss_dice: 2.0727  d2.loss_cls: 0.7571  d2.loss_mask: 0.2078  d2.loss_dice: 1.9556  d3.loss_cls: 0.7338  d3.loss_mask: 0.2009  d3.loss_dice: 1.8655  d4.loss_cls: 0.7181  d4.loss_mask: 0.1996  d4.loss_dice: 1.8485  d5.loss_cls: 0.7213  d5.loss_mask: 0.1918  d5.loss_dice: 1.8434  d6.loss_cls: 0.7091  d6.loss_mask: 0.1919  d6.loss_dice: 1.7837  d7.loss_cls: 0.7056  d7.loss_mask: 0.1908  d7.loss_dice: 1.7817  d8.loss_cls: 0.6933  d8.loss_mask: 0.1925  d8.loss_dice: 1.7920
05/08 09:39:46 - mmengine - INFO - Iter(train) [15600/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:52:06  time: 1.4466  data_time: 0.0567  memory: 28752  grad_norm: 84.6830  loss: 29.4051  loss_cls: 0.7324  loss_mask: 0.1922  loss_dice: 1.8663  d0.loss_cls: 1.0949  d0.loss_mask: 0.2342  d0.loss_dice: 2.1541  d1.loss_cls: 0.7791  d1.loss_mask: 0.2194  d1.loss_dice: 2.1662  d2.loss_cls: 0.7621  d2.loss_mask: 0.2096  d2.loss_dice: 2.0434  d3.loss_cls: 0.7412  d3.loss_mask: 0.2030  d3.loss_dice: 1.9504  d4.loss_cls: 0.7149  d4.loss_mask: 0.2001  d4.loss_dice: 1.9357  d5.loss_cls: 0.7246  d5.loss_mask: 0.1963  d5.loss_dice: 1.9211  d6.loss_cls: 0.7339  d6.loss_mask: 0.1931  d6.loss_dice: 1.8483  d7.loss_cls: 0.6658  d7.loss_mask: 0.2006  d7.loss_dice: 1.9325  d8.loss_cls: 0.6859  d8.loss_mask: 0.1990  d8.loss_dice: 1.9049
05/08 09:40:59 - mmengine - INFO - Iter(train) [15650/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:50:48  time: 1.4470  data_time: 0.0538  memory: 28854  grad_norm: 91.2617  loss: 28.1425  loss_cls: 0.7228  loss_mask: 0.1808  loss_dice: 1.7716  d0.loss_cls: 1.0731  d0.loss_mask: 0.2225  d0.loss_dice: 2.0413  d1.loss_cls: 0.7574  d1.loss_mask: 0.2082  d1.loss_dice: 2.0597  d2.loss_cls: 0.7448  d2.loss_mask: 0.1978  d2.loss_dice: 1.9488  d3.loss_cls: 0.7338  d3.loss_mask: 0.1925  d3.loss_dice: 1.8528  d4.loss_cls: 0.7047  d4.loss_mask: 0.1892  d4.loss_dice: 1.8437  d5.loss_cls: 0.6955  d5.loss_mask: 0.1873  d5.loss_dice: 1.8223  d6.loss_cls: 0.7306  d6.loss_mask: 0.1824  d6.loss_dice: 1.7466  d7.loss_cls: 0.6713  d7.loss_mask: 0.1846  d7.loss_dice: 1.8108  d8.loss_cls: 0.7048  d8.loss_mask: 0.1819  d8.loss_dice: 1.7787
05/08 09:42:10 - mmengine - INFO - Iter(train) [15700/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:49:31  time: 1.4394  data_time: 0.0575  memory: 29814  grad_norm: 67.4192  loss: 29.8498  loss_cls: 0.7250  loss_mask: 0.2073  loss_dice: 1.8868  d0.loss_cls: 1.1090  d0.loss_mask: 0.2460  d0.loss_dice: 2.1720  d1.loss_cls: 0.7802  d1.loss_mask: 0.2378  d1.loss_dice: 2.1921  d2.loss_cls: 0.7679  d2.loss_mask: 0.2236  d2.loss_dice: 2.0769  d3.loss_cls: 0.7573  d3.loss_mask: 0.2191  d3.loss_dice: 1.9727  d4.loss_cls: 0.7505  d4.loss_mask: 0.2131  d4.loss_dice: 1.9437  d5.loss_cls: 0.7437  d5.loss_mask: 0.2115  d5.loss_dice: 1.9259  d6.loss_cls: 0.7119  d6.loss_mask: 0.2110  d6.loss_dice: 1.8886  d7.loss_cls: 0.7302  d7.loss_mask: 0.2070  d7.loss_dice: 1.8992  d8.loss_cls: 0.7404  d8.loss_mask: 0.2077  d8.loss_dice: 1.8917
05/08 09:43:22 - mmengine - INFO - Iter(train) [15750/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:48:13  time: 1.4339  data_time: 0.0554  memory: 28305  grad_norm: 67.7170  loss: 29.9165  loss_cls: 0.7180  loss_mask: 0.1978  loss_dice: 1.9252  d0.loss_cls: 1.0961  d0.loss_mask: 0.2383  d0.loss_dice: 2.1799  d1.loss_cls: 0.7654  d1.loss_mask: 0.2280  d1.loss_dice: 2.2203  d2.loss_cls: 0.7468  d2.loss_mask: 0.2122  d2.loss_dice: 2.1112  d3.loss_cls: 0.7396  d3.loss_mask: 0.2084  d3.loss_dice: 2.0006  d4.loss_cls: 0.7337  d4.loss_mask: 0.2065  d4.loss_dice: 1.9659  d5.loss_cls: 0.7277  d5.loss_mask: 0.2022  d5.loss_dice: 1.9578  d6.loss_cls: 0.6743  d6.loss_mask: 0.2017  d6.loss_dice: 1.9479  d7.loss_cls: 0.7218  d7.loss_mask: 0.2007  d7.loss_dice: 1.9430  d8.loss_cls: 0.7016  d8.loss_mask: 0.2015  d8.loss_dice: 1.9426
05/08 09:44:36 - mmengine - INFO - Iter(train) [15800/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:46:56  time: 1.4679  data_time: 0.1009  memory: 28709  grad_norm: 71.7868  loss: 30.9794  loss_cls: 0.6901  loss_mask: 0.2050  loss_dice: 2.0238  d0.loss_cls: 1.1053  d0.loss_mask: 0.2448  d0.loss_dice: 2.2880  d1.loss_cls: 0.7886  d1.loss_mask: 0.2347  d1.loss_dice: 2.3070  d2.loss_cls: 0.7776  d2.loss_mask: 0.2187  d2.loss_dice: 2.1908  d3.loss_cls: 0.7514  d3.loss_mask: 0.2117  d3.loss_dice: 2.0889  d4.loss_cls: 0.7185  d4.loss_mask: 0.2121  d4.loss_dice: 2.0615  d5.loss_cls: 0.7242  d5.loss_mask: 0.2080  d5.loss_dice: 2.0560  d6.loss_cls: 0.7386  d6.loss_mask: 0.2035  d6.loss_dice: 2.0216  d7.loss_cls: 0.7259  d7.loss_mask: 0.2018  d7.loss_dice: 2.0293  d8.loss_cls: 0.7269  d8.loss_mask: 0.2066  d8.loss_dice: 2.0186
05/08 09:45:47 - mmengine - INFO - Iter(train) [15850/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:45:38  time: 1.4298  data_time: 0.0559  memory: 29012  grad_norm: 79.3372  loss: 28.5986  loss_cls: 0.6511  loss_mask: 0.1975  loss_dice: 1.8384  d0.loss_cls: 1.0848  d0.loss_mask: 0.2335  d0.loss_dice: 2.0889  d1.loss_cls: 0.7577  d1.loss_mask: 0.2244  d1.loss_dice: 2.0955  d2.loss_cls: 0.7496  d2.loss_mask: 0.2163  d2.loss_dice: 1.9742  d3.loss_cls: 0.7315  d3.loss_mask: 0.2078  d3.loss_dice: 1.8840  d4.loss_cls: 0.6858  d4.loss_mask: 0.2052  d4.loss_dice: 1.8715  d5.loss_cls: 0.7055  d5.loss_mask: 0.1976  d5.loss_dice: 1.8541  d6.loss_cls: 0.7126  d6.loss_mask: 0.1942  d6.loss_dice: 1.8159  d7.loss_cls: 0.7391  d7.loss_mask: 0.1951  d7.loss_dice: 1.7964  d8.loss_cls: 0.6795  d8.loss_mask: 0.1938  d8.loss_dice: 1.8175
05/08 09:47:00 - mmengine - INFO - Iter(train) [15900/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:44:21  time: 1.4510  data_time: 0.0515  memory: 29690  grad_norm: 57.3141  loss: 30.7784  loss_cls: 0.6630  loss_mask: 0.2063  loss_dice: 2.0278  d0.loss_cls: 1.1181  d0.loss_mask: 0.2457  d0.loss_dice: 2.2658  d1.loss_cls: 0.7809  d1.loss_mask: 0.2332  d1.loss_dice: 2.3073  d2.loss_cls: 0.7386  d2.loss_mask: 0.2223  d2.loss_dice: 2.1972  d3.loss_cls: 0.7398  d3.loss_mask: 0.2166  d3.loss_dice: 2.0857  d4.loss_cls: 0.6790  d4.loss_mask: 0.2116  d4.loss_dice: 2.0854  d5.loss_cls: 0.7014  d5.loss_mask: 0.2071  d5.loss_dice: 2.0583  d6.loss_cls: 0.7104  d6.loss_mask: 0.2061  d6.loss_dice: 2.0213  d7.loss_cls: 0.6816  d7.loss_mask: 0.2080  d7.loss_dice: 2.0365  d8.loss_cls: 0.6835  d8.loss_mask: 0.2056  d8.loss_dice: 2.0342
05/08 09:48:12 - mmengine - INFO - Iter(train) [15950/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:43:03  time: 1.4551  data_time: 0.0617  memory: 28536  grad_norm: 57.3617  loss: 29.4892  loss_cls: 0.6590  loss_mask: 0.1984  loss_dice: 1.9250  d0.loss_cls: 1.0932  d0.loss_mask: 0.2333  d0.loss_dice: 2.1528  d1.loss_cls: 0.7580  d1.loss_mask: 0.2210  d1.loss_dice: 2.1884  d2.loss_cls: 0.7358  d2.loss_mask: 0.2092  d2.loss_dice: 2.0638  d3.loss_cls: 0.7350  d3.loss_mask: 0.2059  d3.loss_dice: 1.9616  d4.loss_cls: 0.6704  d4.loss_mask: 0.2047  d4.loss_dice: 1.9652  d5.loss_cls: 0.7083  d5.loss_mask: 0.2001  d5.loss_dice: 1.9332  d6.loss_cls: 0.7253  d6.loss_mask: 0.1983  d6.loss_dice: 1.9072  d7.loss_cls: 0.7073  d7.loss_mask: 0.1977  d7.loss_dice: 1.9097  d8.loss_cls: 0.6968  d8.loss_mask: 0.1974  d8.loss_dice: 1.9271
05/08 09:49:24 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 09:49:24 - mmengine - INFO - Iter(train) [16000/20000]  base_lr: 1.0000e-04 lr: 1.0000e-05  eta: 1:41:46  time: 1.4304  data_time: 0.0572  memory: 28442  grad_norm: 65.6731  loss: 29.4617  loss_cls: 0.7033  loss_mask: 0.2060  loss_dice: 1.8780  d0.loss_cls: 1.0817  d0.loss_mask: 0.2405  d0.loss_dice: 2.1280  d1.loss_cls: 0.7806  d1.loss_mask: 0.2284  d1.loss_dice: 2.1384  d2.loss_cls: 0.7775  d2.loss_mask: 0.2172  d2.loss_dice: 2.0186  d3.loss_cls: 0.7741  d3.loss_mask: 0.2149  d3.loss_dice: 1.9139  d4.loss_cls: 0.6996  d4.loss_mask: 0.2142  d4.loss_dice: 1.9157  d5.loss_cls: 0.7492  d5.loss_mask: 0.2110  d5.loss_dice: 1.9035  d6.loss_cls: 0.7854  d6.loss_mask: 0.2046  d6.loss_dice: 1.8447  d7.loss_cls: 0.7160  d7.loss_mask: 0.2056  d7.loss_dice: 1.8887  d8.loss_cls: 0.7382  d8.loss_mask: 0.2058  d8.loss_dice: 1.8783
05/08 09:49:24 - mmengine - INFO - Saving checkpoint at 16000 iterations
05/08 09:50:15 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9713  data_time: 0.0279  memory: 3258  
05/08 09:50:38 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.40s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 09:50:47 - mmengine - INFO - start multi processing evaluation ...
DONE (t=49.98s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.345
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.630
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.303
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.199
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.404
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.834
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.540
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.898
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.516
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.398
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.610
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.917
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.540
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.540
05/08 09:51:37 - mmengine - INFO - segm_mAP_copypaste: 0.345 0.630 0.303 0.199 0.404 0.834
05/08 09:51:38 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.3450  coco/segm_mAP_50: 0.6300  coco/segm_mAP_75: 0.3030  coco/segm_mAP_s: 0.1990  coco/segm_mAP_m: 0.4040  coco/segm_mAP_l: 0.8340  data_time: 0.0278  time: 0.9691
05/08 09:52:50 - mmengine - INFO - Iter(train) [16050/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:40:49  time: 3.0850  data_time: 1.6927  memory: 29719  grad_norm: 71.7009  loss: 30.0482  loss_cls: 0.7378  loss_mask: 0.1970  loss_dice: 1.9028  d0.loss_cls: 1.0849  d0.loss_mask: 0.2335  d0.loss_dice: 2.2262  d1.loss_cls: 0.7820  d1.loss_mask: 0.2257  d1.loss_dice: 2.2180  d2.loss_cls: 0.7970  d2.loss_mask: 0.2165  d2.loss_dice: 2.0893  d3.loss_cls: 0.7888  d3.loss_mask: 0.2105  d3.loss_dice: 1.9696  d4.loss_cls: 0.7418  d4.loss_mask: 0.2048  d4.loss_dice: 1.9571  d5.loss_cls: 0.7324  d5.loss_mask: 0.2055  d5.loss_dice: 1.9656  d6.loss_cls: 0.7856  d6.loss_mask: 0.1994  d6.loss_dice: 1.8896  d7.loss_cls: 0.7036  d7.loss_mask: 0.1980  d7.loss_dice: 1.9370  d8.loss_cls: 0.7341  d8.loss_mask: 0.1973  d8.loss_dice: 1.9168
05/08 09:54:02 - mmengine - INFO - Iter(train) [16100/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:39:31  time: 1.4388  data_time: 0.0556  memory: 28948  grad_norm: 62.7285  loss: 29.7600  loss_cls: 0.7296  loss_mask: 0.1894  loss_dice: 1.9020  d0.loss_cls: 1.0917  d0.loss_mask: 0.2238  d0.loss_dice: 2.2117  d1.loss_cls: 0.7730  d1.loss_mask: 0.2141  d1.loss_dice: 2.2049  d2.loss_cls: 0.7806  d2.loss_mask: 0.2057  d2.loss_dice: 2.0750  d3.loss_cls: 0.7660  d3.loss_mask: 0.1996  d3.loss_dice: 1.9720  d4.loss_cls: 0.7042  d4.loss_mask: 0.1956  d4.loss_dice: 1.9653  d5.loss_cls: 0.7155  d5.loss_mask: 0.1962  d5.loss_dice: 1.9611  d6.loss_cls: 0.7742  d6.loss_mask: 0.1916  d6.loss_dice: 1.8905  d7.loss_cls: 0.6845  d7.loss_mask: 0.1907  d7.loss_dice: 1.9340  d8.loss_cls: 0.7060  d8.loss_mask: 0.1903  d8.loss_dice: 1.9212
05/08 09:55:13 - mmengine - INFO - Iter(train) [16150/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:38:13  time: 1.4252  data_time: 0.0613  memory: 28674  grad_norm: 57.0094  loss: 28.7269  loss_cls: 0.7445  loss_mask: 0.1921  loss_dice: 1.7846  d0.loss_cls: 1.0714  d0.loss_mask: 0.2339  d0.loss_dice: 2.0979  d1.loss_cls: 0.7743  d1.loss_mask: 0.2200  d1.loss_dice: 2.0873  d2.loss_cls: 0.7859  d2.loss_mask: 0.2115  d2.loss_dice: 1.9577  d3.loss_cls: 0.7676  d3.loss_mask: 0.2048  d3.loss_dice: 1.8581  d4.loss_cls: 0.7204  d4.loss_mask: 0.2004  d4.loss_dice: 1.8473  d5.loss_cls: 0.7271  d5.loss_mask: 0.2000  d5.loss_dice: 1.8420  d6.loss_cls: 0.7737  d6.loss_mask: 0.1945  d6.loss_dice: 1.7899  d7.loss_cls: 0.7038  d7.loss_mask: 0.1932  d7.loss_dice: 1.8100  d8.loss_cls: 0.7363  d8.loss_mask: 0.1927  d8.loss_dice: 1.8037
05/08 09:56:23 - mmengine - INFO - Iter(train) [16200/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:36:55  time: 1.4022  data_time: 0.0539  memory: 29114  grad_norm: 61.2386  loss: 28.5812  loss_cls: 0.7263  loss_mask: 0.1808  loss_dice: 1.8044  d0.loss_cls: 1.0686  d0.loss_mask: 0.2138  d0.loss_dice: 2.1033  d1.loss_cls: 0.7506  d1.loss_mask: 0.2058  d1.loss_dice: 2.0997  d2.loss_cls: 0.7634  d2.loss_mask: 0.1976  d2.loss_dice: 1.9800  d3.loss_cls: 0.7566  d3.loss_mask: 0.1923  d3.loss_dice: 1.8730  d4.loss_cls: 0.7010  d4.loss_mask: 0.1879  d4.loss_dice: 1.8560  d5.loss_cls: 0.7081  d5.loss_mask: 0.1875  d5.loss_dice: 1.8649  d6.loss_cls: 0.7730  d6.loss_mask: 0.1826  d6.loss_dice: 1.7914  d7.loss_cls: 0.6850  d7.loss_mask: 0.1804  d7.loss_dice: 1.8290  d8.loss_cls: 0.7237  d8.loss_mask: 0.1799  d8.loss_dice: 1.8146
05/08 09:57:37 - mmengine - INFO - Iter(train) [16250/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:35:38  time: 1.4798  data_time: 0.1057  memory: 29016  grad_norm: 70.2254  loss: 28.4703  loss_cls: 0.7379  loss_mask: 0.1860  loss_dice: 1.7675  d0.loss_cls: 1.0833  d0.loss_mask: 0.2233  d0.loss_dice: 2.0828  d1.loss_cls: 0.7622  d1.loss_mask: 0.2134  d1.loss_dice: 2.0810  d2.loss_cls: 0.7624  d2.loss_mask: 0.2047  d2.loss_dice: 1.9588  d3.loss_cls: 0.7656  d3.loss_mask: 0.1977  d3.loss_dice: 1.8468  d4.loss_cls: 0.7145  d4.loss_mask: 0.1918  d4.loss_dice: 1.8268  d5.loss_cls: 0.7217  d5.loss_mask: 0.1934  d5.loss_dice: 1.8327  d6.loss_cls: 0.7713  d6.loss_mask: 0.1866  d6.loss_dice: 1.7672  d7.loss_cls: 0.6890  d7.loss_mask: 0.1864  d7.loss_dice: 1.8015  d8.loss_cls: 0.7506  d8.loss_mask: 0.1846  d8.loss_dice: 1.7786
05/08 09:58:49 - mmengine - INFO - Iter(train) [16300/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:34:20  time: 1.4362  data_time: 0.0605  memory: 28467  grad_norm: 57.2722  loss: 28.6811  loss_cls: 0.7413  loss_mask: 0.1938  loss_dice: 1.7851  d0.loss_cls: 1.0917  d0.loss_mask: 0.2317  d0.loss_dice: 2.0901  d1.loss_cls: 0.7686  d1.loss_mask: 0.2208  d1.loss_dice: 2.0864  d2.loss_cls: 0.7724  d2.loss_mask: 0.2111  d2.loss_dice: 1.9629  d3.loss_cls: 0.7699  d3.loss_mask: 0.2033  d3.loss_dice: 1.8556  d4.loss_cls: 0.7154  d4.loss_mask: 0.1984  d4.loss_dice: 1.8422  d5.loss_cls: 0.7148  d5.loss_mask: 0.1990  d5.loss_dice: 1.8420  d6.loss_cls: 0.7703  d6.loss_mask: 0.1949  d6.loss_dice: 1.7851  d7.loss_cls: 0.7072  d7.loss_mask: 0.1910  d7.loss_dice: 1.8047  d8.loss_cls: 0.7457  d8.loss_mask: 0.1921  d8.loss_dice: 1.7936
05/08 10:00:00 - mmengine - INFO - Iter(train) [16350/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:33:03  time: 1.4322  data_time: 0.0509  memory: 29515  grad_norm: 53.5629  loss: 28.5912  loss_cls: 0.7489  loss_mask: 0.1848  loss_dice: 1.7802  d0.loss_cls: 1.0868  d0.loss_mask: 0.2198  d0.loss_dice: 2.0902  d1.loss_cls: 0.7667  d1.loss_mask: 0.2124  d1.loss_dice: 2.0972  d2.loss_cls: 0.7710  d2.loss_mask: 0.2020  d2.loss_dice: 1.9590  d3.loss_cls: 0.7619  d3.loss_mask: 0.1966  d3.loss_dice: 1.8548  d4.loss_cls: 0.7264  d4.loss_mask: 0.1910  d4.loss_dice: 1.8419  d5.loss_cls: 0.7163  d5.loss_mask: 0.1912  d5.loss_dice: 1.8431  d6.loss_cls: 0.7704  d6.loss_mask: 0.1868  d6.loss_dice: 1.7755  d7.loss_cls: 0.7180  d7.loss_mask: 0.1829  d7.loss_dice: 1.8008  d8.loss_cls: 0.7297  d8.loss_mask: 0.1868  d8.loss_dice: 1.7982
05/08 10:01:12 - mmengine - INFO - Iter(train) [16400/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:31:45  time: 1.4290  data_time: 0.0585  memory: 28200  grad_norm: 66.1029  loss: 29.1279  loss_cls: 0.7549  loss_mask: 0.1909  loss_dice: 1.8159  d0.loss_cls: 1.0854  d0.loss_mask: 0.2259  d0.loss_dice: 2.1283  d1.loss_cls: 0.7797  d1.loss_mask: 0.2175  d1.loss_dice: 2.1321  d2.loss_cls: 0.7857  d2.loss_mask: 0.2082  d2.loss_dice: 1.9940  d3.loss_cls: 0.7854  d3.loss_mask: 0.2008  d3.loss_dice: 1.8927  d4.loss_cls: 0.7399  d4.loss_mask: 0.1946  d4.loss_dice: 1.8701  d5.loss_cls: 0.7485  d5.loss_mask: 0.1961  d5.loss_dice: 1.8731  d6.loss_cls: 0.7788  d6.loss_mask: 0.1906  d6.loss_dice: 1.8146  d7.loss_cls: 0.7439  d7.loss_mask: 0.1856  d7.loss_dice: 1.8373  d8.loss_cls: 0.7234  d8.loss_mask: 0.1935  d8.loss_dice: 1.8406
05/08 10:02:24 - mmengine - INFO - Iter(train) [16450/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:30:28  time: 1.4431  data_time: 0.0629  memory: 28349  grad_norm: 60.4337  loss: 29.0728  loss_cls: 0.7523  loss_mask: 0.1871  loss_dice: 1.8173  d0.loss_cls: 1.0905  d0.loss_mask: 0.2256  d0.loss_dice: 2.1305  d1.loss_cls: 0.7792  d1.loss_mask: 0.2146  d1.loss_dice: 2.1328  d2.loss_cls: 0.7830  d2.loss_mask: 0.2033  d2.loss_dice: 1.9954  d3.loss_cls: 0.7699  d3.loss_mask: 0.1981  d3.loss_dice: 1.9004  d4.loss_cls: 0.7316  d4.loss_mask: 0.1922  d4.loss_dice: 1.8722  d5.loss_cls: 0.7394  d5.loss_mask: 0.1924  d5.loss_dice: 1.8752  d6.loss_cls: 0.7697  d6.loss_mask: 0.1877  d6.loss_dice: 1.8207  d7.loss_cls: 0.7341  d7.loss_mask: 0.1831  d7.loss_dice: 1.8406  d8.loss_cls: 0.7084  d8.loss_mask: 0.1915  d8.loss_dice: 1.8539
05/08 10:03:35 - mmengine - INFO - Iter(train) [16500/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:29:10  time: 1.4137  data_time: 0.0508  memory: 29138  grad_norm: 57.0152  loss: 29.7665  loss_cls: 0.7498  loss_mask: 0.2030  loss_dice: 1.8644  d0.loss_cls: 1.0948  d0.loss_mask: 0.2430  d0.loss_dice: 2.1951  d1.loss_cls: 0.7758  d1.loss_mask: 0.2316  d1.loss_dice: 2.1927  d2.loss_cls: 0.7771  d2.loss_mask: 0.2228  d2.loss_dice: 2.0574  d3.loss_cls: 0.7770  d3.loss_mask: 0.2158  d3.loss_dice: 1.9461  d4.loss_cls: 0.7332  d4.loss_mask: 0.2092  d4.loss_dice: 1.9251  d5.loss_cls: 0.7374  d5.loss_mask: 0.2094  d5.loss_dice: 1.9282  d6.loss_cls: 0.7570  d6.loss_mask: 0.2049  d6.loss_dice: 1.8722  d7.loss_cls: 0.7294  d7.loss_mask: 0.1992  d7.loss_dice: 1.8849  d8.loss_cls: 0.7170  d8.loss_mask: 0.2063  d8.loss_dice: 1.9067
05/08 10:04:45 - mmengine - INFO - Iter(train) [16550/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:27:52  time: 1.4060  data_time: 0.0537  memory: 29011  grad_norm: 60.3017  loss: 29.0279  loss_cls: 0.7508  loss_mask: 0.1878  loss_dice: 1.8164  d0.loss_cls: 1.0819  d0.loss_mask: 0.2281  d0.loss_dice: 2.1479  d1.loss_cls: 0.7580  d1.loss_mask: 0.2158  d1.loss_dice: 2.1460  d2.loss_cls: 0.7596  d2.loss_mask: 0.2043  d2.loss_dice: 2.0003  d3.loss_cls: 0.7656  d3.loss_mask: 0.1999  d3.loss_dice: 1.9021  d4.loss_cls: 0.7222  d4.loss_mask: 0.1945  d4.loss_dice: 1.8724  d5.loss_cls: 0.7294  d5.loss_mask: 0.1951  d5.loss_dice: 1.8795  d6.loss_cls: 0.7387  d6.loss_mask: 0.1912  d6.loss_dice: 1.8219  d7.loss_cls: 0.7408  d7.loss_mask: 0.1853  d7.loss_dice: 1.8378  d8.loss_cls: 0.7110  d8.loss_mask: 0.1912  d8.loss_dice: 1.8525
05/08 10:05:57 - mmengine - INFO - Iter(train) [16600/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:26:35  time: 1.4412  data_time: 0.0606  memory: 29149  grad_norm: 76.1901  loss: 29.7074  loss_cls: 0.7610  loss_mask: 0.1895  loss_dice: 1.8710  d0.loss_cls: 1.0948  d0.loss_mask: 0.2253  d0.loss_dice: 2.2020  d1.loss_cls: 0.7710  d1.loss_mask: 0.2187  d1.loss_dice: 2.2112  d2.loss_cls: 0.7768  d2.loss_mask: 0.2078  d2.loss_dice: 2.0600  d3.loss_cls: 0.7846  d3.loss_mask: 0.2019  d3.loss_dice: 1.9550  d4.loss_cls: 0.7314  d4.loss_mask: 0.1978  d4.loss_dice: 1.9312  d5.loss_cls: 0.7530  d5.loss_mask: 0.1958  d5.loss_dice: 1.9256  d6.loss_cls: 0.7450  d6.loss_mask: 0.1931  d6.loss_dice: 1.8792  d7.loss_cls: 0.7436  d7.loss_mask: 0.1878  d7.loss_dice: 1.8822  d8.loss_cls: 0.7105  d8.loss_mask: 0.1935  d8.loss_dice: 1.9071
05/08 10:07:09 - mmengine - INFO - Iter(train) [16650/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:25:18  time: 1.4475  data_time: 0.1012  memory: 29311  grad_norm: 60.3142  loss: 29.4769  loss_cls: 0.7605  loss_mask: 0.1970  loss_dice: 1.8420  d0.loss_cls: 1.0965  d0.loss_mask: 0.2409  d0.loss_dice: 2.1808  d1.loss_cls: 0.7691  d1.loss_mask: 0.2280  d1.loss_dice: 2.1709  d2.loss_cls: 0.7677  d2.loss_mask: 0.2176  d2.loss_dice: 2.0327  d3.loss_cls: 0.7779  d3.loss_mask: 0.2116  d3.loss_dice: 1.9203  d4.loss_cls: 0.7317  d4.loss_mask: 0.2061  d4.loss_dice: 1.9032  d5.loss_cls: 0.7527  d5.loss_mask: 0.2032  d5.loss_dice: 1.8957  d6.loss_cls: 0.7356  d6.loss_mask: 0.1999  d6.loss_dice: 1.8577  d7.loss_cls: 0.7588  d7.loss_mask: 0.1935  d7.loss_dice: 1.8455  d8.loss_cls: 0.7057  d8.loss_mask: 0.2005  d8.loss_dice: 1.8735
05/08 10:08:22 - mmengine - INFO - Iter(train) [16700/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:24:01  time: 1.4499  data_time: 0.0578  memory: 28630  grad_norm: 62.8206  loss: 29.5105  loss_cls: 0.7605  loss_mask: 0.1930  loss_dice: 1.8396  d0.loss_cls: 1.0969  d0.loss_mask: 0.2343  d0.loss_dice: 2.1829  d1.loss_cls: 0.7796  d1.loss_mask: 0.2223  d1.loss_dice: 2.1827  d2.loss_cls: 0.7714  d2.loss_mask: 0.2116  d2.loss_dice: 2.0433  d3.loss_cls: 0.7727  d3.loss_mask: 0.2061  d3.loss_dice: 1.9351  d4.loss_cls: 0.7323  d4.loss_mask: 0.2013  d4.loss_dice: 1.9089  d5.loss_cls: 0.7501  d5.loss_mask: 0.1994  d5.loss_dice: 1.9008  d6.loss_cls: 0.7405  d6.loss_mask: 0.1961  d6.loss_dice: 1.8562  d7.loss_cls: 0.7567  d7.loss_mask: 0.1903  d7.loss_dice: 1.8471  d8.loss_cls: 0.7248  d8.loss_mask: 0.1967  d8.loss_dice: 1.8775
05/08 10:09:34 - mmengine - INFO - Iter(train) [16750/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:22:44  time: 1.4477  data_time: 0.0539  memory: 28856  grad_norm: 60.8761  loss: 29.1497  loss_cls: 0.7610  loss_mask: 0.1899  loss_dice: 1.8118  d0.loss_cls: 1.0922  d0.loss_mask: 0.2280  d0.loss_dice: 2.1491  d1.loss_cls: 0.7770  d1.loss_mask: 0.2189  d1.loss_dice: 2.1462  d2.loss_cls: 0.7664  d2.loss_mask: 0.2087  d2.loss_dice: 2.0061  d3.loss_cls: 0.7726  d3.loss_mask: 0.2030  d3.loss_dice: 1.9025  d4.loss_cls: 0.7258  d4.loss_mask: 0.2000  d4.loss_dice: 1.8831  d5.loss_cls: 0.7482  d5.loss_mask: 0.1975  d5.loss_dice: 1.8686  d6.loss_cls: 0.7481  d6.loss_mask: 0.1927  d6.loss_dice: 1.8277  d7.loss_cls: 0.7589  d7.loss_mask: 0.1873  d7.loss_dice: 1.8214  d8.loss_cls: 0.7166  d8.loss_mask: 0.1936  d8.loss_dice: 1.8464
05/08 10:10:46 - mmengine - INFO - Iter(train) [16800/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:21:26  time: 1.4308  data_time: 0.0542  memory: 28929  grad_norm: 90.1370  loss: 29.9125  loss_cls: 0.7514  loss_mask: 0.1940  loss_dice: 1.8799  d0.loss_cls: 1.1062  d0.loss_mask: 0.2337  d0.loss_dice: 2.2349  d1.loss_cls: 0.7716  d1.loss_mask: 0.2257  d1.loss_dice: 2.2406  d2.loss_cls: 0.7662  d2.loss_mask: 0.2125  d2.loss_dice: 2.0880  d3.loss_cls: 0.7709  d3.loss_mask: 0.2072  d3.loss_dice: 1.9729  d4.loss_cls: 0.7165  d4.loss_mask: 0.2029  d4.loss_dice: 1.9580  d5.loss_cls: 0.7395  d5.loss_mask: 0.2006  d5.loss_dice: 1.9405  d6.loss_cls: 0.7509  d6.loss_mask: 0.1949  d6.loss_dice: 1.8988  d7.loss_cls: 0.7550  d7.loss_mask: 0.1917  d7.loss_dice: 1.8856  d8.loss_cls: 0.7060  d8.loss_mask: 0.1973  d8.loss_dice: 1.9187
05/08 10:11:59 - mmengine - INFO - Iter(train) [16850/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:20:09  time: 1.4554  data_time: 0.0531  memory: 28900  grad_norm: 68.4219  loss: 29.0650  loss_cls: 0.7629  loss_mask: 0.1787  loss_dice: 1.8173  d0.loss_cls: 1.0892  d0.loss_mask: 0.2165  d0.loss_dice: 2.1492  d1.loss_cls: 0.7640  d1.loss_mask: 0.2075  d1.loss_dice: 2.1543  d2.loss_cls: 0.7683  d2.loss_mask: 0.1968  d2.loss_dice: 2.0122  d3.loss_cls: 0.7778  d3.loss_mask: 0.1905  d3.loss_dice: 1.8961  d4.loss_cls: 0.7294  d4.loss_mask: 0.1882  d4.loss_dice: 1.8882  d5.loss_cls: 0.7521  d5.loss_mask: 0.1851  d5.loss_dice: 1.8680  d6.loss_cls: 0.7502  d6.loss_mask: 0.1798  d6.loss_dice: 1.8261  d7.loss_cls: 0.7636  d7.loss_mask: 0.1766  d7.loss_dice: 1.8170  d8.loss_cls: 0.7210  d8.loss_mask: 0.1828  d8.loss_dice: 1.8557
05/08 10:13:11 - mmengine - INFO - Iter(train) [16900/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:18:52  time: 1.4428  data_time: 0.0522  memory: 29539  grad_norm: 56.8688  loss: 29.8852  loss_cls: 0.7576  loss_mask: 0.1928  loss_dice: 1.8765  d0.loss_cls: 1.1063  d0.loss_mask: 0.2333  d0.loss_dice: 2.2233  d1.loss_cls: 0.7746  d1.loss_mask: 0.2232  d1.loss_dice: 2.2318  d2.loss_cls: 0.7771  d2.loss_mask: 0.2122  d2.loss_dice: 2.0800  d3.loss_cls: 0.7867  d3.loss_mask: 0.2061  d3.loss_dice: 1.9552  d4.loss_cls: 0.7239  d4.loss_mask: 0.2032  d4.loss_dice: 1.9567  d5.loss_cls: 0.7446  d5.loss_mask: 0.2003  d5.loss_dice: 1.9287  d6.loss_cls: 0.7608  d6.loss_mask: 0.1934  d6.loss_dice: 1.8745  d7.loss_cls: 0.7674  d7.loss_mask: 0.1913  d7.loss_dice: 1.8737  d8.loss_cls: 0.7307  d8.loss_mask: 0.1960  d8.loss_dice: 1.9035
05/08 10:14:22 - mmengine - INFO - Iter(train) [16950/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:17:35  time: 1.4175  data_time: 0.0552  memory: 28712  grad_norm: 61.6632  loss: 28.6141  loss_cls: 0.7406  loss_mask: 0.1914  loss_dice: 1.7851  d0.loss_cls: 1.0889  d0.loss_mask: 0.2297  d0.loss_dice: 2.1070  d1.loss_cls: 0.7562  d1.loss_mask: 0.2193  d1.loss_dice: 2.1096  d2.loss_cls: 0.7524  d2.loss_mask: 0.2090  d2.loss_dice: 1.9635  d3.loss_cls: 0.7660  d3.loss_mask: 0.2022  d3.loss_dice: 1.8490  d4.loss_cls: 0.7059  d4.loss_mask: 0.2010  d4.loss_dice: 1.8559  d5.loss_cls: 0.7305  d5.loss_mask: 0.1974  d5.loss_dice: 1.8209  d6.loss_cls: 0.7438  d6.loss_mask: 0.1923  d6.loss_dice: 1.7807  d7.loss_cls: 0.7502  d7.loss_mask: 0.1896  d7.loss_dice: 1.7723  d8.loss_cls: 0.7141  d8.loss_mask: 0.1935  d8.loss_dice: 1.7961
05/08 10:15:34 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 10:15:34 - mmengine - INFO - Iter(train) [17000/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:16:18  time: 1.4510  data_time: 0.0541  memory: 29246  grad_norm: 65.1599  loss: 29.8900  loss_cls: 0.7583  loss_mask: 0.1848  loss_dice: 1.8947  d0.loss_cls: 1.0944  d0.loss_mask: 0.2193  d0.loss_dice: 2.2237  d1.loss_cls: 0.7716  d1.loss_mask: 0.2131  d1.loss_dice: 2.2307  d2.loss_cls: 0.7782  d2.loss_mask: 0.2033  d2.loss_dice: 2.0839  d3.loss_cls: 0.7867  d3.loss_mask: 0.1969  d3.loss_dice: 1.9603  d4.loss_cls: 0.7321  d4.loss_mask: 0.1962  d4.loss_dice: 1.9669  d5.loss_cls: 0.7448  d5.loss_mask: 0.1913  d5.loss_dice: 1.9409  d6.loss_cls: 0.7591  d6.loss_mask: 0.1862  d6.loss_dice: 1.8944  d7.loss_cls: 0.7608  d7.loss_mask: 0.1861  d7.loss_dice: 1.8866  d8.loss_cls: 0.7446  d8.loss_mask: 0.1876  d8.loss_dice: 1.9123
05/08 10:15:34 - mmengine - INFO - Saving checkpoint at 17000 iterations
05/08 10:16:26 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9799  data_time: 0.0267  memory: 3258  
05/08 10:16:50 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.39s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 10:16:57 - mmengine - INFO - start multi processing evaluation ...
DONE (t=52.70s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.339
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.654
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.287
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.398
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.857
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.532
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.922
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.508
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.378
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.610
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.917
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.532
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.532
05/08 10:17:50 - mmengine - INFO - segm_mAP_copypaste: 0.339 0.654 0.287 0.200 0.398 0.857
05/08 10:17:51 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.3390  coco/segm_mAP_50: 0.6540  coco/segm_mAP_75: 0.2870  coco/segm_mAP_s: 0.2000  coco/segm_mAP_m: 0.3980  coco/segm_mAP_l: 0.8570  data_time: 0.0267  time: 0.9776
05/08 10:19:03 - mmengine - INFO - Iter(train) [17050/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:15:15  time: 3.1322  data_time: 1.7898  memory: 28961  grad_norm: 63.2317  loss: 28.3343  loss_cls: 0.7507  loss_mask: 0.1800  loss_dice: 1.7547  d0.loss_cls: 1.0842  d0.loss_mask: 0.2170  d0.loss_dice: 2.0688  d1.loss_cls: 0.7673  d1.loss_mask: 0.2053  d1.loss_dice: 2.0614  d2.loss_cls: 0.7764  d2.loss_mask: 0.1956  d2.loss_dice: 1.9230  d3.loss_cls: 0.7944  d3.loss_mask: 0.1909  d3.loss_dice: 1.8135  d4.loss_cls: 0.7337  d4.loss_mask: 0.1914  d4.loss_dice: 1.8278  d5.loss_cls: 0.7431  d5.loss_mask: 0.1859  d5.loss_dice: 1.7951  d6.loss_cls: 0.7683  d6.loss_mask: 0.1801  d6.loss_dice: 1.7423  d7.loss_cls: 0.7464  d7.loss_mask: 0.1815  d7.loss_dice: 1.7520  d8.loss_cls: 0.7574  d8.loss_mask: 0.1816  d8.loss_dice: 1.7643
05/08 10:20:14 - mmengine - INFO - Iter(train) [17100/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:13:58  time: 1.4300  data_time: 0.0568  memory: 27969  grad_norm: 60.2101  loss: 28.2580  loss_cls: 0.7479  loss_mask: 0.1814  loss_dice: 1.7483  d0.loss_cls: 1.0788  d0.loss_mask: 0.2174  d0.loss_dice: 2.0679  d1.loss_cls: 0.7638  d1.loss_mask: 0.2081  d1.loss_dice: 2.0656  d2.loss_cls: 0.7722  d2.loss_mask: 0.1998  d2.loss_dice: 1.9318  d3.loss_cls: 0.7746  d3.loss_mask: 0.1939  d3.loss_dice: 1.8204  d4.loss_cls: 0.7202  d4.loss_mask: 0.1931  d4.loss_dice: 1.8263  d5.loss_cls: 0.7228  d5.loss_mask: 0.1879  d5.loss_dice: 1.8034  d6.loss_cls: 0.7506  d6.loss_mask: 0.1819  d6.loss_dice: 1.7418  d7.loss_cls: 0.7161  d7.loss_mask: 0.1851  d7.loss_dice: 1.7650  d8.loss_cls: 0.7395  d8.loss_mask: 0.1822  d8.loss_dice: 1.7701
05/08 10:21:27 - mmengine - INFO - Iter(train) [17150/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:12:41  time: 1.4531  data_time: 0.0518  memory: 29668  grad_norm: 58.2549  loss: 29.6453  loss_cls: 0.7487  loss_mask: 0.1848  loss_dice: 1.8670  d0.loss_cls: 1.1147  d0.loss_mask: 0.2234  d0.loss_dice: 2.2132  d1.loss_cls: 0.7709  d1.loss_mask: 0.2130  d1.loss_dice: 2.2071  d2.loss_cls: 0.7686  d2.loss_mask: 0.2049  d2.loss_dice: 2.0717  d3.loss_cls: 0.7756  d3.loss_mask: 0.1967  d3.loss_dice: 1.9495  d4.loss_cls: 0.7299  d4.loss_mask: 0.1943  d4.loss_dice: 1.9526  d5.loss_cls: 0.7307  d5.loss_mask: 0.1908  d5.loss_dice: 1.9288  d6.loss_cls: 0.7525  d6.loss_mask: 0.1851  d6.loss_dice: 1.8683  d7.loss_cls: 0.7038  d7.loss_mask: 0.1888  d7.loss_dice: 1.8930  d8.loss_cls: 0.7365  d8.loss_mask: 0.1862  d8.loss_dice: 1.8940
05/08 10:22:40 - mmengine - INFO - Iter(train) [17200/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:11:24  time: 1.4542  data_time: 0.0582  memory: 29223  grad_norm: 51.7810  loss: 28.5816  loss_cls: 0.7573  loss_mask: 0.1798  loss_dice: 1.7757  d0.loss_cls: 1.0780  d0.loss_mask: 0.2177  d0.loss_dice: 2.0985  d1.loss_cls: 0.7707  d1.loss_mask: 0.2082  d1.loss_dice: 2.0964  d2.loss_cls: 0.7673  d2.loss_mask: 0.1987  d2.loss_dice: 1.9574  d3.loss_cls: 0.7788  d3.loss_mask: 0.1930  d3.loss_dice: 1.8454  d4.loss_cls: 0.7280  d4.loss_mask: 0.1921  d4.loss_dice: 1.8597  d5.loss_cls: 0.7332  d5.loss_mask: 0.1875  d5.loss_dice: 1.8314  d6.loss_cls: 0.7578  d6.loss_mask: 0.1811  d6.loss_dice: 1.7713  d7.loss_cls: 0.6923  d7.loss_mask: 0.1856  d7.loss_dice: 1.8072  d8.loss_cls: 0.7499  d8.loss_mask: 0.1807  d8.loss_dice: 1.8009
05/08 10:23:52 - mmengine - INFO - Iter(train) [17250/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:10:07  time: 1.4562  data_time: 0.0567  memory: 28655  grad_norm: 52.7272  loss: 29.8868  loss_cls: 0.7670  loss_mask: 0.1957  loss_dice: 1.8611  d0.loss_cls: 1.1202  d0.loss_mask: 0.2359  d0.loss_dice: 2.2089  d1.loss_cls: 0.7793  d1.loss_mask: 0.2253  d1.loss_dice: 2.2123  d2.loss_cls: 0.7783  d2.loss_mask: 0.2161  d2.loss_dice: 2.0651  d3.loss_cls: 0.7854  d3.loss_mask: 0.2094  d3.loss_dice: 1.9485  d4.loss_cls: 0.7457  d4.loss_mask: 0.2079  d4.loss_dice: 1.9594  d5.loss_cls: 0.7418  d5.loss_mask: 0.2046  d5.loss_dice: 1.9326  d6.loss_cls: 0.7707  d6.loss_mask: 0.1969  d6.loss_dice: 1.8659  d7.loss_cls: 0.6917  d7.loss_mask: 0.2046  d7.loss_dice: 1.9086  d8.loss_cls: 0.7541  d8.loss_mask: 0.1985  d8.loss_dice: 1.8951
05/08 10:25:04 - mmengine - INFO - Iter(train) [17300/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:08:49  time: 1.4289  data_time: 0.0565  memory: 29217  grad_norm: 54.2022  loss: 29.1053  loss_cls: 0.7349  loss_mask: 0.1894  loss_dice: 1.8163  d0.loss_cls: 1.0927  d0.loss_mask: 0.2289  d0.loss_dice: 2.1770  d1.loss_cls: 0.7642  d1.loss_mask: 0.2190  d1.loss_dice: 2.1578  d2.loss_cls: 0.7626  d2.loss_mask: 0.2089  d2.loss_dice: 2.0158  d3.loss_cls: 0.7732  d3.loss_mask: 0.2031  d3.loss_dice: 1.8912  d4.loss_cls: 0.7337  d4.loss_mask: 0.2002  d4.loss_dice: 1.8999  d5.loss_cls: 0.7220  d5.loss_mask: 0.1972  d5.loss_dice: 1.8755  d6.loss_cls: 0.7525  d6.loss_mask: 0.1892  d6.loss_dice: 1.8160  d7.loss_cls: 0.6688  d7.loss_mask: 0.1953  d7.loss_dice: 1.8578  d8.loss_cls: 0.7267  d8.loss_mask: 0.1922  d8.loss_dice: 1.8432
05/08 10:26:16 - mmengine - INFO - Iter(train) [17350/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:07:32  time: 1.4361  data_time: 0.0588  memory: 28723  grad_norm: 58.1643  loss: 29.0564  loss_cls: 0.7205  loss_mask: 0.1815  loss_dice: 1.8250  d0.loss_cls: 1.0874  d0.loss_mask: 0.2208  d0.loss_dice: 2.1735  d1.loss_cls: 0.7688  d1.loss_mask: 0.2095  d1.loss_dice: 2.1686  d2.loss_cls: 0.7644  d2.loss_mask: 0.2007  d2.loss_dice: 2.0239  d3.loss_cls: 0.7793  d3.loss_mask: 0.1947  d3.loss_dice: 1.8947  d4.loss_cls: 0.7410  d4.loss_mask: 0.1899  d4.loss_dice: 1.8947  d5.loss_cls: 0.7272  d5.loss_mask: 0.1873  d5.loss_dice: 1.8764  d6.loss_cls: 0.7452  d6.loss_mask: 0.1807  d6.loss_dice: 1.8206  d7.loss_cls: 0.6663  d7.loss_mask: 0.1875  d7.loss_dice: 1.8696  d8.loss_cls: 0.7232  d8.loss_mask: 0.1842  d8.loss_dice: 1.8493
05/08 10:27:27 - mmengine - INFO - Iter(train) [17400/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:06:15  time: 1.4266  data_time: 0.0582  memory: 28930  grad_norm: 56.1643  loss: 29.3632  loss_cls: 0.7175  loss_mask: 0.1952  loss_dice: 1.8517  d0.loss_cls: 1.0974  d0.loss_mask: 0.2344  d0.loss_dice: 2.1700  d1.loss_cls: 0.7709  d1.loss_mask: 0.2250  d1.loss_dice: 2.1662  d2.loss_cls: 0.7677  d2.loss_mask: 0.2129  d2.loss_dice: 2.0339  d3.loss_cls: 0.7810  d3.loss_mask: 0.2074  d3.loss_dice: 1.9130  d4.loss_cls: 0.7427  d4.loss_mask: 0.2034  d4.loss_dice: 1.9196  d5.loss_cls: 0.7317  d5.loss_mask: 0.1999  d5.loss_dice: 1.8974  d6.loss_cls: 0.7524  d6.loss_mask: 0.1944  d6.loss_dice: 1.8374  d7.loss_cls: 0.6703  d7.loss_mask: 0.2008  d7.loss_dice: 1.8794  d8.loss_cls: 0.7327  d8.loss_mask: 0.1958  d8.loss_dice: 1.8611
05/08 10:28:41 - mmengine - INFO - Iter(train) [17450/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:04:58  time: 1.4823  data_time: 0.1047  memory: 28978  grad_norm: 56.4841  loss: 29.2431  loss_cls: 0.7074  loss_mask: 0.1944  loss_dice: 1.8434  d0.loss_cls: 1.1209  d0.loss_mask: 0.2339  d0.loss_dice: 2.1705  d1.loss_cls: 0.7903  d1.loss_mask: 0.2245  d1.loss_dice: 2.1629  d2.loss_cls: 0.7724  d2.loss_mask: 0.2123  d2.loss_dice: 2.0185  d3.loss_cls: 0.7856  d3.loss_mask: 0.2039  d3.loss_dice: 1.8907  d4.loss_cls: 0.7476  d4.loss_mask: 0.2006  d4.loss_dice: 1.8938  d5.loss_cls: 0.7358  d5.loss_mask: 0.1968  d5.loss_dice: 1.8787  d6.loss_cls: 0.7548  d6.loss_mask: 0.1920  d6.loss_dice: 1.8184  d7.loss_cls: 0.6567  d7.loss_mask: 0.1986  d7.loss_dice: 1.8697  d8.loss_cls: 0.7302  d8.loss_mask: 0.1951  d8.loss_dice: 1.8426
05/08 10:29:53 - mmengine - INFO - Iter(train) [17500/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:03:41  time: 1.4299  data_time: 0.0520  memory: 28392  grad_norm: 65.6222  loss: 27.7696  loss_cls: 0.6854  loss_mask: 0.1796  loss_dice: 1.7418  d0.loss_cls: 1.0808  d0.loss_mask: 0.2176  d0.loss_dice: 2.0345  d1.loss_cls: 0.7558  d1.loss_mask: 0.2073  d1.loss_dice: 2.0378  d2.loss_cls: 0.7489  d2.loss_mask: 0.1966  d2.loss_dice: 1.9036  d3.loss_cls: 0.7660  d3.loss_mask: 0.1895  d3.loss_dice: 1.7870  d4.loss_cls: 0.7252  d4.loss_mask: 0.1871  d4.loss_dice: 1.7977  d5.loss_cls: 0.7179  d5.loss_mask: 0.1814  d5.loss_dice: 1.7674  d6.loss_cls: 0.7326  d6.loss_mask: 0.1781  d6.loss_dice: 1.7180  d7.loss_cls: 0.6480  d7.loss_mask: 0.1843  d7.loss_dice: 1.7681  d8.loss_cls: 0.7149  d8.loss_mask: 0.1796  d8.loss_dice: 1.7370
05/08 10:31:04 - mmengine - INFO - Iter(train) [17550/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:02:24  time: 1.4288  data_time: 0.0513  memory: 28082  grad_norm: 60.9178  loss: 28.2594  loss_cls: 0.6887  loss_mask: 0.1876  loss_dice: 1.7810  d0.loss_cls: 1.0829  d0.loss_mask: 0.2237  d0.loss_dice: 2.0770  d1.loss_cls: 0.7607  d1.loss_mask: 0.2145  d1.loss_dice: 2.0684  d2.loss_cls: 0.7561  d2.loss_mask: 0.2020  d2.loss_dice: 1.9292  d3.loss_cls: 0.7728  d3.loss_mask: 0.1948  d3.loss_dice: 1.8116  d4.loss_cls: 0.7370  d4.loss_mask: 0.1920  d4.loss_dice: 1.8252  d5.loss_cls: 0.7310  d5.loss_mask: 0.1893  d5.loss_dice: 1.7999  d6.loss_cls: 0.7320  d6.loss_mask: 0.1857  d6.loss_dice: 1.7607  d7.loss_cls: 0.6657  d7.loss_mask: 0.1923  d7.loss_dice: 1.8039  d8.loss_cls: 0.7313  d8.loss_mask: 0.1882  d8.loss_dice: 1.7743
05/08 10:32:16 - mmengine - INFO - Iter(train) [17600/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 1:01:07  time: 1.4496  data_time: 0.0561  memory: 28606  grad_norm: 68.1880  loss: 29.2352  loss_cls: 0.6989  loss_mask: 0.1913  loss_dice: 1.8545  d0.loss_cls: 1.1057  d0.loss_mask: 0.2298  d0.loss_dice: 2.1449  d1.loss_cls: 0.7758  d1.loss_mask: 0.2188  d1.loss_dice: 2.1534  d2.loss_cls: 0.7698  d2.loss_mask: 0.2079  d2.loss_dice: 2.0226  d3.loss_cls: 0.7870  d3.loss_mask: 0.1993  d3.loss_dice: 1.9045  d4.loss_cls: 0.7402  d4.loss_mask: 0.1970  d4.loss_dice: 1.9136  d5.loss_cls: 0.7318  d5.loss_mask: 0.1937  d5.loss_dice: 1.8905  d6.loss_cls: 0.7363  d6.loss_mask: 0.1906  d6.loss_dice: 1.8418  d7.loss_cls: 0.6739  d7.loss_mask: 0.1949  d7.loss_dice: 1.8775  d8.loss_cls: 0.7404  d8.loss_mask: 0.1915  d8.loss_dice: 1.8572
05/08 10:33:29 - mmengine - INFO - Iter(train) [17650/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 0:59:50  time: 1.4433  data_time: 0.0575  memory: 29482  grad_norm: 58.1861  loss: 28.9817  loss_cls: 0.6762  loss_mask: 0.1958  loss_dice: 1.8408  d0.loss_cls: 1.1109  d0.loss_mask: 0.2323  d0.loss_dice: 2.1545  d1.loss_cls: 0.7657  d1.loss_mask: 0.2220  d1.loss_dice: 2.1480  d2.loss_cls: 0.7593  d2.loss_mask: 0.2122  d2.loss_dice: 2.0001  d3.loss_cls: 0.7702  d3.loss_mask: 0.2035  d3.loss_dice: 1.8772  d4.loss_cls: 0.7230  d4.loss_mask: 0.2006  d4.loss_dice: 1.8892  d5.loss_cls: 0.7189  d5.loss_mask: 0.1984  d5.loss_dice: 1.8698  d6.loss_cls: 0.7172  d6.loss_mask: 0.1951  d6.loss_dice: 1.8215  d7.loss_cls: 0.6644  d7.loss_mask: 0.1994  d7.loss_dice: 1.8581  d8.loss_cls: 0.7297  d8.loss_mask: 0.1946  d8.loss_dice: 1.8330
05/08 10:34:41 - mmengine - INFO - Iter(train) [17700/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 0:58:33  time: 1.4479  data_time: 0.0567  memory: 28991  grad_norm: 63.8062  loss: 28.2938  loss_cls: 0.6700  loss_mask: 0.1920  loss_dice: 1.7846  d0.loss_cls: 1.0975  d0.loss_mask: 0.2318  d0.loss_dice: 2.0846  d1.loss_cls: 0.7590  d1.loss_mask: 0.2183  d1.loss_dice: 2.0770  d2.loss_cls: 0.7491  d2.loss_mask: 0.2096  d2.loss_dice: 1.9435  d3.loss_cls: 0.7727  d3.loss_mask: 0.2014  d3.loss_dice: 1.8123  d4.loss_cls: 0.7209  d4.loss_mask: 0.1999  d4.loss_dice: 1.8252  d5.loss_cls: 0.7134  d5.loss_mask: 0.1953  d5.loss_dice: 1.8070  d6.loss_cls: 0.7057  d6.loss_mask: 0.1941  d6.loss_dice: 1.7714  d7.loss_cls: 0.6689  d7.loss_mask: 0.1959  d7.loss_dice: 1.8026  d8.loss_cls: 0.7244  d8.loss_mask: 0.1924  d8.loss_dice: 1.7731
05/08 10:35:53 - mmengine - INFO - Iter(train) [17750/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 0:57:16  time: 1.4388  data_time: 0.0551  memory: 29184  grad_norm: 75.7536  loss: 28.4765  loss_cls: 0.6689  loss_mask: 0.1944  loss_dice: 1.7980  d0.loss_cls: 1.1156  d0.loss_mask: 0.2374  d0.loss_dice: 2.1093  d1.loss_cls: 0.7604  d1.loss_mask: 0.2246  d1.loss_dice: 2.0994  d2.loss_cls: 0.7541  d2.loss_mask: 0.2119  d2.loss_dice: 1.9456  d3.loss_cls: 0.7774  d3.loss_mask: 0.2031  d3.loss_dice: 1.8269  d4.loss_cls: 0.7193  d4.loss_mask: 0.2012  d4.loss_dice: 1.8452  d5.loss_cls: 0.7143  d5.loss_mask: 0.1981  d5.loss_dice: 1.8162  d6.loss_cls: 0.6968  d6.loss_mask: 0.1969  d6.loss_dice: 1.7852  d7.loss_cls: 0.6689  d7.loss_mask: 0.1983  d7.loss_dice: 1.8068  d8.loss_cls: 0.7242  d8.loss_mask: 0.1951  d8.loss_dice: 1.7829
05/08 10:37:06 - mmengine - INFO - Iter(train) [17800/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 0:55:59  time: 1.4640  data_time: 0.0515  memory: 29058  grad_norm: 68.0333  loss: 27.5882  loss_cls: 0.6584  loss_mask: 0.1801  loss_dice: 1.7464  d0.loss_cls: 1.0944  d0.loss_mask: 0.2200  d0.loss_dice: 2.0165  d1.loss_cls: 0.7558  d1.loss_mask: 0.2071  d1.loss_dice: 2.0119  d2.loss_cls: 0.7494  d2.loss_mask: 0.1959  d2.loss_dice: 1.8831  d3.loss_cls: 0.7685  d3.loss_mask: 0.1868  d3.loss_dice: 1.7609  d4.loss_cls: 0.7081  d4.loss_mask: 0.1875  d4.loss_dice: 1.7847  d5.loss_cls: 0.7031  d5.loss_mask: 0.1834  d5.loss_dice: 1.7617  d6.loss_cls: 0.6849  d6.loss_mask: 0.1839  d6.loss_dice: 1.7315  d7.loss_cls: 0.6621  d7.loss_mask: 0.1839  d7.loss_dice: 1.7495  d8.loss_cls: 0.7232  d8.loss_mask: 0.1805  d8.loss_dice: 1.7249
05/08 10:38:19 - mmengine - INFO - Iter(train) [17850/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 0:54:43  time: 1.4542  data_time: 0.1073  memory: 28277  grad_norm: 81.7409  loss: 28.9858  loss_cls: 0.6765  loss_mask: 0.1917  loss_dice: 1.8742  d0.loss_cls: 1.0876  d0.loss_mask: 0.2300  d0.loss_dice: 2.1591  d1.loss_cls: 0.7521  d1.loss_mask: 0.2180  d1.loss_dice: 2.1445  d2.loss_cls: 0.7485  d2.loss_mask: 0.2057  d2.loss_dice: 1.9997  d3.loss_cls: 0.7626  d3.loss_mask: 0.1979  d3.loss_dice: 1.8840  d4.loss_cls: 0.7073  d4.loss_mask: 0.1986  d4.loss_dice: 1.9111  d5.loss_cls: 0.6981  d5.loss_mask: 0.1961  d5.loss_dice: 1.8912  d6.loss_cls: 0.6782  d6.loss_mask: 0.1952  d6.loss_dice: 1.8699  d7.loss_cls: 0.6680  d7.loss_mask: 0.1936  d7.loss_dice: 1.8753  d8.loss_cls: 0.7349  d8.loss_mask: 0.1896  d8.loss_dice: 1.8466
05/08 10:39:32 - mmengine - INFO - Iter(train) [17900/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 0:53:26  time: 1.4688  data_time: 0.0597  memory: 29312  grad_norm: 73.6905  loss: 28.3093  loss_cls: 0.7144  loss_mask: 0.1854  loss_dice: 1.7843  d0.loss_cls: 1.1045  d0.loss_mask: 0.2261  d0.loss_dice: 2.0790  d1.loss_cls: 0.7627  d1.loss_mask: 0.2131  d1.loss_dice: 2.0647  d2.loss_cls: 0.7556  d2.loss_mask: 0.2026  d2.loss_dice: 1.9263  d3.loss_cls: 0.7714  d3.loss_mask: 0.1939  d3.loss_dice: 1.8086  d4.loss_cls: 0.7178  d4.loss_mask: 0.1939  d4.loss_dice: 1.8382  d5.loss_cls: 0.7170  d5.loss_mask: 0.1900  d5.loss_dice: 1.8162  d6.loss_cls: 0.6846  d6.loss_mask: 0.1897  d6.loss_dice: 1.7911  d7.loss_cls: 0.6892  d7.loss_mask: 0.1885  d7.loss_dice: 1.7962  d8.loss_cls: 0.7638  d8.loss_mask: 0.1843  d8.loss_dice: 1.7562
05/08 10:40:44 - mmengine - INFO - Iter(train) [17950/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 0:52:09  time: 1.4340  data_time: 0.0533  memory: 29025  grad_norm: 66.1915  loss: 29.2183  loss_cls: 0.7358  loss_mask: 0.1948  loss_dice: 1.8476  d0.loss_cls: 1.1068  d0.loss_mask: 0.2347  d0.loss_dice: 2.1519  d1.loss_cls: 0.7597  d1.loss_mask: 0.2213  d1.loss_dice: 2.1405  d2.loss_cls: 0.7570  d2.loss_mask: 0.2117  d2.loss_dice: 1.9955  d3.loss_cls: 0.7691  d3.loss_mask: 0.2034  d3.loss_dice: 1.8934  d4.loss_cls: 0.7258  d4.loss_mask: 0.2038  d4.loss_dice: 1.9190  d5.loss_cls: 0.7296  d5.loss_mask: 0.2005  d5.loss_dice: 1.8863  d6.loss_cls: 0.6945  d6.loss_mask: 0.2008  d6.loss_dice: 1.8694  d7.loss_cls: 0.7203  d7.loss_mask: 0.1979  d7.loss_dice: 1.8673  d8.loss_cls: 0.7684  d8.loss_mask: 0.1937  d8.loss_dice: 1.8178
05/08 10:41:56 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 10:41:56 - mmengine - INFO - Iter(train) [18000/20000]  base_lr: 1.0000e-05 lr: 1.0000e-06  eta: 0:50:52  time: 1.4393  data_time: 0.0604  memory: 30362  grad_norm: 73.3001  loss: 29.2492  loss_cls: 0.7414  loss_mask: 0.1861  loss_dice: 1.8551  d0.loss_cls: 1.1067  d0.loss_mask: 0.2222  d0.loss_dice: 2.1599  d1.loss_cls: 0.7741  d1.loss_mask: 0.2118  d1.loss_dice: 2.1456  d2.loss_cls: 0.7733  d2.loss_mask: 0.2001  d2.loss_dice: 1.9983  d3.loss_cls: 0.7817  d3.loss_mask: 0.1922  d3.loss_dice: 1.8955  d4.loss_cls: 0.7351  d4.loss_mask: 0.1923  d4.loss_dice: 1.9269  d5.loss_cls: 0.7415  d5.loss_mask: 0.1898  d5.loss_dice: 1.8867  d6.loss_cls: 0.7058  d6.loss_mask: 0.1912  d6.loss_dice: 1.8745  d7.loss_cls: 0.7133  d7.loss_mask: 0.1881  d7.loss_dice: 1.8755  d8.loss_cls: 0.7762  d8.loss_mask: 0.1844  d8.loss_dice: 1.8238
05/08 10:41:56 - mmengine - INFO - Saving checkpoint at 18000 iterations
05/08 10:42:48 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9808  data_time: 0.0410  memory: 3258  
05/08 10:43:11 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.31s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 10:43:19 - mmengine - INFO - start multi processing evaluation ...
DONE (t=50.60s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.320
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.621
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.276
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.190
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.365
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.849
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.524
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.875
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.508
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.388
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.589
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.917
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.524
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.524
05/08 10:44:10 - mmengine - INFO - segm_mAP_copypaste: 0.320 0.621 0.276 0.190 0.365 0.849
05/08 10:44:11 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.3200  coco/segm_mAP_50: 0.6210  coco/segm_mAP_75: 0.2760  coco/segm_mAP_s: 0.1900  coco/segm_mAP_m: 0.3650  coco/segm_mAP_l: 0.8490  data_time: 0.0406  time: 0.9784
05/08 10:45:22 - mmengine - INFO - Iter(train) [18050/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:49:44  time: 3.0800  data_time: 1.7089  memory: 29588  grad_norm: 102.9006  loss: 29.0000  loss_cls: 0.7466  loss_mask: 0.1922  loss_dice: 1.8207  d0.loss_cls: 1.1003  d0.loss_mask: 0.2290  d0.loss_dice: 2.1373  d1.loss_cls: 0.7712  d1.loss_mask: 0.2162  d1.loss_dice: 2.1134  d2.loss_cls: 0.7617  d2.loss_mask: 0.2071  d2.loss_dice: 1.9713  d3.loss_cls: 0.7744  d3.loss_mask: 0.1989  d3.loss_dice: 1.8613  d4.loss_cls: 0.7384  d4.loss_mask: 0.1998  d4.loss_dice: 1.9012  d5.loss_cls: 0.7440  d5.loss_mask: 0.1970  d5.loss_dice: 1.8638  d6.loss_cls: 0.7032  d6.loss_mask: 0.1966  d6.loss_dice: 1.8431  d7.loss_cls: 0.7125  d7.loss_mask: 0.1948  d7.loss_dice: 1.8418  d8.loss_cls: 0.7790  d8.loss_mask: 0.1900  d8.loss_dice: 1.7933
05/08 10:46:34 - mmengine - INFO - Iter(train) [18100/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:48:27  time: 1.4412  data_time: 0.0558  memory: 28913  grad_norm: 65.0096  loss: 29.5985  loss_cls: 0.7503  loss_mask: 0.1891  loss_dice: 1.8795  d0.loss_cls: 1.0991  d0.loss_mask: 0.2199  d0.loss_dice: 2.1893  d1.loss_cls: 0.7744  d1.loss_mask: 0.2118  d1.loss_dice: 2.1761  d2.loss_cls: 0.7680  d2.loss_mask: 0.2015  d2.loss_dice: 2.0312  d3.loss_cls: 0.7840  d3.loss_mask: 0.1933  d3.loss_dice: 1.9269  d4.loss_cls: 0.7502  d4.loss_mask: 0.1957  d4.loss_dice: 1.9582  d5.loss_cls: 0.7508  d5.loss_mask: 0.1929  d5.loss_dice: 1.9184  d6.loss_cls: 0.7101  d6.loss_mask: 0.1934  d6.loss_dice: 1.9017  d7.loss_cls: 0.7184  d7.loss_mask: 0.1902  d7.loss_dice: 1.9003  d8.loss_cls: 0.7810  d8.loss_mask: 0.1868  d8.loss_dice: 1.8560
05/08 10:47:46 - mmengine - INFO - Iter(train) [18150/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:47:10  time: 1.4370  data_time: 0.0579  memory: 28051  grad_norm: 79.5782  loss: 27.3206  loss_cls: 0.7325  loss_mask: 0.1795  loss_dice: 1.6891  d0.loss_cls: 1.0741  d0.loss_mask: 0.2140  d0.loss_dice: 1.9734  d1.loss_cls: 0.7432  d1.loss_mask: 0.2023  d1.loss_dice: 1.9509  d2.loss_cls: 0.7509  d2.loss_mask: 0.1934  d2.loss_dice: 1.8284  d3.loss_cls: 0.7720  d3.loss_mask: 0.1864  d3.loss_dice: 1.7271  d4.loss_cls: 0.7302  d4.loss_mask: 0.1861  d4.loss_dice: 1.7559  d5.loss_cls: 0.7315  d5.loss_mask: 0.1844  d5.loss_dice: 1.7256  d6.loss_cls: 0.6943  d6.loss_mask: 0.1842  d6.loss_dice: 1.7064  d7.loss_cls: 0.7039  d7.loss_mask: 0.1832  d7.loss_dice: 1.7113  d8.loss_cls: 0.7550  d8.loss_mask: 0.1794  d8.loss_dice: 1.6722
05/08 10:48:59 - mmengine - INFO - Iter(train) [18200/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:45:53  time: 1.4549  data_time: 0.0561  memory: 28003  grad_norm: 65.8643  loss: 28.2899  loss_cls: 0.7532  loss_mask: 0.1873  loss_dice: 1.7524  d0.loss_cls: 1.0836  d0.loss_mask: 0.2246  d0.loss_dice: 2.0575  d1.loss_cls: 0.7597  d1.loss_mask: 0.2132  d1.loss_dice: 2.0402  d2.loss_cls: 0.7613  d2.loss_mask: 0.2016  d2.loss_dice: 1.8959  d3.loss_cls: 0.7772  d3.loss_mask: 0.1941  d3.loss_dice: 1.8056  d4.loss_cls: 0.7523  d4.loss_mask: 0.1948  d4.loss_dice: 1.8288  d5.loss_cls: 0.7580  d5.loss_mask: 0.1925  d5.loss_dice: 1.7941  d6.loss_cls: 0.7149  d6.loss_mask: 0.1923  d6.loss_dice: 1.7736  d7.loss_cls: 0.7259  d7.loss_mask: 0.1898  d7.loss_dice: 1.7767  d8.loss_cls: 0.7717  d8.loss_mask: 0.1854  d8.loss_dice: 1.7317
05/08 10:50:11 - mmengine - INFO - Iter(train) [18250/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:44:37  time: 1.4561  data_time: 0.1079  memory: 28475  grad_norm: 94.9786  loss: 28.3103  loss_cls: 0.7416  loss_mask: 0.1865  loss_dice: 1.7625  d0.loss_cls: 1.0816  d0.loss_mask: 0.2266  d0.loss_dice: 2.0675  d1.loss_cls: 0.7627  d1.loss_mask: 0.2126  d1.loss_dice: 2.0417  d2.loss_cls: 0.7626  d2.loss_mask: 0.2016  d2.loss_dice: 1.9034  d3.loss_cls: 0.7761  d3.loss_mask: 0.1935  d3.loss_dice: 1.8080  d4.loss_cls: 0.7456  d4.loss_mask: 0.1940  d4.loss_dice: 1.8330  d5.loss_cls: 0.7476  d5.loss_mask: 0.1919  d5.loss_dice: 1.8036  d6.loss_cls: 0.7072  d6.loss_mask: 0.1913  d6.loss_dice: 1.7849  d7.loss_cls: 0.7194  d7.loss_mask: 0.1890  d7.loss_dice: 1.7832  d8.loss_cls: 0.7645  d8.loss_mask: 0.1853  d8.loss_dice: 1.7413
05/08 10:51:23 - mmengine - INFO - Iter(train) [18300/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:43:20  time: 1.4278  data_time: 0.0575  memory: 28423  grad_norm: 77.0367  loss: 29.7276  loss_cls: 0.7470  loss_mask: 0.1969  loss_dice: 1.8857  d0.loss_cls: 1.0909  d0.loss_mask: 0.2327  d0.loss_dice: 2.2047  d1.loss_cls: 0.7692  d1.loss_mask: 0.2229  d1.loss_dice: 2.1766  d2.loss_cls: 0.7635  d2.loss_mask: 0.2124  d2.loss_dice: 2.0312  d3.loss_cls: 0.7847  d3.loss_mask: 0.2036  d3.loss_dice: 1.9297  d4.loss_cls: 0.7503  d4.loss_mask: 0.2053  d4.loss_dice: 1.9619  d5.loss_cls: 0.7452  d5.loss_mask: 0.2025  d5.loss_dice: 1.9288  d6.loss_cls: 0.7137  d6.loss_mask: 0.2020  d6.loss_dice: 1.9085  d7.loss_cls: 0.7207  d7.loss_mask: 0.1995  d7.loss_dice: 1.9127  d8.loss_cls: 0.7677  d8.loss_mask: 0.1951  d8.loss_dice: 1.8619
05/08 10:52:35 - mmengine - INFO - Iter(train) [18350/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:42:03  time: 1.4423  data_time: 0.0543  memory: 29738  grad_norm: 68.4560  loss: 29.9765  loss_cls: 0.7243  loss_mask: 0.2095  loss_dice: 1.9119  d0.loss_cls: 1.1109  d0.loss_mask: 0.2478  d0.loss_dice: 2.2339  d1.loss_cls: 0.7587  d1.loss_mask: 0.2330  d1.loss_dice: 2.2060  d2.loss_cls: 0.7531  d2.loss_mask: 0.2214  d2.loss_dice: 2.0609  d3.loss_cls: 0.7675  d3.loss_mask: 0.2146  d3.loss_dice: 1.9552  d4.loss_cls: 0.7330  d4.loss_mask: 0.2151  d4.loss_dice: 1.9869  d5.loss_cls: 0.7351  d5.loss_mask: 0.2131  d5.loss_dice: 1.9467  d6.loss_cls: 0.7047  d6.loss_mask: 0.2119  d6.loss_dice: 1.9250  d7.loss_cls: 0.7082  d7.loss_mask: 0.2108  d7.loss_dice: 1.9337  d8.loss_cls: 0.7528  d8.loss_mask: 0.2064  d8.loss_dice: 1.8844
05/08 10:53:47 - mmengine - INFO - Iter(train) [18400/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:40:46  time: 1.4445  data_time: 0.0546  memory: 28329  grad_norm: 68.8551  loss: 28.0142  loss_cls: 0.7339  loss_mask: 0.1932  loss_dice: 1.7280  d0.loss_cls: 1.0822  d0.loss_mask: 0.2333  d0.loss_dice: 2.0283  d1.loss_cls: 0.7566  d1.loss_mask: 0.2193  d1.loss_dice: 2.0086  d2.loss_cls: 0.7531  d2.loss_mask: 0.2075  d2.loss_dice: 1.8774  d3.loss_cls: 0.7686  d3.loss_mask: 0.2004  d3.loss_dice: 1.7812  d4.loss_cls: 0.7394  d4.loss_mask: 0.2013  d4.loss_dice: 1.8031  d5.loss_cls: 0.7389  d5.loss_mask: 0.1994  d5.loss_dice: 1.7731  d6.loss_cls: 0.7090  d6.loss_mask: 0.1977  d6.loss_dice: 1.7553  d7.loss_cls: 0.7134  d7.loss_mask: 0.1961  d7.loss_dice: 1.7522  d8.loss_cls: 0.7583  d8.loss_mask: 0.1920  d8.loss_dice: 1.7133
05/08 10:54:59 - mmengine - INFO - Iter(train) [18450/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:39:29  time: 1.4462  data_time: 0.0509  memory: 29181  grad_norm: 82.0775  loss: 29.2061  loss_cls: 0.7426  loss_mask: 0.1996  loss_dice: 1.8260  d0.loss_cls: 1.0939  d0.loss_mask: 0.2419  d0.loss_dice: 2.1460  d1.loss_cls: 0.7701  d1.loss_mask: 0.2262  d1.loss_dice: 2.1161  d2.loss_cls: 0.7656  d2.loss_mask: 0.2154  d2.loss_dice: 1.9832  d3.loss_cls: 0.7816  d3.loss_mask: 0.2072  d3.loss_dice: 1.8809  d4.loss_cls: 0.7506  d4.loss_mask: 0.2084  d4.loss_dice: 1.9048  d5.loss_cls: 0.7531  d5.loss_mask: 0.2061  d5.loss_dice: 1.8665  d6.loss_cls: 0.7152  d6.loss_mask: 0.2055  d6.loss_dice: 1.8493  d7.loss_cls: 0.7197  d7.loss_mask: 0.2021  d7.loss_dice: 1.8521  d8.loss_cls: 0.7665  d8.loss_mask: 0.1992  d8.loss_dice: 1.8107
05/08 10:56:11 - mmengine - INFO - Iter(train) [18500/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:38:12  time: 1.4355  data_time: 0.0517  memory: 28394  grad_norm: 78.3680  loss: 28.0014  loss_cls: 0.7308  loss_mask: 0.1889  loss_dice: 1.7326  d0.loss_cls: 1.0720  d0.loss_mask: 0.2231  d0.loss_dice: 2.0271  d1.loss_cls: 0.7489  d1.loss_mask: 0.2120  d1.loss_dice: 2.0133  d2.loss_cls: 0.7531  d2.loss_mask: 0.2024  d2.loss_dice: 1.8791  d3.loss_cls: 0.7662  d3.loss_mask: 0.1968  d3.loss_dice: 1.7872  d4.loss_cls: 0.7427  d4.loss_mask: 0.1959  d4.loss_dice: 1.8132  d5.loss_cls: 0.7462  d5.loss_mask: 0.1951  d5.loss_dice: 1.7799  d6.loss_cls: 0.7071  d6.loss_mask: 0.1940  d6.loss_dice: 1.7623  d7.loss_cls: 0.7148  d7.loss_mask: 0.1904  d7.loss_dice: 1.7622  d8.loss_cls: 0.7508  d8.loss_mask: 0.1886  d8.loss_dice: 1.7247
05/08 10:57:22 - mmengine - INFO - Iter(train) [18550/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:36:55  time: 1.4209  data_time: 0.0586  memory: 28485  grad_norm: 62.2296  loss: 27.7524  loss_cls: 0.7305  loss_mask: 0.1777  loss_dice: 1.7232  d0.loss_cls: 1.0814  d0.loss_mask: 0.2139  d0.loss_dice: 2.0209  d1.loss_cls: 0.7528  d1.loss_mask: 0.2017  d1.loss_dice: 2.0011  d2.loss_cls: 0.7507  d2.loss_mask: 0.1915  d2.loss_dice: 1.8668  d3.loss_cls: 0.7644  d3.loss_mask: 0.1852  d3.loss_dice: 1.7718  d4.loss_cls: 0.7371  d4.loss_mask: 0.1856  d4.loss_dice: 1.7999  d5.loss_cls: 0.7365  d5.loss_mask: 0.1835  d5.loss_dice: 1.7639  d6.loss_cls: 0.7011  d6.loss_mask: 0.1823  d6.loss_dice: 1.7474  d7.loss_cls: 0.7151  d7.loss_mask: 0.1797  d7.loss_dice: 1.7517  d8.loss_cls: 0.7510  d8.loss_mask: 0.1768  d8.loss_dice: 1.7072
05/08 10:58:34 - mmengine - INFO - Iter(train) [18600/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:35:39  time: 1.4361  data_time: 0.0526  memory: 28658  grad_norm: 71.0420  loss: 28.6555  loss_cls: 0.7432  loss_mask: 0.1871  loss_dice: 1.7978  d0.loss_cls: 1.0856  d0.loss_mask: 0.2247  d0.loss_dice: 2.1032  d1.loss_cls: 0.7519  d1.loss_mask: 0.2127  d1.loss_dice: 2.0833  d2.loss_cls: 0.7512  d2.loss_mask: 0.2017  d2.loss_dice: 1.9432  d3.loss_cls: 0.7645  d3.loss_mask: 0.1954  d3.loss_dice: 1.8504  d4.loss_cls: 0.7420  d4.loss_mask: 0.1946  d4.loss_dice: 1.8730  d5.loss_cls: 0.7407  d5.loss_mask: 0.1939  d5.loss_dice: 1.8405  d6.loss_cls: 0.7081  d6.loss_mask: 0.1926  d6.loss_dice: 1.8206  d7.loss_cls: 0.7197  d7.loss_mask: 0.1894  d7.loss_dice: 1.8231  d8.loss_cls: 0.7486  d8.loss_mask: 0.1864  d8.loss_dice: 1.7864
05/08 10:59:48 - mmengine - INFO - Iter(train) [18650/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:34:22  time: 1.4806  data_time: 0.1015  memory: 29433  grad_norm: 85.0345  loss: 28.5192  loss_cls: 0.7414  loss_mask: 0.1810  loss_dice: 1.7838  d0.loss_cls: 1.0989  d0.loss_mask: 0.2173  d0.loss_dice: 2.1008  d1.loss_cls: 0.7528  d1.loss_mask: 0.2078  d1.loss_dice: 2.0756  d2.loss_cls: 0.7513  d2.loss_mask: 0.1975  d2.loss_dice: 1.9438  d3.loss_cls: 0.7650  d3.loss_mask: 0.1886  d3.loss_dice: 1.8376  d4.loss_cls: 0.7391  d4.loss_mask: 0.1897  d4.loss_dice: 1.8656  d5.loss_cls: 0.7458  d5.loss_mask: 0.1873  d5.loss_dice: 1.8279  d6.loss_cls: 0.7099  d6.loss_mask: 0.1873  d6.loss_dice: 1.8101  d7.loss_cls: 0.7148  d7.loss_mask: 0.1843  d7.loss_dice: 1.8061  d8.loss_cls: 0.7515  d8.loss_mask: 0.1811  d8.loss_dice: 1.7757
05/08 11:01:01 - mmengine - INFO - Iter(train) [18700/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:33:05  time: 1.4610  data_time: 0.0651  memory: 28965  grad_norm: 68.6142  loss: 29.3855  loss_cls: 0.7544  loss_mask: 0.1825  loss_dice: 1.8589  d0.loss_cls: 1.1103  d0.loss_mask: 0.2200  d0.loss_dice: 2.1742  d1.loss_cls: 0.7666  d1.loss_mask: 0.2092  d1.loss_dice: 2.1578  d2.loss_cls: 0.7631  d2.loss_mask: 0.1976  d2.loss_dice: 2.0176  d3.loss_cls: 0.7776  d3.loss_mask: 0.1902  d3.loss_dice: 1.9138  d4.loss_cls: 0.7499  d4.loss_mask: 0.1913  d4.loss_dice: 1.9398  d5.loss_cls: 0.7567  d5.loss_mask: 0.1882  d5.loss_dice: 1.9000  d6.loss_cls: 0.7131  d6.loss_mask: 0.1883  d6.loss_dice: 1.8823  d7.loss_cls: 0.7252  d7.loss_mask: 0.1862  d7.loss_dice: 1.8821  d8.loss_cls: 0.7566  d8.loss_mask: 0.1837  d8.loss_dice: 1.8482
05/08 11:02:13 - mmengine - INFO - Iter(train) [18750/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:31:49  time: 1.4327  data_time: 0.0529  memory: 29345  grad_norm: 71.3499  loss: 28.1810  loss_cls: 0.7537  loss_mask: 0.1873  loss_dice: 1.7359  d0.loss_cls: 1.0911  d0.loss_mask: 0.2257  d0.loss_dice: 2.0525  d1.loss_cls: 0.7619  d1.loss_mask: 0.2151  d1.loss_dice: 2.0308  d2.loss_cls: 0.7598  d2.loss_mask: 0.2043  d2.loss_dice: 1.8960  d3.loss_cls: 0.7723  d3.loss_mask: 0.1959  d3.loss_dice: 1.7995  d4.loss_cls: 0.7514  d4.loss_mask: 0.1949  d4.loss_dice: 1.8159  d5.loss_cls: 0.7584  d5.loss_mask: 0.1923  d5.loss_dice: 1.7799  d6.loss_cls: 0.7056  d6.loss_mask: 0.1931  d6.loss_dice: 1.7654  d7.loss_cls: 0.7147  d7.loss_mask: 0.1905  d7.loss_dice: 1.7632  d8.loss_cls: 0.7528  d8.loss_mask: 0.1878  d8.loss_dice: 1.7334
05/08 11:03:26 - mmengine - INFO - Iter(train) [18800/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:30:32  time: 1.4655  data_time: 0.0675  memory: 29164  grad_norm: 66.8585  loss: 28.0272  loss_cls: 0.7464  loss_mask: 0.1768  loss_dice: 1.7427  d0.loss_cls: 1.0985  d0.loss_mask: 0.2118  d0.loss_dice: 2.0464  d1.loss_cls: 0.7656  d1.loss_mask: 0.2007  d1.loss_dice: 2.0250  d2.loss_cls: 0.7574  d2.loss_mask: 0.1902  d2.loss_dice: 1.8883  d3.loss_cls: 0.7680  d3.loss_mask: 0.1835  d3.loss_dice: 1.7942  d4.loss_cls: 0.7490  d4.loss_mask: 0.1833  d4.loss_dice: 1.8129  d5.loss_cls: 0.7564  d5.loss_mask: 0.1814  d5.loss_dice: 1.7801  d6.loss_cls: 0.7090  d6.loss_mask: 0.1813  d6.loss_dice: 1.7618  d7.loss_cls: 0.7122  d7.loss_mask: 0.1797  d7.loss_dice: 1.7695  d8.loss_cls: 0.7469  d8.loss_mask: 0.1768  d8.loss_dice: 1.7319
05/08 11:04:37 - mmengine - INFO - Iter(train) [18850/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:29:15  time: 1.4181  data_time: 0.0554  memory: 27741  grad_norm: 78.5555  loss: 27.9003  loss_cls: 0.7391  loss_mask: 0.1800  loss_dice: 1.7318  d0.loss_cls: 1.0767  d0.loss_mask: 0.2169  d0.loss_dice: 2.0348  d1.loss_cls: 0.7519  d1.loss_mask: 0.2067  d1.loss_dice: 2.0089  d2.loss_cls: 0.7499  d2.loss_mask: 0.1967  d2.loss_dice: 1.8819  d3.loss_cls: 0.7674  d3.loss_mask: 0.1890  d3.loss_dice: 1.7801  d4.loss_cls: 0.7431  d4.loss_mask: 0.1891  d4.loss_dice: 1.8061  d5.loss_cls: 0.7494  d5.loss_mask: 0.1868  d5.loss_dice: 1.7676  d6.loss_cls: 0.7017  d6.loss_mask: 0.1863  d6.loss_dice: 1.7562  d7.loss_cls: 0.7128  d7.loss_mask: 0.1836  d7.loss_dice: 1.7570  d8.loss_cls: 0.7374  d8.loss_mask: 0.1813  d8.loss_dice: 1.7304
05/08 11:05:50 - mmengine - INFO - Iter(train) [18900/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:27:59  time: 1.4606  data_time: 0.0591  memory: 29168  grad_norm: 77.8407  loss: 28.7479  loss_cls: 0.7562  loss_mask: 0.1881  loss_dice: 1.7885  d0.loss_cls: 1.1017  d0.loss_mask: 0.2282  d0.loss_dice: 2.1096  d1.loss_cls: 0.7678  d1.loss_mask: 0.2164  d1.loss_dice: 2.0850  d2.loss_cls: 0.7589  d2.loss_mask: 0.2049  d2.loss_dice: 1.9433  d3.loss_cls: 0.7706  d3.loss_mask: 0.1965  d3.loss_dice: 1.8528  d4.loss_cls: 0.7516  d4.loss_mask: 0.1970  d4.loss_dice: 1.8700  d5.loss_cls: 0.7584  d5.loss_mask: 0.1940  d5.loss_dice: 1.8297  d6.loss_cls: 0.7140  d6.loss_mask: 0.1939  d6.loss_dice: 1.8124  d7.loss_cls: 0.7250  d7.loss_mask: 0.1915  d7.loss_dice: 1.8129  d8.loss_cls: 0.7502  d8.loss_mask: 0.1896  d8.loss_dice: 1.7891
05/08 11:07:02 - mmengine - INFO - Iter(train) [18950/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:26:42  time: 1.4460  data_time: 0.0627  memory: 28153  grad_norm: 77.7849  loss: 28.1788  loss_cls: 0.7638  loss_mask: 0.1792  loss_dice: 1.7437  d0.loss_cls: 1.0862  d0.loss_mask: 0.2177  d0.loss_dice: 2.0649  d1.loss_cls: 0.7638  d1.loss_mask: 0.2042  d1.loss_dice: 2.0347  d2.loss_cls: 0.7655  d2.loss_mask: 0.1937  d2.loss_dice: 1.8958  d3.loss_cls: 0.7718  d3.loss_mask: 0.1877  d3.loss_dice: 1.8005  d4.loss_cls: 0.7657  d4.loss_mask: 0.1864  d4.loss_dice: 1.8133  d5.loss_cls: 0.7685  d5.loss_mask: 0.1851  d5.loss_dice: 1.7805  d6.loss_cls: 0.7149  d6.loss_mask: 0.1846  d6.loss_dice: 1.7626  d7.loss_cls: 0.7271  d7.loss_mask: 0.1818  d7.loss_dice: 1.7644  d8.loss_cls: 0.7504  d8.loss_mask: 0.1801  d8.loss_dice: 1.7403
05/08 11:08:14 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 11:08:14 - mmengine - INFO - Iter(train) [19000/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:25:26  time: 1.4342  data_time: 0.0570  memory: 28744  grad_norm: 74.3245  loss: 28.6007  loss_cls: 0.7456  loss_mask: 0.1860  loss_dice: 1.7802  d0.loss_cls: 1.0892  d0.loss_mask: 0.2240  d0.loss_dice: 2.0981  d1.loss_cls: 0.7606  d1.loss_mask: 0.2122  d1.loss_dice: 2.0716  d2.loss_cls: 0.7585  d2.loss_mask: 0.2018  d2.loss_dice: 1.9367  d3.loss_cls: 0.7723  d3.loss_mask: 0.1942  d3.loss_dice: 1.8462  d4.loss_cls: 0.7563  d4.loss_mask: 0.1935  d4.loss_dice: 1.8593  d5.loss_cls: 0.7592  d5.loss_mask: 0.1916  d5.loss_dice: 1.8219  d6.loss_cls: 0.7113  d6.loss_mask: 0.1906  d6.loss_dice: 1.8071  d7.loss_cls: 0.7215  d7.loss_mask: 0.1886  d7.loss_dice: 1.8125  d8.loss_cls: 0.7407  d8.loss_mask: 0.1875  d8.loss_dice: 1.7819
05/08 11:08:14 - mmengine - INFO - Saving checkpoint at 19000 iterations
05/08 11:09:06 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9804  data_time: 0.0277  memory: 3258  
05/08 11:09:30 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.35s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 11:09:37 - mmengine - INFO - start multi processing evaluation ...
DONE (t=52.33s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.326
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.623
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.281
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.190
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.370
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.875
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.533
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.875
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.531
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.390
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.603
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.917
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.533
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.533
05/08 11:10:29 - mmengine - INFO - segm_mAP_copypaste: 0.326 0.623 0.281 0.190 0.370 0.875
05/08 11:10:31 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.3260  coco/segm_mAP_50: 0.6230  coco/segm_mAP_75: 0.2810  coco/segm_mAP_s: 0.1900  coco/segm_mAP_m: 0.3700  coco/segm_mAP_l: 0.8750  data_time: 0.0276  time: 0.9781
05/08 11:11:44 - mmengine - INFO - Iter(train) [19050/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:24:14  time: 3.1559  data_time: 1.7983  memory: 29900  grad_norm: 75.7447  loss: 29.4812  loss_cls: 0.7541  loss_mask: 0.1925  loss_dice: 1.8550  d0.loss_cls: 1.1077  d0.loss_mask: 0.2299  d0.loss_dice: 2.1663  d1.loss_cls: 0.7675  d1.loss_mask: 0.2187  d1.loss_dice: 2.1490  d2.loss_cls: 0.7676  d2.loss_mask: 0.2093  d2.loss_dice: 2.0060  d3.loss_cls: 0.7768  d3.loss_mask: 0.2030  d3.loss_dice: 1.9170  d4.loss_cls: 0.7670  d4.loss_mask: 0.2013  d4.loss_dice: 1.9295  d5.loss_cls: 0.7654  d5.loss_mask: 0.1979  d5.loss_dice: 1.8964  d6.loss_cls: 0.7180  d6.loss_mask: 0.1980  d6.loss_dice: 1.8795  d7.loss_cls: 0.7320  d7.loss_mask: 0.1942  d7.loss_dice: 1.8800  d8.loss_cls: 0.7534  d8.loss_mask: 0.1945  d8.loss_dice: 1.8538
05/08 11:12:56 - mmengine - INFO - Iter(train) [19100/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:22:57  time: 1.4470  data_time: 0.0535  memory: 30405  grad_norm: 73.4234  loss: 29.1632  loss_cls: 0.7581  loss_mask: 0.1877  loss_dice: 1.8271  d0.loss_cls: 1.1001  d0.loss_mask: 0.2217  d0.loss_dice: 2.1452  d1.loss_cls: 0.7703  d1.loss_mask: 0.2140  d1.loss_dice: 2.1225  d2.loss_cls: 0.7674  d2.loss_mask: 0.2024  d2.loss_dice: 1.9794  d3.loss_cls: 0.7860  d3.loss_mask: 0.1954  d3.loss_dice: 1.8827  d4.loss_cls: 0.7663  d4.loss_mask: 0.1951  d4.loss_dice: 1.9053  d5.loss_cls: 0.7707  d5.loss_mask: 0.1943  d5.loss_dice: 1.8668  d6.loss_cls: 0.7207  d6.loss_mask: 0.1939  d6.loss_dice: 1.8518  d7.loss_cls: 0.7299  d7.loss_mask: 0.1917  d7.loss_dice: 1.8496  d8.loss_cls: 0.7522  d8.loss_mask: 0.1901  d8.loss_dice: 1.8246
05/08 11:14:07 - mmengine - INFO - Iter(train) [19150/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:21:40  time: 1.4183  data_time: 0.0553  memory: 28504  grad_norm: 73.2026  loss: 28.8836  loss_cls: 0.7471  loss_mask: 0.1844  loss_dice: 1.8206  d0.loss_cls: 1.0898  d0.loss_mask: 0.2204  d0.loss_dice: 2.1272  d1.loss_cls: 0.7560  d1.loss_mask: 0.2111  d1.loss_dice: 2.1026  d2.loss_cls: 0.7583  d2.loss_mask: 0.1989  d2.loss_dice: 1.9579  d3.loss_cls: 0.7693  d3.loss_mask: 0.1924  d3.loss_dice: 1.8723  d4.loss_cls: 0.7593  d4.loss_mask: 0.1928  d4.loss_dice: 1.8896  d5.loss_cls: 0.7639  d5.loss_mask: 0.1907  d5.loss_dice: 1.8485  d6.loss_cls: 0.7081  d6.loss_mask: 0.1894  d6.loss_dice: 1.8449  d7.loss_cls: 0.7208  d7.loss_mask: 0.1857  d7.loss_dice: 1.8389  d8.loss_cls: 0.7375  d8.loss_mask: 0.1860  d8.loss_dice: 1.8193
05/08 11:15:20 - mmengine - INFO - Iter(train) [19200/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:20:23  time: 1.4518  data_time: 0.0599  memory: 28890  grad_norm: 77.0278  loss: 29.1253  loss_cls: 0.7570  loss_mask: 0.1920  loss_dice: 1.8143  d0.loss_cls: 1.1041  d0.loss_mask: 0.2336  d0.loss_dice: 2.1276  d1.loss_cls: 0.7654  d1.loss_mask: 0.2209  d1.loss_dice: 2.1130  d2.loss_cls: 0.7644  d2.loss_mask: 0.2087  d2.loss_dice: 1.9691  d3.loss_cls: 0.7734  d3.loss_mask: 0.2017  d3.loss_dice: 1.8905  d4.loss_cls: 0.7589  d4.loss_mask: 0.2002  d4.loss_dice: 1.8995  d5.loss_cls: 0.7711  d5.loss_mask: 0.1984  d5.loss_dice: 1.8606  d6.loss_cls: 0.7167  d6.loss_mask: 0.1989  d6.loss_dice: 1.8493  d7.loss_cls: 0.7280  d7.loss_mask: 0.1952  d7.loss_dice: 1.8474  d8.loss_cls: 0.7456  d8.loss_mask: 0.1944  d8.loss_dice: 1.8254
05/08 11:16:32 - mmengine - INFO - Iter(train) [19250/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:19:07  time: 1.4391  data_time: 0.0573  memory: 28955  grad_norm: 72.2393  loss: 28.5691  loss_cls: 0.7485  loss_mask: 0.1846  loss_dice: 1.7757  d0.loss_cls: 1.0882  d0.loss_mask: 0.2248  d0.loss_dice: 2.0938  d1.loss_cls: 0.7623  d1.loss_mask: 0.2109  d1.loss_dice: 2.0722  d2.loss_cls: 0.7647  d2.loss_mask: 0.1999  d2.loss_dice: 1.9345  d3.loss_cls: 0.7749  d3.loss_mask: 0.1934  d3.loss_dice: 1.8356  d4.loss_cls: 0.7612  d4.loss_mask: 0.1926  d4.loss_dice: 1.8479  d5.loss_cls: 0.7626  d5.loss_mask: 0.1908  d5.loss_dice: 1.8147  d6.loss_cls: 0.7114  d6.loss_mask: 0.1919  d6.loss_dice: 1.8076  d7.loss_cls: 0.7213  d7.loss_mask: 0.1879  d7.loss_dice: 1.8024  d8.loss_cls: 0.7362  d8.loss_mask: 0.1879  d8.loss_dice: 1.7884
05/08 11:17:43 - mmengine - INFO - Iter(train) [19300/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:17:50  time: 1.4215  data_time: 0.0643  memory: 29074  grad_norm: 68.7673  loss: 29.0988  loss_cls: 0.7601  loss_mask: 0.1904  loss_dice: 1.8154  d0.loss_cls: 1.1069  d0.loss_mask: 0.2295  d0.loss_dice: 2.1443  d1.loss_cls: 0.7743  d1.loss_mask: 0.2175  d1.loss_dice: 2.1173  d2.loss_cls: 0.7718  d2.loss_mask: 0.2051  d2.loss_dice: 1.9707  d3.loss_cls: 0.7771  d3.loss_mask: 0.1977  d3.loss_dice: 1.8719  d4.loss_cls: 0.7651  d4.loss_mask: 0.1972  d4.loss_dice: 1.8864  d5.loss_cls: 0.7747  d5.loss_mask: 0.1952  d5.loss_dice: 1.8520  d6.loss_cls: 0.7172  d6.loss_mask: 0.1960  d6.loss_dice: 1.8405  d7.loss_cls: 0.7346  d7.loss_mask: 0.1923  d7.loss_dice: 1.8400  d8.loss_cls: 0.7460  d8.loss_mask: 0.1920  d8.loss_dice: 1.8197
05/08 11:18:54 - mmengine - INFO - Iter(train) [19350/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:16:33  time: 1.4271  data_time: 0.0584  memory: 28289  grad_norm: 74.3036  loss: 28.5284  loss_cls: 0.7388  loss_mask: 0.1840  loss_dice: 1.7825  d0.loss_cls: 1.0902  d0.loss_mask: 0.2214  d0.loss_dice: 2.0966  d1.loss_cls: 0.7494  d1.loss_mask: 0.2107  d1.loss_dice: 2.0771  d2.loss_cls: 0.7540  d2.loss_mask: 0.1978  d2.loss_dice: 1.9339  d3.loss_cls: 0.7597  d3.loss_mask: 0.1920  d3.loss_dice: 1.8465  d4.loss_cls: 0.7429  d4.loss_mask: 0.1912  d4.loss_dice: 1.8612  d5.loss_cls: 0.7493  d5.loss_mask: 0.1895  d5.loss_dice: 1.8238  d6.loss_cls: 0.7007  d6.loss_mask: 0.1904  d6.loss_dice: 1.8211  d7.loss_cls: 0.7133  d7.loss_mask: 0.1867  d7.loss_dice: 1.8119  d8.loss_cls: 0.7262  d8.loss_mask: 0.1872  d8.loss_dice: 1.7988
05/08 11:20:05 - mmengine - INFO - Iter(train) [19400/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:15:17  time: 1.4258  data_time: 0.0599  memory: 28213  grad_norm: 77.1470  loss: 27.8921  loss_cls: 0.7388  loss_mask: 0.1787  loss_dice: 1.7308  d0.loss_cls: 1.0775  d0.loss_mask: 0.2138  d0.loss_dice: 2.0268  d1.loss_cls: 0.7554  d1.loss_mask: 0.2035  d1.loss_dice: 2.0061  d2.loss_cls: 0.7543  d2.loss_mask: 0.1935  d2.loss_dice: 1.8697  d3.loss_cls: 0.7654  d3.loss_mask: 0.1862  d3.loss_dice: 1.7902  d4.loss_cls: 0.7531  d4.loss_mask: 0.1862  d4.loss_dice: 1.8017  d5.loss_cls: 0.7584  d5.loss_mask: 0.1845  d5.loss_dice: 1.7657  d6.loss_cls: 0.7011  d6.loss_mask: 0.1851  d6.loss_dice: 1.7651  d7.loss_cls: 0.7151  d7.loss_mask: 0.1815  d7.loss_dice: 1.7554  d8.loss_cls: 0.7233  d8.loss_mask: 0.1817  d8.loss_dice: 1.7437
05/08 11:21:17 - mmengine - INFO - Iter(train) [19450/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:14:00  time: 1.4384  data_time: 0.1044  memory: 29561  grad_norm: 81.8717  loss: 28.9622  loss_cls: 0.7399  loss_mask: 0.1868  loss_dice: 1.8257  d0.loss_cls: 1.0997  d0.loss_mask: 0.2258  d0.loss_dice: 2.1398  d1.loss_cls: 0.7472  d1.loss_mask: 0.2138  d1.loss_dice: 2.1193  d2.loss_cls: 0.7504  d2.loss_mask: 0.2023  d2.loss_dice: 1.9667  d3.loss_cls: 0.7578  d3.loss_mask: 0.1949  d3.loss_dice: 1.8878  d4.loss_cls: 0.7484  d4.loss_mask: 0.1948  d4.loss_dice: 1.8975  d5.loss_cls: 0.7540  d5.loss_mask: 0.1929  d5.loss_dice: 1.8611  d6.loss_cls: 0.7028  d6.loss_mask: 0.1938  d6.loss_dice: 1.8519  d7.loss_cls: 0.7195  d7.loss_mask: 0.1895  d7.loss_dice: 1.8492  d8.loss_cls: 0.7246  d8.loss_mask: 0.1906  d8.loss_dice: 1.8337
05/08 11:22:29 - mmengine - INFO - Iter(train) [19500/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:12:44  time: 1.4308  data_time: 0.0614  memory: 29164  grad_norm: 87.7678  loss: 30.0638  loss_cls: 0.7584  loss_mask: 0.1843  loss_dice: 1.9091  d0.loss_cls: 1.1134  d0.loss_mask: 0.2228  d0.loss_dice: 2.2540  d1.loss_cls: 0.7694  d1.loss_mask: 0.2138  d1.loss_dice: 2.2267  d2.loss_cls: 0.7748  d2.loss_mask: 0.2010  d2.loss_dice: 2.0843  d3.loss_cls: 0.7784  d3.loss_mask: 0.1939  d3.loss_dice: 1.9769  d4.loss_cls: 0.7634  d4.loss_mask: 0.1932  d4.loss_dice: 1.9896  d5.loss_cls: 0.7700  d5.loss_mask: 0.1912  d5.loss_dice: 1.9486  d6.loss_cls: 0.7175  d6.loss_mask: 0.1919  d6.loss_dice: 1.9354  d7.loss_cls: 0.7291  d7.loss_mask: 0.1884  d7.loss_dice: 1.9403  d8.loss_cls: 0.7383  d8.loss_mask: 0.1884  d8.loss_dice: 1.9172
05/08 11:23:40 - mmengine - INFO - Iter(train) [19550/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:11:27  time: 1.4293  data_time: 0.0616  memory: 28245  grad_norm: 74.7164  loss: 27.6204  loss_cls: 0.7586  loss_mask: 0.1750  loss_dice: 1.6995  d0.loss_cls: 1.0680  d0.loss_mask: 0.2115  d0.loss_dice: 1.9993  d1.loss_cls: 0.7491  d1.loss_mask: 0.2008  d1.loss_dice: 1.9837  d2.loss_cls: 0.7454  d2.loss_mask: 0.1902  d2.loss_dice: 1.8533  d3.loss_cls: 0.7604  d3.loss_mask: 0.1847  d3.loss_dice: 1.7669  d4.loss_cls: 0.7495  d4.loss_mask: 0.1835  d4.loss_dice: 1.7788  d5.loss_cls: 0.7550  d5.loss_mask: 0.1824  d5.loss_dice: 1.7438  d6.loss_cls: 0.7034  d6.loss_mask: 0.1827  d6.loss_dice: 1.7358  d7.loss_cls: 0.7291  d7.loss_mask: 0.1777  d7.loss_dice: 1.7253  d8.loss_cls: 0.7206  d8.loss_mask: 0.1805  d8.loss_dice: 1.7257
05/08 11:24:50 - mmengine - INFO - Iter(train) [19600/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:10:11  time: 1.4043  data_time: 0.0560  memory: 29466  grad_norm: 119.0409  loss: 30.0701  loss_cls: 0.7637  loss_mask: 0.1916  loss_dice: 1.9080  d0.loss_cls: 1.1066  d0.loss_mask: 0.2249  d0.loss_dice: 2.2334  d1.loss_cls: 0.7583  d1.loss_mask: 0.2194  d1.loss_dice: 2.2131  d2.loss_cls: 0.7677  d2.loss_mask: 0.2096  d2.loss_dice: 2.0751  d3.loss_cls: 0.7789  d3.loss_mask: 0.2012  d3.loss_dice: 1.9763  d4.loss_cls: 0.7601  d4.loss_mask: 0.2010  d4.loss_dice: 1.9907  d5.loss_cls: 0.7698  d5.loss_mask: 0.1987  d5.loss_dice: 1.9487  d6.loss_cls: 0.7116  d6.loss_mask: 0.2000  d6.loss_dice: 1.9393  d7.loss_cls: 0.7330  d7.loss_mask: 0.1956  d7.loss_dice: 1.9318  d8.loss_cls: 0.7357  d8.loss_mask: 0.1967  d8.loss_dice: 1.9293
05/08 11:26:02 - mmengine - INFO - Iter(train) [19650/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:08:54  time: 1.4310  data_time: 0.0636  memory: 28781  grad_norm: 79.1341  loss: 28.1299  loss_cls: 0.7536  loss_mask: 0.1782  loss_dice: 1.7477  d0.loss_cls: 1.0824  d0.loss_mask: 0.2154  d0.loss_dice: 2.0569  d1.loss_cls: 0.7535  d1.loss_mask: 0.2044  d1.loss_dice: 2.0324  d2.loss_cls: 0.7559  d2.loss_mask: 0.1935  d2.loss_dice: 1.8960  d3.loss_cls: 0.7646  d3.loss_mask: 0.1871  d3.loss_dice: 1.8091  d4.loss_cls: 0.7433  d4.loss_mask: 0.1870  d4.loss_dice: 1.8261  d5.loss_cls: 0.7582  d5.loss_mask: 0.1848  d5.loss_dice: 1.7857  d6.loss_cls: 0.7006  d6.loss_mask: 0.1858  d6.loss_dice: 1.7815  d7.loss_cls: 0.7234  d7.loss_mask: 0.1805  d7.loss_dice: 1.7689  d8.loss_cls: 0.7242  d8.loss_mask: 0.1829  d8.loss_dice: 1.7664
05/08 11:27:14 - mmengine - INFO - Iter(train) [19700/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:07:38  time: 1.4368  data_time: 0.0567  memory: 28543  grad_norm: 72.8139  loss: 29.2448  loss_cls: 0.7765  loss_mask: 0.1910  loss_dice: 1.8158  d0.loss_cls: 1.1045  d0.loss_mask: 0.2268  d0.loss_dice: 2.1330  d1.loss_cls: 0.7795  d1.loss_mask: 0.2180  d1.loss_dice: 2.1143  d2.loss_cls: 0.7787  d2.loss_mask: 0.2071  d2.loss_dice: 1.9809  d3.loss_cls: 0.7856  d3.loss_mask: 0.1992  d3.loss_dice: 1.8874  d4.loss_cls: 0.7685  d4.loss_mask: 0.1994  d4.loss_dice: 1.8968  d5.loss_cls: 0.7810  d5.loss_mask: 0.1975  d5.loss_dice: 1.8546  d6.loss_cls: 0.7319  d6.loss_mask: 0.1983  d6.loss_dice: 1.8529  d7.loss_cls: 0.7502  d7.loss_mask: 0.1944  d7.loss_dice: 1.8419  d8.loss_cls: 0.7445  d8.loss_mask: 0.1948  d8.loss_dice: 1.8399
05/08 11:28:25 - mmengine - INFO - Iter(train) [19750/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:06:21  time: 1.4282  data_time: 0.0622  memory: 28796  grad_norm: 95.9590  loss: 30.3640  loss_cls: 0.7800  loss_mask: 0.1944  loss_dice: 1.9200  d0.loss_cls: 1.1199  d0.loss_mask: 0.2330  d0.loss_dice: 2.2429  d1.loss_cls: 0.7757  d1.loss_mask: 0.2245  d1.loss_dice: 2.2358  d2.loss_cls: 0.7729  d2.loss_mask: 0.2128  d2.loss_dice: 2.0874  d3.loss_cls: 0.7831  d3.loss_mask: 0.2051  d3.loss_dice: 1.9946  d4.loss_cls: 0.7713  d4.loss_mask: 0.2048  d4.loss_dice: 2.0006  d5.loss_cls: 0.7844  d5.loss_mask: 0.2014  d5.loss_dice: 1.9597  d6.loss_cls: 0.7274  d6.loss_mask: 0.2027  d6.loss_dice: 1.9483  d7.loss_cls: 0.7527  d7.loss_mask: 0.1977  d7.loss_dice: 1.9415  d8.loss_cls: 0.7485  d8.loss_mask: 0.1990  d8.loss_dice: 1.9422
05/08 11:29:37 - mmengine - INFO - Iter(train) [19800/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:05:05  time: 1.4296  data_time: 0.0571  memory: 28988  grad_norm: 73.2785  loss: 28.4308  loss_cls: 0.7522  loss_mask: 0.1913  loss_dice: 1.7588  d0.loss_cls: 1.0966  d0.loss_mask: 0.2326  d0.loss_dice: 2.0773  d1.loss_cls: 0.7539  d1.loss_mask: 0.2184  d1.loss_dice: 2.0448  d2.loss_cls: 0.7541  d2.loss_mask: 0.2073  d2.loss_dice: 1.9168  d3.loss_cls: 0.7607  d3.loss_mask: 0.1996  d3.loss_dice: 1.8274  d4.loss_cls: 0.7521  d4.loss_mask: 0.1998  d4.loss_dice: 1.8343  d5.loss_cls: 0.7575  d5.loss_mask: 0.1972  d5.loss_dice: 1.7965  d6.loss_cls: 0.6999  d6.loss_mask: 0.1987  d6.loss_dice: 1.7952  d7.loss_cls: 0.7252  d7.loss_mask: 0.1936  d7.loss_dice: 1.7879  d8.loss_cls: 0.7192  d8.loss_mask: 0.1954  d8.loss_dice: 1.7865
05/08 11:30:50 - mmengine - INFO - Iter(train) [19850/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:03:48  time: 1.4647  data_time: 0.1108  memory: 28642  grad_norm: 72.3877  loss: 28.6918  loss_cls: 0.7652  loss_mask: 0.1910  loss_dice: 1.7664  d0.loss_cls: 1.0912  d0.loss_mask: 0.2328  d0.loss_dice: 2.0874  d1.loss_cls: 0.7603  d1.loss_mask: 0.2182  d1.loss_dice: 2.0705  d2.loss_cls: 0.7613  d2.loss_mask: 0.2087  d2.loss_dice: 1.9290  d3.loss_cls: 0.7738  d3.loss_mask: 0.2014  d3.loss_dice: 1.8427  d4.loss_cls: 0.7614  d4.loss_mask: 0.2013  d4.loss_dice: 1.8489  d5.loss_cls: 0.7767  d5.loss_mask: 0.1980  d5.loss_dice: 1.8120  d6.loss_cls: 0.7172  d6.loss_mask: 0.2005  d6.loss_dice: 1.8098  d7.loss_cls: 0.7367  d7.loss_mask: 0.1943  d7.loss_dice: 1.7981  d8.loss_cls: 0.7339  d8.loss_mask: 0.1982  d8.loss_dice: 1.8049
05/08 11:32:02 - mmengine - INFO - Iter(train) [19900/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:02:32  time: 1.4405  data_time: 0.0607  memory: 28314  grad_norm: 75.8915  loss: 29.2067  loss_cls: 0.7620  loss_mask: 0.1940  loss_dice: 1.8177  d0.loss_cls: 1.1023  d0.loss_mask: 0.2353  d0.loss_dice: 2.1509  d1.loss_cls: 0.7638  d1.loss_mask: 0.2216  d1.loss_dice: 2.1242  d2.loss_cls: 0.7699  d2.loss_mask: 0.2100  d2.loss_dice: 1.9822  d3.loss_cls: 0.7719  d3.loss_mask: 0.2027  d3.loss_dice: 1.8909  d4.loss_cls: 0.7649  d4.loss_mask: 0.2011  d4.loss_dice: 1.8912  d5.loss_cls: 0.7735  d5.loss_mask: 0.2007  d5.loss_dice: 1.8580  d6.loss_cls: 0.7130  d6.loss_mask: 0.2011  d6.loss_dice: 1.8536  d7.loss_cls: 0.7327  d7.loss_mask: 0.1968  d7.loss_dice: 1.8489  d8.loss_cls: 0.7283  d8.loss_mask: 0.1982  d8.loss_dice: 1.8452
05/08 11:33:14 - mmengine - INFO - Iter(train) [19950/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:01:16  time: 1.4395  data_time: 0.0626  memory: 28394  grad_norm: 68.2011  loss: 28.6654  loss_cls: 0.7568  loss_mask: 0.1865  loss_dice: 1.7822  d0.loss_cls: 1.0938  d0.loss_mask: 0.2251  d0.loss_dice: 2.0950  d1.loss_cls: 0.7560  d1.loss_mask: 0.2154  d1.loss_dice: 2.0803  d2.loss_cls: 0.7552  d2.loss_mask: 0.2043  d2.loss_dice: 1.9409  d3.loss_cls: 0.7656  d3.loss_mask: 0.1965  d3.loss_dice: 1.8507  d4.loss_cls: 0.7546  d4.loss_mask: 0.1958  d4.loss_dice: 1.8608  d5.loss_cls: 0.7619  d5.loss_mask: 0.1939  d5.loss_dice: 1.8294  d6.loss_cls: 0.7018  d6.loss_mask: 0.1941  d6.loss_dice: 1.8179  d7.loss_cls: 0.7267  d7.loss_mask: 0.1890  d7.loss_dice: 1.8088  d8.loss_cls: 0.7232  d8.loss_mask: 0.1911  d8.loss_dice: 1.8120
05/08 11:34:27 - mmengine - INFO - Exp name: mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20250508_024904
05/08 11:34:27 - mmengine - INFO - Iter(train) [20000/20000]  base_lr: 1.0000e-06 lr: 1.0000e-07  eta: 0:00:00  time: 1.4576  data_time: 0.0568  memory: 29385  grad_norm: 89.3792  loss: 28.6495  loss_cls: 0.7582  loss_mask: 0.1782  loss_dice: 1.7865  d0.loss_cls: 1.1028  d0.loss_mask: 0.2174  d0.loss_dice: 2.1023  d1.loss_cls: 0.7573  d1.loss_mask: 0.2061  d1.loss_dice: 2.0802  d2.loss_cls: 0.7596  d2.loss_mask: 0.1950  d2.loss_dice: 1.9465  d3.loss_cls: 0.7649  d3.loss_mask: 0.1879  d3.loss_dice: 1.8551  d4.loss_cls: 0.7585  d4.loss_mask: 0.1880  d4.loss_dice: 1.8637  d5.loss_cls: 0.7689  d5.loss_mask: 0.1853  d5.loss_dice: 1.8234  d6.loss_cls: 0.7048  d6.loss_mask: 0.1867  d6.loss_dice: 1.8305  d7.loss_cls: 0.7254  d7.loss_mask: 0.1812  d7.loss_dice: 1.8118  d8.loss_cls: 0.7266  d8.loss_mask: 0.1826  d8.loss_dice: 1.8143
05/08 11:34:27 - mmengine - INFO - Saving checkpoint at 20000 iterations
05/08 11:35:19 - mmengine - INFO - Iter(val) [50/50]    eta: 0:00:00  time: 0.9826  data_time: 0.0407  memory: 3258  
05/08 11:35:42 - mmengine - INFO - Evaluating segm...
Loading and preparing results...
DONE (t=3.33s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/08 11:35:50 - mmengine - INFO - start multi processing evaluation ...
DONE (t=51.95s).
Accumulating evaluation results...
DONE (t=0.06s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.316
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.598
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.282
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.204
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.358
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.846
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.534
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.859
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.547
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.394
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.601
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.917
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.534
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.534
05/08 11:36:42 - mmengine - INFO - segm_mAP_copypaste: 0.316 0.598 0.282 0.204 0.358 0.846
05/08 11:36:43 - mmengine - INFO - Iter(val) [50/50]    coco/segm_mAP: 0.3160  coco/segm_mAP_50: 0.5980  coco/segm_mAP_75: 0.2820  coco/segm_mAP_s: 0.2040  coco/segm_mAP_m: 0.3580  coco/segm_mAP_l: 0.8460  data_time: 0.0403  time: 0.9805

## TestLoop

/data1/max/instance_segmentation/mapchallenge-instance-segmentation
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
05/09 10:30:46 - mmengine - [4m[97mINFO[0m - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.2 (main, Jul 16 2024, 09:34:26) [GCC 11.4.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 1969011766
    GPU 0: NVIDIA L40S
    CUDA_HOME: None
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.4.1+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.19.1+cu121
    OpenCV: 4.11.0
    MMEngine: 0.10.6

Runtime environment:
    cudnn_benchmark: False
    dist_cfg: {'backend': 'nccl'}
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    seed: 1969011766
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

05/09 10:30:47 - mmengine - [4m[97mINFO[0m - Config:
_delete_ = True
auto_scale_lr = dict(base_batch_size=32, enable=False)
backbone_embed_multi = dict(decay_mult=0.0, lr_mult=0.1)
backbone_norm_multi = dict(decay_mult=0.0, lr_mult=0.1)
backend_args = None
base_batch_size = 32
batch_augments = [
    dict(
        img_pad_value=0,
        mask_pad_value=0,
        pad_mask=True,
        pad_seg=False,
        size=(
            320,
            320,
        ),
        type='BatchFixedSizePad'),
]
batch_size = 32
custom_keys = dict({
    'absolute_pos_embed':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone':
    dict(decay_mult=1.0, lr_mult=0.1),
    'backbone.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.patch_embed.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.0.blocks.0.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.0.blocks.1.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.0.downsample.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.1.blocks.0.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.1.blocks.1.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.1.downsample.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.0.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.1.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.10.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.11.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.12.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.13.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.14.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.15.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.16.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.17.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.2.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.3.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.4.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.5.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.6.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.7.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.8.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.blocks.9.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.2.downsample.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.3.blocks.0.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'backbone.stages.3.blocks.1.norm':
    dict(decay_mult=0.0, lr_mult=0.1),
    'level_embed':
    dict(decay_mult=0.0, lr_mult=1.0),
    'query_embed':
    dict(decay_mult=0.0, lr_mult=1.0),
    'query_feat':
    dict(decay_mult=0.0, lr_mult=1.0),
    'relative_position_bias_table':
    dict(decay_mult=0.0, lr_mult=0.1)
})
data_root = '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/'
dataset_type = 'SatelliteDataset'
default_hooks = dict(
    checkpoint=dict(
        by_epoch=False,
        interval=1000,
        max_keep_ckpts=3,
        rule='greater',
        save_best='coco/segm_mAP_50',
        save_last=True,
        type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(
        draw=True,
        show=True,
        test_out_dir=
        'work_dirs/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco/',
        type='DetVisualizationHook',
        wait_time=2))
default_scope = 'mmdet'
depths = [
    2,
    2,
    18,
    2,
]
dynamic_intervals = [
    (
        19200.0,
        20000,
    ),
]
embed_multi = dict(decay_mult=0.0, lr_mult=1.0)
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
image_size = (
    320,
    320,
)
interval = 1000
launcher = 'none'
load_from = '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco/best_coco_segm_mAP_50_iter_8000.pth'
log_level = 'INFO'
log_processor = dict(by_epoch=False, type='LogProcessor', window_size=50)
max_iters = 20000
mean = [
    88.03,
    104.33,
    115.77,
]
model = dict(
    backbone=dict(
        attn_drop_rate=0.0,
        convert_weights=True,
        depths=[
            2,
            2,
            18,
            2,
        ],
        drop_path_rate=0.3,
        drop_rate=0.0,
        embed_dims=96,
        frozen_stages=-1,
        init_cfg=dict(
            checkpoint=
            'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth',
            type='Pretrained'),
        mlp_ratio=4,
        num_heads=[
            3,
            6,
            12,
            24,
        ],
        out_indices=(
            0,
            1,
            2,
            3,
        ),
        patch_norm=True,
        qk_scale=None,
        qkv_bias=True,
        type='SwinTransformer',
        window_size=7,
        with_cp=False),
    data_preprocessor=dict(
        batch_augments=[
            dict(
                img_pad_value=0,
                mask_pad_value=0,
                pad_mask=True,
                pad_seg=False,
                size=(
                    320,
                    320,
                ),
                type='BatchFixedSizePad'),
        ],
        bgr_to_rgb=True,
        mask_pad_value=0,
        mean=[
            88.03,
            104.33,
            115.77,
        ],
        pad_mask=True,
        pad_seg=False,
        pad_size_divisor=32,
        seg_pad_value=255,
        std=[
            44.37,
            43.48,
            41.56,
        ],
        type='DetDataPreprocessor'),
    init_cfg=None,
    panoptic_fusion_head=dict(
        init_cfg=None,
        loss_panoptic=None,
        num_stuff_classes=0,
        num_things_classes=1,
        type='MaskFormerFusionHead'),
    panoptic_head=dict(
        enforce_decoder_input_project=False,
        feat_channels=256,
        in_channels=[
            96,
            192,
            384,
            768,
        ],
        loss_cls=dict(
            class_weight=[
                1.0,
                1.0,
            ],
            loss_weight=2.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=False),
        loss_dice=dict(
            activate=True,
            eps=1.0,
            loss_weight=5.0,
            naive_dice=True,
            reduction='mean',
            type='DiceLoss',
            use_sigmoid=True),
        loss_mask=dict(
            loss_weight=5.0,
            reduction='mean',
            type='CrossEntropyLoss',
            use_sigmoid=True),
        num_queries=100,
        num_stuff_classes=0,
        num_things_classes=1,
        num_transformer_feat_level=3,
        out_channels=256,
        pixel_decoder=dict(
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                layer_cfg=dict(
                    ffn_cfg=dict(
                        act_cfg=dict(inplace=True, type='ReLU'),
                        embed_dims=256,
                        feedforward_channels=320,
                        ffn_drop=0.0,
                        num_fcs=2),
                    self_attn_cfg=dict(
                        batch_first=True,
                        dropout=0.0,
                        embed_dims=256,
                        num_heads=8,
                        num_levels=3,
                        num_points=4)),
                num_layers=6),
            norm_cfg=dict(num_groups=32, type='GN'),
            num_outs=3,
            positional_encoding=dict(normalize=True, num_feats=128),
            type='MSDeformAttnPixelDecoder'),
        positional_encoding=dict(normalize=True, num_feats=128),
        strides=[
            4,
            8,
            16,
            32,
        ],
        transformer_decoder=dict(
            init_cfg=None,
            layer_cfg=dict(
                cross_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8),
                ffn_cfg=dict(
                    act_cfg=dict(inplace=True, type='ReLU'),
                    embed_dims=256,
                    feedforward_channels=2048,
                    ffn_drop=0.0,
                    num_fcs=2),
                self_attn_cfg=dict(
                    batch_first=True, dropout=0.0, embed_dims=256,
                    num_heads=8)),
            num_layers=9,
            return_intermediate=True),
        type='Mask2FormerHead'),
    test_cfg=dict(
        filter_low_score=True,
        instance_on=True,
        iou_thr=0.5,
        max_per_image=100,
        panoptic_on=False,
        semantic_on=False),
    train_cfg=dict(
        assigner=dict(
            match_costs=[
                dict(type='ClassificationCost', weight=2.0),
                dict(
                    type='CrossEntropyLossCost', use_sigmoid=True, weight=5.0),
                dict(eps=1.0, pred_act=True, type='DiceCost', weight=5.0),
            ],
            type='HungarianAssigner'),
        importance_sample_ratio=0.75,
        num_points=12544,
        oversample_ratio=3.0,
        sampler=dict(type='MaskPseudoSampler')),
    type='Mask2Former')
num_classes = 1
num_stuff_classes = 1
num_things_classes = 1
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.01, norm_type=2),
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        eps=1e-08,
        lr=0.0001,
        type='AdamW',
        weight_decay=0.05),
    paramwise_cfg=dict(
        custom_keys=dict({
            'absolute_pos_embed':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone':
            dict(decay_mult=1.0, lr_mult=0.1),
            'backbone.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.patch_embed.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.0.blocks.0.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.0.blocks.1.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.0.downsample.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.1.blocks.0.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.1.blocks.1.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.1.downsample.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.0.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.1.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.10.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.11.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.12.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.13.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.14.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.15.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.16.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.17.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.2.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.3.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.4.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.5.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.6.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.7.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.8.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.blocks.9.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.2.downsample.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.3.blocks.0.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'backbone.stages.3.blocks.1.norm':
            dict(decay_mult=0.0, lr_mult=0.1),
            'level_embed':
            dict(decay_mult=0.0, lr_mult=1.0),
            'query_embed':
            dict(decay_mult=0.0, lr_mult=1.0),
            'query_feat':
            dict(decay_mult=0.0, lr_mult=1.0),
            'relative_position_bias_table':
            dict(decay_mult=0.0, lr_mult=0.1)
        }),
        norm_decay_mult=0.0),
    type='OptimWrapper')
param_scheduler = dict(
    begin=0,
    by_epoch=False,
    end=20000,
    gamma=0.1,
    milestones=[
        16000.0,
        18000.0,
    ],
    type='MultiStepLR')
pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth'
std = [
    44.37,
    43.48,
    41.56,
]
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=32,
    dataset=dict(
        ann_file=
        '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/test/annotations/annotation_non_augmented.json',
        backend_args=None,
        data_prefix=dict(
            img=
            '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/test/images/'
        ),
        data_root='/usr/instance_segmentation/',
        filter_cfg=dict(filter_empty_gt=True, min_size=15),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                320,
                320,
            ), type='Resize'),
            dict(
                pad_val=dict(img=(
                    114,
                    114,
                    114,
                )),
                size=(
                    320,
                    320,
                ),
                type='Pad'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='SatelliteDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    ann_file=
    '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/test/annotations/annotation_non_augmented.json',
    backend_args=None,
    format_only=False,
    metric=[
        'segm',
    ],
    type='CocoMetric')
test_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        320,
        320,
    ), type='Resize'),
    dict(pad_val=dict(img=(
        114,
        114,
        114,
    )), size=(
        320,
        320,
    ), type='Pad'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(
    dynamic_intervals=[
        (
            19200.0,
            20000,
        ),
    ],
    max_iters=20000,
    type='IterBasedTrainLoop',
    val_interval=1000)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=32,
    dataset=dict(
        ann_file=
        '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/train/annotations/annotation_non_augmented.json',
        backend_args=None,
        data_prefix=dict(
            img=
            '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/train/images/'
        ),
        data_root='/usr/instance_segmentation/',
        filter_cfg=dict(filter_empty_gt=True, min_size=15),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(
                poly2mask=False,
                type='LoadAnnotations',
                with_bbox=True,
                with_mask=True),
            dict(img_scale=(
                320,
                320,
            ), pad_val=114.0, type='CachedMosaic'),
            dict(
                keep_ratio=True,
                ratio_range=(
                    0.1,
                    2.0,
                ),
                scale=(
                    320,
                    320,
                ),
                type='RandomResize'),
            dict(
                allow_negative_crop=True,
                crop_size=(
                    320,
                    320,
                ),
                recompute_bbox=True,
                type='RandomCrop'),
            dict(type='YOLOXHSVRandomAug'),
            dict(prob=0.5, type='RandomFlip'),
            dict(
                pad_val=dict(img=(
                    114,
                    114,
                    114,
                )),
                size=(
                    320,
                    320,
                ),
                type='Pad'),
            dict(
                img_scale=(
                    320,
                    320,
                ),
                max_cached_images=20,
                pad_val=(
                    114,
                    114,
                    114,
                ),
                ratio_range=(
                    1.0,
                    1.0,
                ),
                type='CachedMixUp'),
            dict(min_gt_bbox_wh=(
                1,
                1,
            ), type='FilterAnnotations'),
            dict(type='PackDetInputs'),
        ],
        type='SatelliteDataset'),
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(
        poly2mask=False,
        type='LoadAnnotations',
        with_bbox=True,
        with_mask=True),
    dict(img_scale=(
        320,
        320,
    ), pad_val=114.0, type='CachedMosaic'),
    dict(
        keep_ratio=True,
        ratio_range=(
            0.1,
            2.0,
        ),
        scale=(
            320,
            320,
        ),
        type='RandomResize'),
    dict(
        allow_negative_crop=True,
        crop_size=(
            320,
            320,
        ),
        recompute_bbox=True,
        type='RandomCrop'),
    dict(type='YOLOXHSVRandomAug'),
    dict(prob=0.5, type='RandomFlip'),
    dict(pad_val=dict(img=(
        114,
        114,
        114,
    )), size=(
        320,
        320,
    ), type='Pad'),
    dict(
        img_scale=(
            320,
            320,
        ),
        max_cached_images=20,
        pad_val=(
            114,
            114,
            114,
        ),
        ratio_range=(
            1.0,
            1.0,
        ),
        type='CachedMixUp'),
    dict(min_gt_bbox_wh=(
        1,
        1,
    ), type='FilterAnnotations'),
    dict(type='PackDetInputs'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=32,
    dataset=dict(
        ann_file=
        '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/val/annotations/annotation_non_augmented.json',
        backend_args=None,
        data_prefix=dict(
            img=
            '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/val/images/'
        ),
        data_root='/usr/instance_segmentation/',
        filter_cfg=dict(filter_empty_gt=True, min_size=15),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                320,
                320,
            ), type='Resize'),
            dict(
                pad_val=dict(img=(
                    114,
                    114,
                    114,
                )),
                size=(
                    320,
                    320,
                ),
                type='Pad'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='SatelliteDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    ann_file=
    '/data1/max/instance_segmentation/mapchallenge-instance-segmentation/mapping-challenge-v2.0/val/annotations/annotation_non_augmented.json',
    backend_args=None,
    format_only=False,
    metric=[
        'segm',
    ],
    type='CocoMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = 'work_dirs/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco'

05/09 10:30:49 - mmengine - [4m[97mINFO[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
/data1/max/instance_segmentation/mmdetection/mmdet/engine/hooks/visualization_hook.py:68: UserWarning: The show is True, it means that only the prediction results are visualized without storing data, so vis_backends needs to be excluded.
  warnings.warn('The show is True, it means that only '
05/09 10:30:49 - mmengine - [4m[97mINFO[0m - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
loading annotations into memory...
Done (t=0.37s)
creating index...
index created!
loading annotations into memory...
Done (t=0.37s)
creating index...
index created!
Loads checkpoint by local backend from path: /data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco/best_coco_segm_mAP_50_iter_8000.pth
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/mmengine/runner/checkpoint.py:347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(filename, map_location=map_location)
05/09 10:30:53 - mmengine - [4m[97mINFO[0m - Load checkpoint from /data1/max/instance_segmentation/mapchallenge-instance-segmentation/work_dirs/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco/best_coco_segm_mAP_50_iter_8000.pth
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/mmengine/visualization/visualizer.py:760: UserWarning: Warning: The bbox is out of bounds, the drawn bbox may not be in the image
  warnings.warn(
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/mmengine/visualization/visualizer.py:831: UserWarning: Warning: The polygon is out of bounds, the drawn polygon may not be in the image
  warnings.warn(
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/mmengine/visualization/visualizer.py:760: UserWarning: Warning: The bbox is out of bounds, the drawn bbox may not be in the image
  warnings.warn(
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/mmengine/visualization/visualizer.py:831: UserWarning: Warning: The polygon is out of bounds, the drawn polygon may not be in the image
  warnings.warn(
/home/watchtower/.pyenv/versions/3.10.2/envs/dmcon/lib/python3.10/site-packages/mmengine/visualization/visualizer.py:508: UserWarning: Warning: The text is out of bounds, the drawn text may not be in the image
  warnings.warn(
05/09 10:33:52 - mmengine - [4m[97mINFO[0m - Iter(test) [ 50/200]    eta: 0:08:49  time: 3.5332  data_time: 2.5717  memory: 2476  
05/09 10:36:53 - mmengine - [4m[97mINFO[0m - Iter(test) [100/200]    eta: 0:05:57  time: 3.6164  data_time: 2.6701  memory: 2475  
05/09 10:39:45 - mmengine - [4m[97mINFO[0m - Iter(test) [150/200]    eta: 0:02:56  time: 3.4566  data_time: 2.4890  memory: 2475  
05/09 10:42:41 - mmengine - [4m[97mINFO[0m - Iter(test) [200/200]    eta: 0:00:00  time: 3.5026  data_time: 2.5647  memory: 2475  
05/09 10:43:01 - mmengine - [4m[97mINFO[0m - Evaluating segm...
Loading and preparing results...
DONE (t=3.30s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
05/09 10:43:08 - mmengine - [4m[97mINFO[0m - start multi processing evaluation ...
0it [00:00, ?it/s]
0it [00:00, ?it/s]
0it [00:00, ?it/s]
0it [00:00, ?it/s]
0it [00:00, ?it/s]
0it [00:00, ?it/s]
0it [00:00, ?it/s]
100%|| 25600/25600 [00:00<00:00, 141945.60it/s]
DONE (t=53.87s).
Accumulating evaluation results...
DONE (t=0.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.380
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.721
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.382
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.495
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.692
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.504
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.879
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.509
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.594
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.783
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.504
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.504
05/09 10:44:02 - mmengine - [4m[97mINFO[0m - segm_mAP_copypaste: 0.380 0.721 0.382 0.152 0.495 0.692
05/09 10:44:02 - mmengine - [4m[97mINFO[0m - segm_mAR_copypaste: 0.504 0.879 0.509 0.338 0.594 0.783 
05/09 10:44:03 - mmengine - [4m[97mINFO[0m - Iter(test) [200/200]    coco/segm_mAP: 0.3800  coco/segm_mAP_50: 0.7210  coco/segm_mAP_75: 0.3820  coco/segm_mAP_s: 0.1520  coco/segm_mAP_m: 0.4950  coco/segm_mAP_l: 0.6920  data_time: 2.5739  time: 3.5272